ALL CODE IN PLAINTEXT
==================================================
Generated: 2025-09-18T02:32:39.767258


================================================================================
FILE: mm/ariadne.py
================================================================================
#===================================================================
# 🍁 A R I A N D E           bot version 6.1 file build 20250917.01
#===================================================================
# last update: 2025 | Sept. 17                  Production ready ✅
#===================================================================
# Ariadne Actual
# mm/ariadne.py
#
# Initialized: August 19, 2025
# Sim Testing: ---   
# Launched: ---
# KuCoin exchange market maker bot. 
# Initial capital of $2,500 CAD
#
# [520] [741] [8]
#===================================================================
# 🜁 THE COMMANDER            ✔ PERSISTANT RUNTIME  ✔ MONIT MANAGED
#===================================================================

# 🔸 Standard Library Imports ======================================

import os
import importlib
import smtplib
import ssl
import uuid
import logging
from datetime import datetime
from email.message import EmailMessage
from email.utils import formataddr
from zoneinfo import ZoneInfo

# 🔸 third-party imports ===========================================

from dotenv import load_dotenv

# 🔸 local application imports =====================================

import mm.config.marcus as marcus
from mm.core.drcalvin import ValueOps
from mm.core.grayson import RiskOps
from mm.utils.seldon_engine.quorra import SigmaOps
from mm.core.petra import Petra
from mm.core.helen import Helen
from mm.core.malcolm import Malcolm
from mm.core.julius import Julius
from mm.core.verity import IntelOps
from mm.utils.seldon_engine.lamar import SigInt
from mm.core.alec import Alec
from mm.utils.nexus_6.rachael import Replicant
from mm.utils.helpers.wintermute import update_heartbeat
from mm.utils.tqdm.agnes import setup_logger

logger = setup_logger("ariadne", level=logging.INFO)

# 🔸 load env for this process =====================================

load_dotenv("mm/data/secrets/.env")

# 🔸 Drop-in Emailer ===============================================

def send_email(subject: str, status: str, title: str, message: str) -> str:

    importlib.reload(marcus)
    if not bool(getattr(marcus, "ALERT_EMAIL_ENABLED", False)):
        return "disabled"
    if str(getattr(marcus, "ALERT_EMAIL_ENCRYPT", "SSL")).upper() != "SSL":
        return "Simple Mail Transfer Protocol not established. No conn."

    host = getattr(marcus, "ALERT_EMAIL_SMTP_SERVER", None)
    port = getattr(marcus, "ALERT_EMAIL_SMTP_PORT", None)
    recipient = getattr(marcus, "ALERT_EMAIL_RECIPIENT", None)

    USERCODE = "ARI"  # hardcode per file

    # ---- Edit Sender Info (per file) ----
    user = os.getenv(f"{USERCODE}_USR")
    pwd = os.getenv(f"{USERCODE}_PWD")
    sender_email = user
    sender_name = os.getenv(f"{USERCODE}_NAME")
    # -------------------------------------

    STATUS_COLORS = {
        "STATCON3": "#F1C232",
        "STATCON2": "#E69138",
        "STATCON1": "#CC0000",
        "SIGCON1":  "#FB6D8B",
        "OPSCON5":  "#F5F5F5",
        "OPSCON1":  "#990000",
    }
    status_text = str(status).upper()
    status_color = STATUS_COLORS.get(status_text, "#BE644C")

    msg = EmailMessage()
    domain = sender_email.split("@")[1] if "@" in sender_email else "hodlcorp.io"
    msg_id = f"<{uuid.uuid4()}@{domain}>"
    msg["Message-ID"] = msg_id
    msg["From"] = formataddr((sender_name, sender_email))
    msg["To"] = recipient
    msg["Subject"] = subject
    msg["X-Priority"] = "1"
    msg["X-MSMail-Priority"] = "High"
    msg["Importance"] = "High"

    now_tz = datetime.now(ZoneInfo("America/Toronto"))
    sent_str = now_tz.strftime("%Y-%m-%d %H:%M:%S America/Toronto")
    epoch_ms = int(now_tz.timestamp() * 1000)
    mid_clean = msg_id.strip("<>").split("@", 1)[0]

    html_body = f"""
<div style="font-family: monospace;">
  <table role="presentation" width="100%" height="20px" cellpadding="8px" cellspacing="0" border="0">
    <tbody><tr style="font-family: Georgia, 'Times New Roman', Times, serif;font-size:20px;font-weight:600;background-color:#333;">
      <td align="left" style="color:#EFEFEF;letter-spacing:12px;">INTCOMM</td>
      <td align="right" style="color:{status_color};letter-spacing:4px;">{status_text}</td>
    </tr>
    <tr width="100%" cellpadding="6px" style="font-family: Tahoma, Geneva, sans-serif;text-align:left;font-size:14px;font-weight:600;color:#333;">
      <td colspan="2">{title}</td>
    </tr>
    <tr width="100%" cellpadding="6px" style="font-family: Tahoma, Geneva, sans-serif;text-align:left;font-size:11px;font-weight:400;line-height:1.5;color:#333;">
      <td colspan="2">{message}</td>
    </tr>
    <tr width="100%" height="25px"><td colspan="2">&nbsp;</td></tr>
  </tbody></table>
  <table role="presentation" width="400px" height="20px" cellpadding="4" cellspacing="0" border="0" style="font-family: Tahoma, Geneva, sans-serif;">
    <tbody><tr style="background-color:#333;">
      <td colspan="2" style="color:#efefef;font-size:12px;font-weight:600;">DOCINT</td>
    </tr>
    <tr style="background-color:#E9E9E5;">
      <td width="30px" style="color:#333;font-size:10px;font-weight:600;">SENT</td>
      <td width="10px" style="color:#333;font-size:10px;font-weight:600;">→</td>
      <td style="color:#333;font-size:11px;font-weight:400;">{sent_str}</td>
    </tr>
    <tr style="background-color:#F2F2F0;">
      <td width="30px" style="color:#333;font-size:10px;font-weight:600;">EPOCH</td>
      <td width="10px" style="color:#333;font-size:10px;font-weight:600;">→</td>
      <td style="color:#333;font-size:11px;font-weight:400;">{epoch_ms} (ms since 1970/01/01 0:00 UTC)</td>
    </tr>
    <tr style="background-color:#E9E9E5;">
      <td width="30px" style="color:#333;font-size:10px;font-weight:600;">m.ID</td>
      <td width="10px" style="color:#333;font-size:10px;font-weight:600;">→</td>
      <td style="color:#333;font-size:11px;font-weight:400;">{mid_clean}</td>
    </tr>
  </tbody></table>
</div>
"""

    msg.add_alternative(html_body, subtype="html")
    ctx = ssl.create_default_context()
    with smtplib.SMTP_SSL(host, port, context=ctx, timeout=10) as s:
        if user and pwd:
            s.login(user, pwd)
        s.send_message(msg)

    return msg_id

# 🔸 Ariadne Class =================================================

class Ariadne:

    def __init__(self, inara, logger):
        self.inara = inara
        self.logger = logger
        self.client = None
        self.mode = None
        self.cycle_count = 0

 # 🔸 STARTUP =======================================================

    def run(self):
        import time

        while True:
            self.logger.info("Starting cycle...")
            self.mode = self.inara.get_mode()
            self.client = self.inara.get_trading_client()
            self.logger.info(f"Mode: {self.mode}, Client: {self.client}")

    # 🔸 OPEN ORDERS RISK ASSESSMENT====================================
            
            current_orders = self.client.get_orders()
            self.logger.info("Get open orders -> Fetched.")
            
            if not current_orders:
                self.logger.info("No open orders, moving on to production cycle.")
            
            for order in current_orders.copy():
                print("Cycling through open orders.") 
                order_id = order["id"]
                
                grayson = RiskOps(order)
                if not grayson.compliant():
                    Alec.cancel_orders_for_pair(order)
                    self.logger.info(f"Order {order_id} canceled by Grayson.")
                    continue

                score = ValueOps.score_pair(order)
                if score < 75:
                    Alec.cancel_orders_for_pair(order)
                    self.logger.info(f"Order {order_id} canceled by Dr. Calvin (score {score}).")
                    continue

                risk_client = SigmaOps(order)
                score2 = risk_client.score_pair()
                if score2 >= 80:
                    continue
                elif 70 <= score2 < 80:
                    Replicant(order).process()
                    current_orders.remove(order)
                    continue
                else:
                    Alec.cancel_orders_for_pair(order)
                    self.logger.info(f"Order {order_id} canceled by Quorra (score {score2}).")
                    continue

    # 🔸 SELL CYCLE ====================================================
            
            self.logger.info("Starting sell cycle...")
        
            petra = Petra(self.client)
            proposals = petra.prepare_sell_orders(Helen.get_positions())

            for proposal in proposals:
                response = SigInt.listen(proposal)
                if response == "expired":
                    score = SigmaOps(proposal).score_pair()
                    if score >= 95:
                        petra.resubmit(proposal)
                elif response == "denied":
                    score = SigmaOps(proposal).score_pair()
                    if score <= 75:
                        self.logger.warning(f"Proposal denied and below threshold: {proposal}")
                elif response == "approved":
                    self.client.place_order(proposal)

    # 🔸 BUY CYCLE =====================================================
            
            best_pairs = Helen.get_best_pairs()
            best_pairs = [p for p in best_pairs if RiskOps(p).compliant()]
            scored_pairs = [(p, SigmaOps(p).score_pair()) for p in best_pairs]

            malcolm = Malcolm(self.client)
            buy_proposals = malcolm.prepare_buy_orders(scored_pairs)

            for proposal in buy_proposals:
                response = SigInt.listen(proposal)
                if response == "expired":
                    score = SigmaOps(proposal).score_pair()
                    if score >= 95:
                        malcolm.resubmit(proposal)
                elif response == "denied":
                    score = SigmaOps(proposal).score_pair()
                    if score <= 75:
                        self.logger.warning(f"Proposal denied and below threshold: {proposal}")
                elif response == "approved":
                    self.client.place_order(proposal)
                    Database.record_order(proposal)

    # 🔸 HOUSEKEEPING CYCLE ============================================

            if self.mode == "simulation":
                Julius().sweep_stale_holds()
                Helen.sweep_stale_holds()

            IntelOps(self.client).scan()

            if self.cycle_count % 10 == 0:
                Database.save_state()

            if self.cycle_count % 6 == 0:  # ~ every 2 minutes if cycle ~20s
                update_heartbeat("ariadne", conn)

            self.cycle_count += 1
            time.sleep(20)
            
# ⚡ Entry Point ⚡ ==================================================
            
print("✔ File loaded")

if __name__ == "__main__":
    print("✔ Main block entered")

    try:
        from mm.utils.helpers import inara
        print("✔ Inara imported")

        bot = Ariadne(inara, logger)
        print("🧬 Ariadne instantiated")

        bot.run()
    except Exception as e:
        print("⛔ CRASH:", e)

================================================================================
FILE: mm/core/verity.py
================================================================================
#===================================================================
# 🍁 A R I A N D E           bot version 6.1 file build 20250917.01
#===================================================================
# last update: 2025 | Sept. 17                  Production ready ✅
#===================================================================
# Verity - Applied IO (Intelligence Officer) (Metrics)
# mm/core/verity.py
#
# Performance and risk metrics tracking
# Statistical analysis of trading operations
# Provides insights for optimization
#
# [520] [741] [8]
#===================================================================
# 🜁 THE COMMANDER            ✖ PERSISTANT RUNTIME  ✖ MONIT MANAGED
#===================================================================

# 🔸 Standard Library Imports ======================================

import logging
import time
import json
from typing import Dict, List, Optional, Any
from datetime import datetime, timedelta
from collections import defaultdict, deque

# 🔸 Logger Setup ==================================================

logger = logging.getLogger('ariadne.metrics')

class IntelOps:
# 💬 Tracks and analyzes all performance and risk metrics

    def __init__(self, history_limit: int = 10000):
        """
        Initialize metrics tracking

        Args:
            history_limit: Maximum data points to keep per metric
        """
        self.logger = logger
        self.history_limit = history_limit

        # 🔹 Core metric storage =======================================

        self.metrics: Dict[str, deque] = defaultdict(lambda: deque(maxlen=history_limit))
        self.counters: Dict[str, int] = defaultdict(int)

        # 🔹 Summary statistics cache ==================================

        self.stats_cache: Dict[str, Dict] = {}
        self.cache_timestamp: float = 0
        self.cache_ttl: float = 60  # 1 minute cache

        # 🔹 Trading-specific metrics ==================================

        self.trades: List[Dict] = []
        self.daily_metrics: Dict[str, Dict] = defaultdict(dict)
        self.session_start: float = time.time()

    def record_metric(self, name: str, value: float, timestamp: Optional[float] = None):
        """
        Record a metric data point

        Args:
            name: Metric name
            value: Metric value
            timestamp: Optional timestamp (defaults to now)
        """
        if timestamp is None:
            timestamp = time.time()

        self.metrics[name].append({
            'value': value,
            'timestamp': timestamp
        })

        # 🔹 Invalidate cache ==========================================

        self.stats_cache.pop(name, None)

    def increment_counter(self, name: str, amount: int = 1):
        """
        Increment a counter metric

        Args:
            name: Counter name
            amount: Amount to increment
        """
        self.counters[name] += amount

    def get_stats(self, metric_name: str, window_seconds: Optional[int] = None) -> Dict:
        """
        Get statistics for a metric

        Args:
            metric_name: Name of metric
            window_seconds: Optional time window (uses all data if None)

        Returns:
            Dict with min, max, avg, count, last
        """
        # 🔹 Check cache first =========================================

        cache_key = f"{metric_name}_{window_seconds}"
        if cache_key in self.stats_cache and time.time() - self.cache_timestamp < self.cache_ttl:
            return self.stats_cache[cache_key]

        if metric_name not in self.metrics:
            return {
                'min': 0,
                'max': 0,
                'avg': 0,
                'count': 0,
                'last': 0,
                'std_dev': 0
            }

        data = list(self.metrics[metric_name])

        # 🔹 Filter by time window if specified ========================

        if window_seconds:
            cutoff = time.time() - window_seconds
            data = [d for d in data if d['timestamp'] > cutoff]

        if not data:
            return {
                'min': 0,
                'max': 0,
                'avg': 0,
                'count': 0,
                'last': 0,
                'std_dev': 0
            }

        values = [d['value'] for d in data]

        # 🔹 Calculate standard deviation ==============================

        avg = sum(values) / len(values)
        variance = sum((x - avg) ** 2 for x in values) / len(values)
        std_dev = variance ** 0.5

        stats = {
            'min': min(values),
            'max': max(values),
            'avg': avg,
            'count': len(values),
            'last': values[-1],
            'std_dev': std_dev
        }

        # 🔹 Cache result ==============================================

        self.stats_cache[cache_key] = stats
        self.cache_timestamp = time.time()

        return stats

    def record_trade(self, trade: Dict):
        """
        Record a completed trade

        Args:
            trade: Trade details dict
        """
        trade['timestamp'] = time.time()
        self.trades.append(trade)

        # 🔹 Update counters ===========================================

        self.increment_counter(f"trades_{trade['side']}")
        self.increment_counter("trades_total")

        # 🔹 Record P&L if available ===================================

        if 'pnl' in trade:
            self.record_metric('trade_pnl', trade['pnl'])
            if trade['pnl'] > 0:
                self.increment_counter('winning_trades')
            else:
                self.increment_counter('losing_trades')

    def get_performance_summary(self) -> Dict:
        """
        Get comprehensive performance summary

        Returns:
            Dict with performance metrics
        """
        # 🔹 Calculate uptime ==========================================

        uptime_seconds = time.time() - self.session_start
        uptime_hours = uptime_seconds / 3600

        # 🔹 Trade statistics ==========================================

        total_trades = self.counters['trades_total']
        winning_trades = self.counters['winning_trades']
        losing_trades = self.counters['losing_trades']

        win_rate = 0
        if total_trades > 0:
            win_rate = winning_trades / total_trades

        # 🔹 P&L statistics ============================================

        pnl_stats = self.get_stats('trade_pnl')
        loop_stats = self.get_stats('loop_time')
        equity_stats = self.get_stats('total_equity')

        return {
            'uptime_hours': round(uptime_hours, 2),
            'total_trades': total_trades,
            'winning_trades': winning_trades,
            'losing_trades': losing_trades,
            'win_rate': round(win_rate, 3),
            'buy_orders': self.counters['trades_buy'],
            'sell_orders': self.counters['trades_sell'],
            'pnl': {
                'total': sum(t.get('pnl', 0) for t in self.trades),
                'average': pnl_stats['avg'],
                'best': pnl_stats['max'],
                'worst': pnl_stats['min']
            },
            'loop_performance': {
                'average_ms': round(loop_stats['avg'] * 1000, 1),
                'min_ms': round(loop_stats['min'] * 1000, 1),
                'max_ms': round(loop_stats['max'] * 1000, 1)
            },
            'equity': {
                'current': equity_stats['last'],
                'high': equity_stats['max'],
                'low': equity_stats['min']
            }
        }

    def get_hourly_metrics(self, hours: int = 24) -> Dict[str, List]:
        """
        Get metrics aggregated by hour

        Args:
            hours: Number of hours to retrieve

        Returns:
            Dict of metric_name -> list of hourly values
        """
        hourly_data = defaultdict(list)
        cutoff = time.time() - (hours * 3600)

        for metric_name, data in self.metrics.items():

            # 🔹 Group by hour =========================================

            hourly_buckets = defaultdict(list)

            for point in data:
                if point['timestamp'] > cutoff:
                    hour = datetime.fromtimestamp(point['timestamp']).replace(
                        minute=0, second=0, microsecond=0
                    )
                    hourly_buckets[hour].append(point['value'])

            # 🔹 Calculate hourly averages =============================

            for hour in sorted(hourly_buckets.keys()):
                values = hourly_buckets[hour]
                hourly_data[metric_name].append({
                    'hour': hour.isoformat(),
                    'avg': sum(values) / len(values),
                    'count': len(values)
                })

        return dict(hourly_data)

    def log_performance_stats(self):
        """Log current performance statistics"""
        summary = self.get_performance_summary()

        self.logger.info(
            f"📊 Performance - "
            f"Uptime: {summary['uptime_hours']:.1f}h | "
            f"Trades: {summary['total_trades']} (Win: {summary['win_rate']:.1%}) | "
            f"P&L: ${summary['pnl']['total']:.2f} | "
            f"Loop: {summary['loop_performance']['average_ms']}ms"
        )

    def export_metrics(self, filepath: str):
        """
        Export all metrics to JSON file

        Args:
            filepath: Output file path
        """
        try:
            export_data = {
                'timestamp': time.time(),
                'session_start': self.session_start,
                'counters': dict(self.counters),
                'summary': self.get_performance_summary(),
                'metrics': {}
            }

        # 🔹 Convert deques to lists for JSON serialization ============

            for name, data in self.metrics.items():
                export_data['metrics'][name] = list(data)

            with open(filepath, 'w') as f:
                json.dump(export_data, f, indent=2)

            self.logger.info(f"Exported metrics to {filepath}")

        except Exception as e:
            self.logger.error(f"Failed to export metrics: {e}")

    def reset_daily_metrics(self):
        """Reset daily tracking (call at UTC midnight)"""

        # 🔹 Store yesterday's data ====================================

        yesterday = (datetime.utcnow() - timedelta(days=1)).strftime('%Y-%m-%d')
        self.daily_metrics[yesterday] = {
            'trades': self.counters['trades_total'],
            'pnl': sum(t.get('pnl', 0) for t in self.trades),
            'win_rate': self.counters['winning_trades'] / max(1, self.counters['trades_total'])
        }

        # 🔹 Reset counters ============================================

        for key in ['trades_total', 'trades_buy', 'trades_sell', 'winning_trades', 'losing_trades']:
            self.counters[key] = 0

        # 🔹 Clear today's trades ======================================

            cutoff = datetime.utcnow().replace(hour=0, minute=0, second=0).timestamp()
            self.trades = [t for t in self.trades if t['timestamp'] < cutoff]

            self.logger.info("Daily metrics reset completed")

================================================================================
FILE: mm/core/grayson.py
================================================================================
#>> A R I A N D E [v 6.1]
#>> last update: 2025 | Sept. 9                ❌ PRODUCTION READY
#>>
#>> Risk Manager
#>> mm/core/grayson.py  
#>>
#>> Analytical layer for market analysis and decision logic. 
#>>
#>> Auth'd -> Commander
#>>
#>> [520] [741] [8]        💫 PERSISTANT RUNTIME  ➰ MONIT MANAGED
#>>────────────────────────────────────────────────────────────────

# Build|20250909.01

import os
import json
import logging
from dataclasses import dataclass
from typing import Dict, Optional, Tuple

import psycopg2

from mm.config import marcus
from mm.conn.conn_kucoin import KucoinClient

# Inara mode helpers
try:
    from mm.utils.helpers.inara import current_mode, is_live_mode
except Exception:
    def current_mode() -> str:
        return getattr(marcus, "MODE", "simulation")
    def is_live_mode() -> bool:
        return current_mode() == "live"

DSN = os.getenv("PG_DSN", "dbname=ariadne user=postgres host=localhost")
logger = logging.getLogger("grayson")
logger.setLevel(logging.INFO)


@dataclass
class Caps:
    quote: str = getattr(marcus, "QUOTE_CURRENCY", "USDT")
    min_trade: float = getattr(marcus, "MIN_TRADE_SIZE", 10.0)
    max_per_pair: float = getattr(marcus, "MAX_EXPOSURE_PER_PAIR", 0.10)      # share of equity
    max_total: float = getattr(marcus, "MAX_TOTAL_EXPOSURE", 0.60)            # share of equity
    max_asset_pct: float = getattr(marcus, "MAX_ASSET_PCT", 0.10)             # share of portfolio per asset
    cap_margin: float = getattr(marcus, "CAP_MARGIN", 0.01)                   # tolerance band
    max_active_pairs: int = getattr(marcus, "MAX_ACTIVE_PAIRS", 10)
    daily_loss_limit: float = getattr(marcus, "DAILY_LOSS_LIMIT", 0.05)
    max_drawdown_pct: float = getattr(marcus, "MAX_DRAWDOWN_PCT", 0.10)


class RiskOps:
    def __init__(self, client: Optional[KucoinClient] = None, cfg: Caps = Caps()):
        self.cfg = cfg
        self.client = client or KucoinClient()
        self.conn = psycopg2.connect(DSN)
        self.cur = self.conn.cursor()

        # Sticky debug for last decision reason (handy in logs/UI)
        self.last_reason: str = ""
        logger.info("Grayson online | mode=%s | quote=%s", current_mode(), self.cfg.quote)

    # ─────────────────────────── External API ───────────────────────────

    def can_trade_pair(self, symbol: str, total_equity: float, *, side: str = "buy",
                       notional: Optional[float] = None) -> bool:
        """Quick gate used by selection/loop. Sets self.last_reason."""
        mode = current_mode()
        if not self._mode_allows(side):
            self.last_reason = f"mode_block:{mode}:{side}"
            return False

        # Notional unknown? Use small probe via last price × some min size (treated as pass-through).
        if notional is None:
            px = self._last_price(symbol)
            if px <= 0:
                self.last_reason = "no_price"
                return False
            # Minimalistic notional (won't be used for enforcement except min_trade check)
            notional = px * max(0.0, float(getattr(marcus, "MIN_TRADE_SIZE", self.cfg.min_trade)) / max(px, 1e-9))

        if not self._min_trade_ok(notional):
            self.last_reason = "min_trade"
            return False

        # Load exposures
        pair_exp, tot_exp, active_pairs = self._exposures(symbol)

        # Per-pair limit (include proposed notional)
        per_cap = (self.cfg.max_per_pair + self.cfg.cap_margin) * max(1e-9, total_equity)
        if pair_exp + notional > per_cap:
            self.last_reason = f"per_pair_cap:{pair_exp + notional:.2f}>{per_cap:.2f}"
            return False

        # Total exposure limit
        tot_cap = (self.cfg.max_total + self.cfg.cap_margin) * max(1e-9, total_equity)
        if tot_exp + notional > tot_cap:
            self.last_reason = f"total_cap:{tot_exp + notional:.2f}>{tot_cap:.2f}"
            return False

        # Max active pairs (BUY only increases count if position/order absent)
        if side.lower() == "buy" and active_pairs >= self.cfg.max_active_pairs and pair_exp <= 0:
            self.last_reason = "max_active_pairs"
            return False

        self.last_reason = "ok"
        return True

    def check_risk_limits(self, total_equity: float) -> Dict[str, str]:
        """
        Global guard called from main loop.
        Returns: {"trading_allowed": bool, "reason": str}
        """
        mode = current_mode()
        if mode in ("halted", "maintenance"):
            return {"trading_allowed": False, "reason": f"mode:{mode}"}

        # Daily loss / drawdown checks (best-effort, fail-open if no data)
        if not self._daily_loss_ok(total_equity):
            return {"trading_allowed": False, "reason": "daily_loss_limit"}
        if not self._drawdown_ok(total_equity):
            return {"trading_allowed": False, "reason": "max_drawdown"}

        return {"trading_allowed": True, "reason": "ok"}

    def vet_risk(self, proposal_id: int, total_equity: Optional[float] = None) -> str:
        """
        Phase-1 vetting for Lamar: sets proposals.risk_vet ← 'approved'|'denied'
        Side-aware: BUY/Sell both go through the same rules; mode gates applied.
        """
        prop = self._fetch_proposal(proposal_id)
        if not prop:
            return "denied"

        # Mode gating first
        if not self._mode_allows(prop["side"]):
            self._set_vet(proposal_id, "risk_vet", "denied")
            self._route_log(proposal_id, "risk.denied", {"reason": "mode_gate", "mode": current_mode(), "side": prop["side"]})
            return "denied"

        # Determine notional and equity
        px = float(prop["price_intent"])
        sz = float(prop["size_intent"])
        notional = px * sz
        eq = float(total_equity if total_equity is not None else getattr(marcus, "INITIAL_CAPITAL", 1000.0))

        ok = self.can_trade_pair(prop["symbol"], eq, side=prop["side"], notional=notional)
        status = "approved" if ok else "denied"
        self._set_vet(proposal_id, "risk_vet", status)
        self._route_log(proposal_id, f"risk.{status}", {
            "symbol": prop["symbol"], "side": prop["side"],
            "notional": notional, "reason": self.last_reason
        })
        return status

    # ─────────────────────────── Internals ────────────────────────────

    def _mode_allows(self, side: str) -> bool:
        side = side.lower()
        m = current_mode()
        if m in ("halted", "maintenance"):
            return False
        if m == "drain" and side == "buy":
            return False
        return True

    def _min_trade_ok(self, notional: float) -> bool:
        try:
            return float(notional) >= float(self.cfg.min_trade)
        except Exception:
            return False

    def _fetch_proposal(self, pid: int) -> Optional[Dict]:
        self.cur.execute("""
            SELECT id, symbol, side, price_intent, size_intent
            FROM proposals
            WHERE id = %s
        """, (pid,))
        row = self.cur.fetchone()
        if not row:
            return None
        return dict(zip(("id","symbol","side","price_intent","size_intent"), row))

    def _set_vet(self, pid: int, col: str, status: str) -> None:
        try:
            self.cur.execute(f"UPDATE proposals SET {col} = %s WHERE id = %s", (status, pid))
            self.conn.commit()
        except Exception as e:
            self.conn.rollback()
            logger.warning("vet update failed (%s) p=%s: %s", col, pid, e)

    def _route_log(self, proposal_id: int, status: str, info: Dict) -> None:
        try:
            self.cur.execute("""
                INSERT INTO proposal_router_log (proposal_id, timestamp, status, details)
                VALUES (%s, NOW(), %s, %s)
            """, (proposal_id, status, json.dumps(info)))
            self.conn.commit()
        except Exception as e:
            self.conn.rollback()
            logger.warning("router log insert failed p=%s: %s", proposal_id, e)

    # ───────────── Exposure accounting (SIM vs LIVE) ─────────────

    def _exposures(self, symbol: str) -> Tuple[float, float, int]:
        """
        Returns (pair_exposure, total_exposure, active_pairs_count) in QUOTE terms.
        Exposure = (long position notional + open BUY orders notional). SELL orders don't increase exposure.
        """
        # Positions
        if is_live_mode():
            pos_map = self._live_positions_notional()
            ord_pair, ord_total = self._open_buy_orders_notional(live=True, symbol=symbol)
        else:
            pos_map = self._sim_positions_notional()
            ord_pair, ord_total = self._open_buy_orders_notional(live=False, symbol=symbol)

        pair_exp = pos_map.get(symbol, 0.0) + ord_pair
        tot_exp = sum(pos_map.values()) + ord_total

        # Active pairs = symbols with positive (position notional OR open orders notional)
        active_pairs = set([s for s, v in pos_map.items() if v > 0.0])
        if ord_pair > 0:
            active_pairs.add(symbol)
        return (pair_exp, tot_exp, len(active_pairs))

    def _sim_positions_notional(self) -> Dict[str, float]:
        self.cur.execute("SELECT symbol, COALESCE(qty,0)::numeric FROM sim_positions")
        out: Dict[str, float] = {}
        rows = self.cur.fetchall() or []
        for sym, qty in rows:
            q = float(qty or 0.0)
            if q <= 0:
                continue
            px = self._last_price(sym)
            if px > 0:
                out[sym] = q * px
        return out

    def _live_positions_notional(self) -> Dict[str, float]:
        # Minimal assumption: positions table with (symbol, qty) or (symbol, quantity)
        try:
            self.cur.execute("SELECT symbol, COALESCE(qty,0)::numeric FROM positions")
        except Exception:
            try:
                self.conn.rollback()
                self.cur.execute("SELECT symbol, COALESCE(quantity,0)::numeric FROM positions")
            except Exception:
                self.conn.rollback()
                return {}
        out: Dict[str, float] = {}
        for sym, qty in (self.cur.fetchall() or []):
            q = float(qty or 0.0)
            if q <= 0:
                continue
            px = self._last_price(sym)
            if px > 0:
                out[sym] = q * px
        return out

    def _open_buy_orders_notional(self, live: bool, symbol: str) -> Tuple[float, float]:
        table = "orders" if live else "sim_orders"
        # status may be 'open' or 'active'; include both
        try:
            self.cur.execute(f"""
                SELECT symbol, COALESCE(price,0)::numeric, COALESCE(size,0)::numeric
                FROM {table}
                WHERE (status = 'open' OR status = 'active') AND side = 'buy'
            """)
        except Exception:
            self.conn.rollback()
            return (0.0, 0.0)

        pair_sum = 0.0
        tot_sum = 0.0
        for sym, price, size in (self.cur.fetchall() or []):
            n = float(price or 0.0) * float(size or 0.0)
            tot_sum += n
            if str(sym).upper() == str(symbol).upper():
                pair_sum += n
        return (pair_sum, tot_sum)

    # ─────────────── Global loss / drawdown checks ───────────────

    def _daily_loss_ok(self, total_equity: float) -> bool:
        """
        Best-effort: check simulated or logged PnL for the current day.
        If unavailable, allow trading (fail-open).
        """
        try:
            # Try a risk_metrics table for 'daily_return' or 'daily_pnl'
            self.cur.execute("""
                SELECT metric, value
                FROM risk_metrics
                WHERE metric IN ('daily_return','daily_pnl')
                ORDER BY timestamp DESC
                LIMIT 1
            """)
            row = self.cur.fetchone()
            if not row:
                return True
            metric, value = row
            v = float(value or 0.0)
            if metric == "daily_return":
                return v >= -self.cfg.daily_loss_limit
            if metric == "daily_pnl":
                # Interpret as absolute loss cap against equity
                return (v / max(1e-9, total_equity)) >= -self.cfg.daily_loss_limit
            return True
        except Exception:
            self.conn.rollback()
            return True

    def _drawdown_ok(self, total_equity: float) -> bool:
        """
        Best-effort: check last recorded drawdown in risk_metrics or performance_metrics.
        Fail-open if not available.
        """
        try:
            self.cur.execute("""
                SELECT metric, value
                FROM risk_metrics
                WHERE metric IN ('max_drawdown','drawdown')
                ORDER BY timestamp DESC
                LIMIT 1
            """)
            row = self.cur.fetchone()
            if not row:
                return True
            metric, value = row
            dd = float(value or 0.0)
            # If already a fraction (0..1) use directly; if looks like %, normalize
            if dd > 1.5:  # treat as percent
                dd = dd / 100.0
            return dd <= self.cfg.max_drawdown_pct + self.cfg.cap_margin
        except Exception:
            self.conn.rollback()
            return True

    # ───────────────────── Market pricing helpers ─────────────────────

    def _last_price(self, symbol: str) -> float:
        try:
            p = float(self.client.last_trade_price(symbol) or 0.0)
            if p > 0:
                return p
            # Fallback: mid from best bid/ask
            bid = float(self.client.best_bid_price(symbol) or 0.0)
            ask = float(self.client.best_ask_price(symbol) or 0.0)
            if bid > 0 and ask > 0:
                return (bid + ask) / 2.0
            return 0.0
        except Exception:
            return 0.0

================================================================================
FILE: mm/core/christian.py
================================================================================
#===================================================================
# 🍁 A R I A N D E           bot version 6.1 file build 20250916.01
#===================================================================
# last update: 2025 | Sept. 16                  Production ready ❌
#===================================================================
# Christian
# mm/core/christian.py
#
# Stub implementation to prevent import errors.
# All methods are no-ops for simulation mode.
#
# [520] [741] [8]
#===================================================================
# 🔰 THE COMMANDER            ✖ PERSISTANT RUNTIME  ✖ MONIT MANAGED
#===================================================================

import logging

logger = logging.getLogger("ariadne.christian")

class Christian:
    """
    Accounting Manager - Stub Implementation
    
    This is a stub to prevent import errors. In simulation mode,
    all accounting is handled by other components. 
    """
    
    def __init__(self, client=None, config=None):
        self.client = client
        self.config = config
        logger.info("Christian (Accounting Manager) stub initialized")
    
    def record_trade(self, *args, **kwargs):
        """Record a completed trade - stub"""
        pass
    
    def record_fee(self, *args, **kwargs):
        """Record trading fees - stub"""
        pass
    
    def record_pnl(self, *args, **kwargs):
        """Record profit/loss - stub"""
        pass
    
    def get_ledger(self, *args, **kwargs):
        """Get accounting ledger - stub"""
        return {}
    
    def balance_books(self, *args, **kwargs):
        """Balance the books - stub"""
        return True
    
    def generate_report(self, *args, **kwargs):
        """Generate accounting report - stub"""
        return "Accounting report not available in simulation mode"
    
    def audit_positions(self, *args, **kwargs):
        """Audit current positions - stub"""
        return True
    
    def reconcile_balances(self, *args, **kwargs):
        """Reconcile balances - stub"""
        return True

================================================================================
FILE: mm/core/ash.py
================================================================================
#>> A R I A N D E v6
#>> last update: 2025 | Sept. 4
#>>
#>> ASH - Balance Cache Manager
#>> mm/core/ash.py
#>>
#>> Maintains real-time balance cache for Julius
#>> Updates every second from database and exchange
#>>
#>> Auth'd -> Commander
#>>
#>> [520] [741] [8]
#>>────────────────────────────────────────────────────────────────

# Build|20250904.01

import os
import sys
import json
import signal
import time
from typing import Dict, Any
from pathlib import Path
from decimal import Decimal
from dotenv import load_dotenv
import psycopg2
import psycopg2.extras

# Load environment variables
load_dotenv()

# Add parent directory to path
sys.path.append('/root/Echelon/valentrix')

from mm.utils.helpers.wintermute import (
    get_logger,
    now_pack,
    write_pid_file,
    cleanup_pid_file,
    get_db_connection,
    release_db_connection
)

from mm.conn.conn_kucoin import KucoinClient

# Fetch the current operating mode from Inara
def get_mode_safe() -> str: 
    try: 
        from mm.utils.helpers import inara 
        return inara.get_mode() 
    except Exception: 
        return "halted"

# Configuration
BALANCE_CACHE_PATH = "/root/Echelon/valentrix/mm/data/source/balances.json"
PID_FILE = "/root/Echelon/valentrix/mm/core/ash.pid"
LOG_FILE = "/root/Echelon/valentrix/mm/core/ash.log"
UPDATE_INTERVAL = 1  # seconds

# Global shutdown flag
shutdown_requested = False

# Logger
log = get_logger("ash", LOG_FILE)

# Signal handler
def signal_handler(signum, frame):
    global shutdown_requested
    log.info(f"[SHUTDOWN] Received signal {signum}")
    shutdown_requested = True

signal.signal(signal.SIGINT, signal_handler)
signal.signal(signal.SIGTERM, signal_handler)

class BalanceCache:
    def __init__(self):
        self.kucoin = KucoinClient()
        self.cycle_count = 0
    
    def get_simulation_balances(self) -> Dict:
        """Get balances from sim_balances table."""
        conn = get_db_connection()
        try:
            cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
            
            # Get all balances
            cur.execute("""
                SELECT asset, available, hold, (available + hold) as total
                FROM sim_balances
                WHERE (available + hold) > 0
            """)
            balances = {}
            for row in cur.fetchall():
                balances[row['asset']] = {
                    'available': float(row['available']),
                    'hold': float(row['hold']),
                    'total': float(row['total'])
                }
            
            # Get current prices for assets
            asset_values = {}
            total_equity = 0.0
            
            for asset, bal in balances.items():
                if asset == 'USDT':
                    asset_values[asset] = {
                        'native': bal['total'],
                        'usdt_value': bal['total']
                    }
                    total_equity += bal['total']
                else:
                    # Get price from tickstick
                    symbol = f"{asset}-USDT"
                    cur.execute("""
                        SELECT last FROM tickstick 
                        WHERE symbol = %s 
                        ORDER BY timestamp DESC 
                        LIMIT 1
                    """, (symbol,))
                    price_row = cur.fetchone()
                    
                    if price_row and price_row['last']:
                        price = float(price_row['last'])
                        usdt_value = bal['total'] * price
                        asset_values[asset] = {
                            'native': bal['total'],
                            'usdt_value': usdt_value
                        }
                        total_equity += usdt_value
                    else:
                        asset_values[asset] = {
                            'native': bal['total'],
                            'usdt_value': 0.0
                        }
            
            cur.close()
            
            return {
                'balances': balances,
                'asset_values': asset_values,
                'total_equity': total_equity
            }
            
        finally:
            release_db_connection(conn)
    
    def get_live_balances(self) -> Dict:
        """Get balances from KuCoin exchange."""
        try:
            # Get account balances
            raw_balances = self.kucoin.get_account_balances_detailed()
            
            balances = {}
            asset_values = {}
            total_equity = 0.0
            
            for asset, bal_info in raw_balances.items():
                if bal_info['available'] > 0 or bal_info['hold'] > 0:
                    balances[asset] = {
                        'available': bal_info['available'],
                        'hold': bal_info['hold'],
                        'total': bal_info['available'] + bal_info['hold']
                    }
                    
                    if asset == 'USDT':
                        asset_values[asset] = {
                            'native': balances[asset]['total'],
                            'usdt_value': balances[asset]['total']
                        }
                        total_equity += balances[asset]['total']
                    else:
                        # Get current price
                        symbol = f"{asset}-USDT"
                        price = self.kucoin.last_trade_price(symbol)
                        
                        if price and price > 0:
                            usdt_value = balances[asset]['total'] * price
                            asset_values[asset] = {
                                'native': balances[asset]['total'],
                                'usdt_value': usdt_value
                            }
                            total_equity += usdt_value
                        else:
                            asset_values[asset] = {
                                'native': balances[asset]['total'],
                                'usdt_value': 0.0
                            }
            
            return {
                'balances': balances,
                'asset_values': asset_values,
                'total_equity': total_equity
            }
            
        except Exception as e:
            log.error(f"Failed to get live balances: {e}")
            return {
                'balances': {},
                'asset_values': {},
                'total_equity': 0.0
            }
    
    def update_cache(self):
        """Update balance cache file."""
        self.cycle_count += 1
        
        try:
            mode = get_mode()
            tp = now_pack()
            
            # Get balances based on mode
            if mode == 'simulation':
                sim_data = self.get_simulation_balances()
                live_data = {'balances': {}, 'asset_values': {}, 'total_equity': 0.0}
            else:  # live
                sim_data = self.get_simulation_balances()  # Still track sim for comparison
                live_data = self.get_live_balances()
            
            # Build cache structure
            cache = {
                'version': '1.0',
                'updated_at': tp.iso,
                'updated_epoch_ms': tp.epoch_ms,
                'current_mode': mode,
                'simulation': {
                    'balances': sim_data['balances'],
                    'asset_values': sim_data['asset_values'],
                    'total_equity': sim_data['total_equity']
                },
                'live': {
                    'balances': live_data['balances'],
                    'asset_values': live_data['asset_values'],
                    'total_equity': live_data['total_equity']
                }
            }
            
            # Write atomically
            Path(BALANCE_CACHE_PATH).parent.mkdir(parents=True, exist_ok=True)
            temp_path = f"{BALANCE_CACHE_PATH}.tmp"
            
            with open(temp_path, 'w') as f:
                json.dump(cache, f, indent=2, default=str)
            
            os.rename(temp_path, BALANCE_CACHE_PATH)
            
            if self.cycle_count % 60 == 0:  # Log every minute
                log.info(f"Cache updated - Mode: {mode}, Sim equity: ${sim_data['total_equity']:.2f}, Live equity: ${live_data['total_equity']:.2f}")
                
        except Exception as e:
            log.error(f"Failed to update cache: {e}")
    
    def update_heartbeat(self):
        """Update heartbeat in database."""
        try:
            conn = get_db_connection()
            cur = conn.cursor()
            cur.execute("""
                INSERT INTO heartbeats (process_name, last_heartbeat, status, pid, cycle_count)
                VALUES ('ash', NOW(), 'caching', %s, %s)
                ON CONFLICT (process_name)
                DO UPDATE SET 
                    last_heartbeat = NOW(),
                    status = 'caching',
                    pid = %s,
                    cycle_count = %s
            """, (os.getpid(), self.cycle_count, os.getpid(), self.cycle_count))
            conn.commit()
            cur.close()
            release_db_connection(conn)
        except Exception as e:
            log.error(f"Failed to update heartbeat: {e}")
    
    def run(self):
        """Main process loop."""
        log.info(f"[INIT] ASH starting - updating every {UPDATE_INTERVAL}s")
        
        while not shutdown_requested:
            start = time.time()
            
            self.update_cache()
            
            # Update heartbeat every 30 cycles
            if self.cycle_count % 30 == 0:
                self.update_heartbeat()
            
            # Sleep for remainder of interval
            elapsed = time.time() - start
            sleep_time = max(UPDATE_INTERVAL - elapsed, 0.1)
            time.sleep(sleep_time)
        
        log.info("[SHUTDOWN] ASH shutting down")

if __name__ == "__main__":
    try:
        write_pid_file(PID_FILE)
        cache = BalanceCache()
        cache.run()
    finally:
        cleanup_pid_file(PID_FILE)

================================================================================
FILE: mm/core/helen.py
================================================================================
#>> A R I A N D E [v 6.1]
#>> last update: 2025 | Sept. 9                ❌ PRODUCTION READY
#>>
#>> Inventory Manager
#>> mm/core/helen.py
#>>
#>> Ensures orders are legitimate assets we own. 
#>> Approves orders via the proposals table.
#>> Manages assets.
#>>
#>> Auth'd -> Commander
#>>
#>> [520] [741] [8]        💫 PERSISTANT RUNTIME  ➰ MONIT MANAGED  
#>>────────────────────────────────────────────────────────────────

# Build|20250909.01

import os
import json
import logging
from dataclasses import dataclass
from typing import Dict, Optional, Tuple

import psycopg2

from mm.config import marcus
from mm.conn.conn_kucoin import KucoinClient
from mm.utils.helpers.wintermute import parse_symbol  # expects "BTC-USDT" -> ("BTC","USDT")

# Inara (mode gating). Safe fallbacks if helpers not present/newer.
try:
    from mm.utils.helpers.inara import current_mode, is_live_mode
except Exception:
    def current_mode() -> str:
        return getattr(marcus, "MODE", "simulation")
    def is_live_mode() -> bool:
        return current_mode() == "live"

DSN = os.getenv("PG_DSN", "dbname=ariadne user=postgres host=localhost")

logger = logging.getLogger("helen")
logger.setLevel(logging.INFO)


@dataclass
class HelenCfg:
    quote: str = marcus.QUOTE_CURRENCY  # e.g., "USDT"


class Helen:
    """
    Responsibilities
      • Phase 1 (vet): set proposals.invt_vet ← 'approved' | 'denied' based on inventory.
      • Phase 2 (finalize for SELL):
          - SIM: reserve by incrementing sim_positions.hold.
          - LIVE: no-op (exchange reserves), optional availability recheck.
      • Interfaces for Petra:
          - link_asset_hold_to_order(...): no-op in LIVE, SIM just logs (reservation already done).
          - on_cancel(...): SIM releases reservation (decrement sim_positions.hold); LIVE no-op.

    Notes
      • No hold_id is ever written to proposals (by design).
      • We use proposal_id as correlation to compute/rescind reserved qty.
      • get_positions(): used by main bot to read current inventory.
    """

    def __init__(self, client: Optional[KucoinClient] = None, cfg: HelenCfg = HelenCfg()):
        self.cfg = cfg
        self.client = client or KucoinClient()
        self.conn = psycopg2.connect(DSN)
        self.cur = self.conn.cursor()
        logger.info("Helen ready | mode=%s | quote=%s", current_mode(), self.cfg.quote)

    # ───────────────────────── Public API ─────────────────────────

    def get_positions(self) -> Dict[str, float]:
        """Return dict of { 'BTC-USDT': available_qty, ... }.
        SIM → from sim_positions (qty - hold); LIVE → from exchange balances (base assets)."""
        if is_live_mode():
            bals = self.client.get_account_balances_detailed()  # { 'BTC': {'available': x, 'hold': y}, ... }
            out: Dict[str, float] = {}
            for cur, v in (bals or {}).items():
                if cur == self.cfg.quote:
                    continue
                sym = f"{cur}-{self.cfg.quote}"
                out[sym] = float(v.get("available", 0.0))
            return out
        else:
            self.cur.execute("SELECT symbol, COALESCE(qty,0)::numeric, COALESCE(hold,0)::numeric FROM sim_positions")
            out: Dict[str, float] = {}
            for symbol, qty, hold in self.cur.fetchall():
                try:
                    avail = float(qty) - float(hold)
                except Exception:
                    avail = 0.0
                if avail > 0:
                    out[symbol] = avail
            return out

    def vet_inventory(self, proposal_id: int) -> str:
        """Phase 1: set invt_vet = 'approved' or 'denied' for SELL proposal."""
        prop = self._fetch_proposal(proposal_id)
        if not prop or prop["side"] != "sell":
            return "denied"

        ok = self._has_inventory(prop["symbol"], float(prop["size_intent"]))
        status = "approved" if ok else "denied"

        self.cur.execute("UPDATE proposals SET invt_vet = %s WHERE id = %s", (status, proposal_id))
        self.conn.commit()
        self._route_log(proposal_id, f"invt_vet.{status}", {"symbol": prop["symbol"], "size": float(prop["size_intent"])})
        return status

    def finalize_for_sell(self, proposal_id: int) -> bool:
        """
        Phase 2 for Petra-originated proposals: after Lamar sees all vets approved,
        he calls Helen to reserve inventory before signaling Petra.
        SIM: increment sim_positions.hold; LIVE: no DB hold (exchange will reserve at order).
        Returns True if reserved/validated OK.
        """
        prop = self._fetch_proposal(proposal_id, for_finalize=True)
        if not prop or prop["side"] != "sell":
            self._route_log(proposal_id, "invt_finalize.failed", {"reason": "not_found_or_wrong_side"})
            return False

        symbol = prop["symbol"]
        size = float(prop["size_intent"])

        if is_live_mode():
            # Re-validate availability on exchange
            if not self._has_inventory(symbol, size):
                self._route_log(proposal_id, "invt_finalize.denied", {"reason": "insufficient_live_bal"})
                return False
            # Optional: set status=approved here or let Lamar do it; we only log.
            self._route_log(proposal_id, "invt_finalize.live_ok", {"symbol": symbol, "size": size})
            return True

        # SIM mode: place a reservation by bumping sim_positions.hold atomically
        base, _ = parse_symbol(symbol)
        if not self._reserve_sim(symbol, size):
            self._route_log(proposal_id, "invt_finalize.denied", {"reason": "insufficient_sim_bal"})
            return False

        # Optional: status update to 'approved' can be Lamar's responsibility; keep idempotent if present.
        try:
            self.cur.execute("UPDATE proposals SET status='approved' WHERE id=%s", (proposal_id,))
            self.conn.commit()
        except Exception:
            self.conn.rollback()
        self._route_log(proposal_id, "invt_finalize.approved", {"symbol": symbol, "size": size})
        return True

    # Called by Petra after successful order placement
    def link_asset_hold_to_order(self, hold_id: Optional[str] = None, order_id: Optional[str] = None,
                                 correlation_id: Optional[str] = None) -> None:
        """
        SIM: reservation was already made; just log the linkage.
        LIVE: exchange manages reservation; just log.
        Supports both legacy signature (hold_id, order_id) and correlation_id="proposal:{id}".
        """
        info = {"hold_id": hold_id, "order_id": order_id, "correlation_id": correlation_id}
        pid = self._pid_from_corr(correlation_id)
        if pid:
            self._route_log(pid, "invt_linked", info)
        else:
            logger.info("Helen link (no pid): %s", info)

    # Called by Petra on placement failure to unwind a SIM reservation
    def on_cancel(self, order_id: Optional[str] = None, reason: str = "placement_failed",
                  correlation_id: Optional[str] = None) -> None:
        pid = self._pid_from_corr(correlation_id)
        prop = self._fetch_proposal(pid) if pid else None
        if not prop or prop["side"] != "sell":
            self._route_log(pid or -1, "invt_cancel.ignored", {"reason": "no_sell_proposal", "order_id": order_id})
            return

        symbol = prop["symbol"]
        size = float(prop["size_intent"])

        if not is_live_mode():
            self._release_sim(symbol, size)

        # Mark proposal failed (idempotent)
        try:
            self.cur.execute("UPDATE proposals SET status='failed' WHERE id=%s", (pid,))
            self.conn.commit()
        except Exception:
            self.conn.rollback()

        self._route_log(pid, "invt_cancel", {"order_id": order_id, "reason": reason, "symbol": symbol, "size": size})

    # ───────────────────────── Internals ─────────────────────────

    def _fetch_proposal(self, pid: Optional[int], for_finalize: bool = False) -> Optional[Dict]:
        if not pid:
            return None
        if for_finalize:
            # ensure all vets are approved (risk/bank/invt)
            self.cur.execute("""
                SELECT id, symbol, side, price_intent, size_intent, risk_vet, bank_vet, invt_vet
                FROM proposals WHERE id=%s
            """, (pid,))
            row = self.cur.fetchone()
            if not row:
                return None
            d = dict(zip(
                ("id","symbol","side","price_intent","size_intent","risk_vet","bank_vet","invt_vet"), row
            ))
            if (d.get("risk_vet") != "approved") or (d.get("bank_vet") != "approved") or (d.get("invt_vet") != "approved"):
                return None
            return d
        else:
            self.cur.execute("""
                SELECT id, symbol, side, price_intent, size_intent
                FROM proposals WHERE id=%s
            """, (pid,))
            row = self.cur.fetchone()
            return dict(zip(("id","symbol","side","price_intent","size_intent"), row)) if row else None

    def _has_inventory(self, symbol: str, size: float) -> bool:
        """Check available inventory for SELL."""
        base, quote = parse_symbol(symbol)

        if is_live_mode():
            bals = self.client.get_account_balances_detailed()  # LIVE balances per currency
            have = float((bals.get(base) or {}).get("available", 0.0))
            return have >= size

        # SIM: available = qty - hold
        self.cur.execute("SELECT COALESCE(qty,0)::numeric, COALESCE(hold,0)::numeric FROM sim_positions WHERE symbol=%s", (symbol,))
        row = self.cur.fetchone()
        if not row:
            return False
        qty, hold = [float(x or 0.0) for x in row]
        return (qty - hold) >= size

    def _reserve_sim(self, symbol: str, size: float) -> bool:
        """Atomically reserve inventory in SIM by bumping hold; ensure no negative available."""
        # Try optimistic update guarded by availability check in one statement
        self.cur.execute("""
            UPDATE sim_positions
               SET hold = hold + %s
             WHERE symbol = %s
               AND (COALESCE(qty,0) - COALESCE(hold,0)) >= %s
            RETURNING hold
        """, (size, symbol, size))
        ok = self.cur.fetchone() is not None
        if ok:
            self.conn.commit()
        else:
            self.conn.rollback()
        return ok

    def _release_sim(self, symbol: str, size: float) -> None:
        """Release a previous reservation in SIM (best-effort; never go below zero)."""
        self.cur.execute("""
            UPDATE sim_positions
               SET hold = GREATEST(0, COALESCE(hold,0) - %s)
             WHERE symbol = %s
        """, (size, symbol))
        self.conn.commit()

    def _route_log(self, proposal_id: int, status: str, info: Dict) -> None:
        """Append to proposal_router_log for observability."""
        try:
            self.cur.execute("""
                INSERT INTO proposal_router_log (proposal_id, timestamp, status, details)
                VALUES (%s, NOW(), %s, %s)
            """, (proposal_id, status, json.dumps(info)))
            self.conn.commit()
        except Exception as e:
            self.conn.rollback()
            logger.warning("router log insert failed p=%s: %s", proposal_id, e)

    @staticmethod
    def _pid_from_corr(correlation_id: Optional[str]) -> Optional[int]:
        """Parse 'proposal:{id}' → id."""
        if not correlation_id:
            return None
        s = str(correlation_id).strip()
        if s.startswith("proposal:"):
            try:
                return int(s.split(":", 1)[1])
            except Exception:
                return None
        return None

================================================================================
FILE: mm/core/alec.py
================================================================================
#>> A R I A N D E v6
#>> last update: 2025 | Sept. 5
#>>
#>> CANCELLATION MANAGER
#>> mm/core/alec.py
#>>
#>> Responsible for cancelling orders  
#>> Monitors for stale orders and stagnant pricing    
#>> Enforces compliance with configuration parameters
#>>
#>> Auth'd -> Commander
#>>
#>> [520] [741] [8]      
#>>────────────────────────────────────────────────────────────────

# Build|20250905.01

import logging
import time
from typing import Dict, List, Tuple, Optional
from datetime import datetime, timedelta
import os
import importlib
import smtplib
import ssl
import uuid
from email.message import EmailMessage
from email.utils import formataddr
from zoneinfo import ZoneInfo

# third-party imports
from dotenv import load_dotenv

# local application imports
import mm.config.marcus as marcus

# load env for this process
load_dotenv("mm/data/secrets/.env")

# Import config parameters
from mm.config.marcus import (
    ORDER_REFRESH_SECONDS, STALE_ORDER_HOURS,
    MAX_SPREAD_DRIFT_PCT, PRICE_STAGNANT_MINUTES,
    QUOTE_CURRENCY
)

# ── Logger Setup ──────────────────────────────────────────────────────
logger = logging.getLogger('ariadne.termination')

class Alec:
    """
    Manages order cancellations based on various criteria
    """
    
    def __init__(self, trading_client):
        """
        Initialize with trading client reference
        
        Args:
            trading_client: Either KucoinClient or SimClient
        """
        self.client = trading_client
        self.logger = logger
        self.order_history: Dict[str, Dict] = {}  # order_id -> order details
        self.price_history: Dict[str, List[Tuple[float, float]]] = {}  # symbol -> [(timestamp, price)]
        self.cancellation_stats: Dict[str, int] = {
            'stale': 0,
            'stagnant': 0,
            'drift': 0,
            'risk': 0,
            'manual': 0
        }
        
    def send_email(subject: str, status: str, title: str, message: str) -> str:

        importlib.reload(marcus)
        if not bool(getattr(marcus, "ALERT_EMAIL_ENABLED", False)):
            return "disabled"
        if str(getattr(marcus, "ALERT_EMAIL_ENCRYPT", "SSL")).upper() != "SSL":
            return "Simple Mail Transfer Protocol not established. No conn."

        host = getattr(marcus, "ALERT_EMAIL_SMTP_SERVER", None)
        port = getattr(marcus, "ALERT_EMAIL_SMTP_PORT", None)
        recipient = getattr(marcus, "ALERT_EMAIL_RECIPIENT", None)

        USERCODE = "ALE"  # hardcode per file

        # ---- Edit Sender Info (per file) ----
        user = os.getenv(f"{USERCODE}_USR")
        pwd = os.getenv(f"{USERCODE}_PWD")
        sender_email = user
        sender_name = os.getenv(f"{USERCODE}_NAME")
        # -------------------------------------

        # status color map
        STATUS_COLORS = {
            "STATCON3": "#F1C232",	# on the first missing heartbeat 
            "STATCON2": "#E69138",	# on the second missing heartbeat
            "STATCON1": "#CC0000",	# on the third missing heartbeat
            "SIGCON1": 	"#FB6D8B",	# Process never started
            "OPSCON5": 	"#F5F5F5",	# Normal, all systems nominal
            "OPSCON1": 	"#990000",	# Issues detected
        }
        status_text = str(status).upper()
        status_color = STATUS_COLORS.get(status_text, "#BE644C")

        msg = EmailMessage()
        domain = sender_email.split("@")[1] if "@" in sender_email else "hodlcorp.io"
        msg_id = f"<{uuid.uuid4()}@{domain}>"
        msg["Message-ID"] = msg_id
        msg["From"] = formataddr((sender_name, sender_email))
        msg["To"] = recipient
        msg["Subject"] = subject
        msg["X-Priority"] = "1"
        msg["X-MSMail-Priority"] = "High"
        msg["Importance"] = "High"

        # footer fields
        now_tz = datetime.now(ZoneInfo("America/Toronto"))
        sent_str = now_tz.strftime("%Y-%m-%d %H:%M:%S America/Toronto")
        epoch_ms = int(now_tz.timestamp() * 1000)
        mid_clean = msg_id.strip("<>").split("@", 1)[0]

        # full HTML body (single block)
        html_body = f"""
    <div style="font-family: monospace;">
      <table role="presentation" width="100%" height="20px" cellpadding="8px" cellspacing="0" border="0">
        <!-- Top Banner -->
        <tbody><tr style="font-family: Georgia, 'Times New Roman', Times, serif;font-size:20px;font-weight:600;background-color:#333;">
          <td align="left" style="color:#EFEFEF;letter-spacing:12px;">INTCOMM</td>
          <td align="right" style="color:{status_color};letter-spacing:4px;">{status_text}</td>
        </tr>

        <!-- Message Title -->
        <tr width="100%" cellpadding="6px" style="font-family: Tahoma, Geneva, sans-serif;text-align:left;font-size:14px;font-weight:600;color:#333;">
          <td colspan="2">
            {title}
          </td>
        </tr>

        <!-- Message Content -->
        <tr width="100%" cellpadding="6px" style="font-family: Tahoma, Geneva, sans-serif;text-align:left;font-size:11px;font-weight:400;line-height:1.5;color:#333;">
          <td colspan="2">
            {message}
          </td>
        </tr>

        <!-- UNUSED SPACER ROW -->
        <tr width="100%" height="25px"><td colspan="2"> </td></tr>
      </tbody></table>

      <!-- Footer -->
      <table role="presentation" width="400px" height="20px" cellpadding="4" cellspacing="0" border="0" style="font-family: Tahoma, Geneva, sans-serif;">
        <!-- DOCINT -->
        <tbody><tr style="background-color:#333;">
          <td colspan="2" style="color:#efefef;font-size:12px;font-weight:600;">DOCINT</td>
        </tr>

        <tr style="background-color:#E9E9E5;">
          <td width="30px" style="color:#333;font-size:10px;font-weight:600;">SENT</td>

          <td width="10px" style="color:#333;font-size:10px;font-weight:600;">→</td>
          <td style="color:#333;font-size:11px;font-weight:400;">{sent_str}</td>
        </tr>

        <tr style="background-color:#F2F2F0;">
          <td width="30px" style="color:#333;font-size:10px;font-weight:600;">EPOCH</td>
          <td width="10px" style="color:#333;font-size:10px;font-weight:600;">→</td>
          <td style="color:#333;font-size:11px;font-weight:400;">{epoch_ms} (ms since 1970/01/01 0:00 UTC)</td>
        </tr>

        <tr style="background-color:#E9E9E5;">
          <td width="30px" style="color:#333;font-size:10px;font-weight:600;">m.ID</td>
          <td width="10px" style="color:#333;font-size:10px;font-weight:600;">→</td>
          <td style="color:#333;font-size:11px;font-weight:400;">{mid_clean}</td>
        </tr>
      </tbody></table>
    </div>
    """

        msg.add_alternative(html_body, subtype="html")

        ctx = ssl.create_default_context()
        with smtplib.SMTP_SSL(host, port, context=ctx, timeout=10) as s:
            if user and pwd:
                s.login(user, pwd)
            s.send_message(msg)

        return msg_id
    
    def cancel_stale_orders(self) -> List[Tuple[str, str]]:
        """
        Cancel orders that are too old or need repricing
        
        Returns:
            List of (order_id, reason) tuples for cancelled orders
        """
        cancelled = []
        
        try:
            # Get all active orders
            all_orders = self.client.get_orders(status='active')
            
            for order in all_orders:
                order_id = order.get('id')
                symbol = order.get('symbol')
                
                # Check various cancellation criteria
                cancel_reason = self._should_cancel_order(order)
                
                if cancel_reason:
                    success = self._cancel_order(order_id, symbol)
                    if success:
                        cancelled.append((order_id, cancel_reason))
                        self.cancellation_stats[self._categorize_reason(cancel_reason)] += 1
                        
                # Update order tracking
                self.order_history[order_id] = {
                    'symbol': symbol,
                    'side': order.get('side'),
                    'price': float(order.get('price', 0)),
                    'size': float(order.get('size', 0)),
                    'created_at': order.get('createdAt', time.time()),
                    'last_checked': time.time()
                }
                
        except Exception as e:
            self.logger.error(f"Failed to check stale orders: {e}")
            
        if cancelled:
            self.logger.info(f"Cancelled {len(cancelled)} stale orders")
            
        return cancelled
    
    def cancel_all_orders(self) -> int:
        """
        Emergency cancel all open orders
        
        Returns:
            Number of orders cancelled
            
        This entire function is useless. It requires a manual trigger by me, which i dont have. Futher, there is no point in
        sending myself an alert email for something i have to manually do. Fuck Ai. I dont have time to fix this now, but seeing as
        I am the trigger, I can leave it in with no risk. Hence this note. I'll deal with it later.
        """
        cancelled_count = 0
        
        try:
            all_orders = self.client.get_orders(status='active')
            
            self.logger.warning(f"EMERGENCY: Cancelling all {len(all_orders)} open orders")
            
            for order in all_orders:
                order_id = order.get('id')
                symbol = order.get('symbol')
                
                if self._cancel_order(order_id, symbol):
                    cancelled_count += 1
                    
            try:
                send_email(
                    subject="[ STATCON3 ] Alma executed a corrective exit.",
                    status="STATCON3",
                    title="API Connection to KuCoin Failed",
                    message=f"<p><b>Alma was unable to fetch data from KuCoin via the API, the reported error was:</b><br><i>{e}</i></p><p>This exit was coded in to prevent stalling, infinite loops, and other outcomes that prevent Monit from knowing Alma is stuck. Monit <b><i>should</i></b> restart Alma.</p><p>Please ensure that this is the case by logging onto the server and using the command:<br><i>sudo monit status alma</i></p>",
                )
            except:
                pass
            
        except Exception as e:
            self.logger.error(f"Failed to cancel all orders: {e}")
            try:
                send_email(
                    subject="[ STATCON1 ] Alma executed a corrective exit.",
                    status="STATCON1",
                    title="API Connection to KuCoin Failed",
                    message=f"<p><b>Alma was unable to fetch data from KuCoin via the API, the reported error was:</b><br><i>{e}</i></p><p>This exit was coded in to prevent stalling, infinite loops, and other outcomes that prevent Monit from knowing Alma is stuck. Monit <b><i>should</i></b> restart Alma.</p><p>Please ensure that this is the case by logging onto the server and using the command:<br><i>sudo monit status alma</i></p>",
                )
            except:
                pass
            
        return cancelled_count
    
    def cancel_orders_for_pair(self, symbol: str, reason: str = "Manual request") -> int:
        """
        Cancel all orders for a specific trading pair
        
        Args:
            symbol: Trading pair symbol
            reason: Cancellation reason
            
        Returns:
            Number of orders cancelled
        """
        cancelled_count = 0
        
        try:
            orders = self.client.get_orders(symbol=symbol, status='active')
            
            for order in orders:
                order_id = order.get('id')
                if self._cancel_order(order_id, symbol):
                    cancelled_count += 1
                    
            if cancelled_count > 0:
                self.logger.info(f"Cancelled {cancelled_count} orders for {symbol}: {reason}")
                
        except Exception as e:
            self.logger.error(f"Failed to cancel orders for {symbol}: {e}")
            
        return cancelled_count
    
    def cleanup_old_positions(self, positions: Dict[str, float], 
                            hold_hours: int = 24) -> List[str]:
        """
        Identify positions held too long
        
        Args:
            positions: Current positions
            hold_hours: Maximum hold time
            
        Returns:
            List of symbols that should be liquidated
        """
        old_positions = []
        
        # This would need integration with trade history
        # For now, returning empty list
        # TODO: Track position entry times
        
        return old_positions
    
    def _should_cancel_order(self, order: Dict) -> Optional[str]:
        """
        Determine if an order should be cancelled
        
        Args:
            order: Order details
            
        Returns:
            Cancellation reason or None
        """
        order_id = order.get('id')
        symbol = order.get('symbol')
        created_at = order.get('createdAt', 0)
        price = float(order.get('price', 0))
        side = order.get('side')
        
        # Check if order is too old
        if created_at > 0:
            age_seconds = time.time() - (created_at / 1000)  # Convert ms to seconds

            # Refresh threshold (short-term)
            if age_seconds > ORDER_REFRESH_SECONDS:
                return f"stale - {age_seconds/3600:.1f} hours old"

            # Absolute cutoff (long-term)
            if age_seconds > (STALE_ORDER_HOURS * 3600):
                return f"stale - exceeded {STALE_ORDER_HOURS} hour limit"
        
        # Check for price drift
        current_market = self._get_current_market(symbol)
        if current_market:
            mid_price = (current_market['bid'] + current_market['ask']) / 2
            
            if mid_price > 0:
                if side == 'buy':
                    # Buy order should be below mid
                    expected_price = mid_price * 0.995  # Rough estimate
                    drift = abs(price - expected_price) / expected_price
                else:
                    # Sell order should be above mid
                    expected_price = mid_price * 1.005
                    drift = abs(price - expected_price) / expected_price
                
                if drift > MAX_SPREAD_DRIFT_PCT:
                    return f"drift - {drift:.1%} from optimal"
        
        # Check for stagnant prices
        if self._is_price_stagnant(symbol):
            return "stagnant - price not moving"
        
        return None
    
    def _cancel_order(self, order_id: str, symbol: str) -> bool:
        """
        Execute order cancellation
        
        Args:
            order_id: Order ID to cancel
            symbol: Trading pair symbol
            
        Returns:
            True if successful
        """
        try:
            self.client.cancel_order(order_id)
            self.logger.debug(f"Cancelled order {order_id} for {symbol}")
            
            # Remove from tracking
            if order_id in self.order_history:
                del self.order_history[order_id]
                
            return True
            
        except Exception as e:
            self.logger.error(f"Failed to cancel order {order_id}: {e}")
            return False
    
    def _get_current_market(self, symbol: str) -> Optional[Dict]:
        """
        Get current market data for symbol
        
        Args:
            symbol: Trading pair symbol
            
        Returns:
            Market data dict or None
        """
        try:
            tickers = self.client.get_ticker()
            for ticker in tickers:
                if ticker.get('symbol') == symbol:
                    return {
                        'bid': float(ticker.get('buy', 0)),
                        'ask': float(ticker.get('sell', 0)),
                        'last': float(ticker.get('last', 0))
                    }
        except Exception as e:
            self.logger.error(f"Failed to get market data: {e}")
            
        return None
    
    def _is_price_stagnant(self, symbol: str) -> bool:
        """
        Check if price has been stagnant
        
        Args:
            symbol: Trading pair symbol
            
        Returns:
            True if price is stagnant
        """
        if symbol not in self.price_history:
            return False
            
        history = self.price_history[symbol]
        if len(history) < 10:
            return False
            
        # Check price movement over last N minutes
        cutoff = time.time() - (PRICE_STAGNANT_MINUTES * 60)
        recent_prices = [price for ts, price in history if ts > cutoff]
        
        if len(recent_prices) < 2:
            return False
            
        # Calculate price range
        price_range = max(recent_prices) - min(recent_prices)
        avg_price = sum(recent_prices) / len(recent_prices)
        
        if avg_price > 0:
            volatility = price_range / avg_price
            # Consider stagnant if less than 0.1% movement
            return volatility < 0.001
            
        return False
    
    def update_price_tracking(self, symbol: str, price: float):
        """
        Update price history for stagnation detection
        
        Args:
            symbol: Trading pair symbol
            price: Current price
        """
        if symbol not in self.price_history:
            self.price_history[symbol] = []
            
        self.price_history[symbol].append((time.time(), price))
        
        # Keep only last hour
        cutoff = time.time() - 3600
        self.price_history[symbol] = [
            (ts, p) for ts, p in self.price_history[symbol] if ts > cutoff
        ]
    
    def get_cancellation_report(self) -> Dict:
        """
        Get cancellation statistics
        
        Returns:
            Dict with cancellation metrics
        """
        total_cancellations = sum(self.cancellation_stats.values())
        
        return {
            'total_cancellations': total_cancellations,
            'by_reason': self.cancellation_stats.copy(),
            'tracked_orders': len(self.order_history),
            'monitored_symbols': len(self.price_history),
            'oldest_order_age': self._get_oldest_order_age()
        }
    
    def _get_oldest_order_age(self) -> float:
        """Get age of oldest tracked order in hours"""
        if not self.order_history:
            return 0
            
        oldest = min(o['created_at'] for o in self.order_history.values())
        return (time.time() - oldest) / 3600
    
    def _categorize_reason(self, reason: str) -> str:
        """Categorize cancellation reason for stats"""
        if 'stale' in reason:
            return 'stale'
        elif 'drift' in reason:
            return 'drift'
        elif 'stagnant' in reason:
            return 'stagnant'
        elif 'risk' in reason.lower():
            return 'risk'
        else:
            return 'manual'

================================================================================
FILE: mm/core/malcolm.py
================================================================================
#===================================================================
# ?? A R I A N D E           bot version 6.1 file build 20250917.01
#===================================================================
# last update: 2025 | Sept. 17                  Production ready ?
#===================================================================
# Malcolm - Purchasing Manager
# mm/core/malcolm.py
#
# Responsible for entering buy orders 
# Accepts filtered pairs from Dr. Calvin
# Honors balances and risk thresholds
#
# [520] [741] [8]
#===================================================================
# ?? THE COMMANDER            ? PERSISTANT RUNTIME  ? MONIT MANAGED
#===================================================================


import os
import json
import time
import select
import signal
import pathlib
import logging
import logging.handlers
from dataclasses import dataclass
from typing import Optional, Dict, Any

import psycopg2

from mm.config import marcus
from mm.core.grayson import RiskOps
from mm.core.julius import Julius
from mm.conn.conn_kucoin import KucoinClient
from mm.utils.tqt import andi

# Inara helpers (fallbacks keep Malcolm runnable if inara.py is older)
try:
    from mm.utils.helpers.inara import current_mode, can_place_orders, is_live_mode
except Exception:
    def current_mode() -> str:
        return getattr(marcus, "MODE", "simulation")
    def can_place_orders() -> bool:
        return current_mode() in ("live", "simulation", "shadow")
    def is_live_mode() -> bool:
        return current_mode() == "live"

# ────────────────────────── Config / Paths ────────────────────────────
DSN = os.getenv("PG_DSN", "dbname=ariadne user=postgres host=localhost")
CHANNEL_READY  = "proposals.ready.malcolm"
CHANNEL_DENIED = "proposals.denied.malcolm"
PROCESS_NAME   = "malcolm"

LOG_PATH = pathlib.Path("mm/logs/malcolm.log")
PID_PATH = pathlib.Path("mm/utils/soc/malcolm.pid")

HEARTBEAT_SEC = 5
LOOP_SLEEP    = 0.25

logger = logging.getLogger("malcolm")
logger.setLevel(logging.INFO)

def _setup_logging():
    LOG_PATH.parent.mkdir(parents=True, exist_ok=True)
    fh = logging.handlers.RotatingFileHandler(LOG_PATH, maxBytes=5_000_000, backupCount=3)
    fh.setFormatter(logging.Formatter('[%(asctime)s] %(levelname)s %(message)s'))
    logger.addHandler(fh)
    ch = logging.StreamHandler()
    ch.setFormatter(logging.Formatter('%(message)s'))
    logger.addHandler(ch)

def _write_pid():
    PID_PATH.parent.mkdir(parents=True, exist_ok=True)
    PID_PATH.write_text(str(os.getpid()))

@dataclass
class MalcolmCfg:
    quote: str = marcus.QUOTE_CURRENCY
    min_trade: float = getattr(marcus, "MIN_TRADE_SIZE", 10.0)

class Malcolm:
    """
    BUY Originator:
      - Maintains two DB connections (LISTEN vs ops)
      - Reacts to proposals.ready.malcolm / proposals.denied.malcolm
      - On READY: place limit order (mode-gated); link/unwind holds via Julius (sim), queue to Andi
      - On DENIED/EXPIRED: log and stop progression
      - Generates lightweight buy proposals opportunistically (stub; DrCalvin usually drives)
    """

    def __init__(self, client: Optional[KucoinClient] = None, cfg: MalcolmCfg = MalcolmCfg()):
        _setup_logging()
        _write_pid()

        self.cfg = cfg
        self.client = client or KucoinClient()
        self.julius = Julius(self.client)
        self.grayson = RiskOps()

        # LISTEN connection (autocommit ON)
        self.listen_conn = psycopg2.connect(DSN)
        self.listen_conn.set_session(autocommit=True)
        self.listen_cur = self.listen_conn.cursor()
        self.listen_cur.execute(f"LISTEN {CHANNEL_READY};")
        self.listen_cur.execute(f"LISTEN {CHANNEL_DENIED};")

        # OPS connection (for queries/writes)
        self.ops_conn = psycopg2.connect(DSN)
        self.ops_cur  = self.ops_conn.cursor()

        self.running = False
        signal.signal(signal.SIGINT,  self._sig_term)
        signal.signal(signal.SIGTERM, self._sig_term)
        try:
            signal.signal(signal.SIGHUP, self._sig_hup)
        except Exception:
            pass

        logger.info("Malcolm initialized | mode=%s | quote=%s", current_mode(), self.cfg.quote)

    # ─────────────────────────── Main loop ────────────────────────────
    def run_forever(self):
        self.running = True
        last_hb = 0.0
        cycle = 0

        logger.info("Malcolm loop starting…")
        while self.running:
            now = time.time()
            if now - last_hb >= HEARTBEAT_SEC:
                self._heartbeat(cycle)
                last_hb = now

            # Non-blocking wait for notifications
            r, _, _ = select.select([self.listen_conn], [], [], LOOP_SLEEP)
            if r:
                self.listen_conn.poll()
                while self.listen_conn.notifies:
                    note = self.listen_conn.notifies.pop(0)
                    self._route_notify(note.channel, note.payload)

            # Opportunistic proposal generation (kept light; Lamar/DrCalvin do the heavy lifting)
            try:
                self._maybe_generate_proposals()
            except Exception as e:
                logger.debug("proposal-gen skipped: %s", e)

            cycle += 1

        logger.info("Malcolm loop stopped.")

    # ────────────────────── Notifications routing ─────────────────────
    def _route_notify(self, channel: str, payload: str):
        data = self._parse_payload(payload)

        if channel == CHANNEL_READY:
            self._on_ready(data)
        elif channel == CHANNEL_DENIED:
            self._on_denied(data)
        else:
            logger.debug("Ignoring channel=%s payload=%r", channel, payload)

    def _parse_payload(self, payload: str) -> Dict[str, Any]:
        try:
            return json.loads(payload)
        except Exception:
            d = {"raw": payload}
            s = payload.strip()
            if s.startswith("id:"):
                try:
                    d["proposal_id"] = int(s.split(":", 1)[1])
                except Exception:
                    pass
            return d

    # ──────────────────────── Handlers ────────────────────────────────
    def _on_ready(self, data: Dict[str, Any]):
        pid = data.get("proposal_id")
        if not pid:
            logger.error("READY missing proposal_id: %r", data)
            return

        prop = self._fetch_proposal(pid)
        if not prop:
            logger.error("proposal %s not found or not in APPROVED/READY state", pid)
            return

        mode = current_mode()
        # Gate: if mode doesn't allow real placement, just record a shadow finalize
        if not can_place_orders() or mode in ("shadow", "halted", "maintenance", "drain"):
            logger.info("[mode:%s] would BUY %s @%s x%s (shadow only)",
                        mode, prop["symbol"], prop["price_intent"], prop["size_intent"])
            self._mark_finalized(pid, ghost=True)
            self._log_route(pid, status="shadow-finalized", info={"mode": mode})
            return

        # Risk sanity (light; full vet happened upstream)
        if not self._risk_ok(prop):
            self._fail_proposal(pid, reason="risk_blocked")
            return

        # Place limit order on exchange
        try:
            order_id = self.client.create_limit_order(
                symbol=prop["symbol"],
                side="buy",
                price=float(prop["price_intent"]),
                size=float(prop["size_intent"]),
            )
        except Exception as e:
            logger.error("order placement failed proposal=%s: %s", pid, e)
            # Unwind any sim hold via Julius (live mode → no-op)
            self._safe_julius_cancel(reason="placement_failed", proposal_id=pid)
            self._fail_proposal(pid, reason="placement_failed")
            return

        # Link hold → order in SIM (live reserves are internal to exchange)
        self._safe_julius_link(order_id=order_id, proposal_id=pid)

        # Persist order intent via Andi (TQT)
        self._safe_andi_queue(
            proposal_id=pid,
            symbol=prop["symbol"],
            side="buy",
            price=float(prop["price_intent"]),
            size=float(prop["size_intent"]),
            order_id=order_id,
            mode=mode,
            origin="malcolm",
        )

        # finalize
        self._mark_finalized(pid)
        self._log_route(pid, status="finalized", info={"order_id": order_id})

    def _on_denied(self, data: Dict[str, Any]):
        pid = data.get("proposal_id")
        typ = data.get("type", "denied")  # "denied" | "expired"
        if not pid:
            logger.error("DENIED missing proposal_id: %r", data)
            return
        self._log_route(pid, status=typ, info=data)
        # no further action

    # ─────────────────────── Proposal plumbing ────────────────────────
    def _fetch_proposal(self, pid: int) -> Optional[Dict[str, Any]]:
        self.ops_cur.execute("""
            SELECT id, symbol, side, price_intent, size_intent, status
            FROM proposals
            WHERE id = %s AND side = 'buy' AND status IN ('approved','ready')
        """, (pid,))
        row = self.ops_cur.fetchone()
        if not row:
            return None
        keys = ("id", "symbol", "side", "price_intent", "size_intent", "status")
        return dict(zip(keys, row))

    def _mark_finalized(self, pid: int, ghost: bool = False):
        new_status = "shadow_finalized" if ghost else "finalized"
        self.ops_cur.execute("UPDATE proposals SET status = %s WHERE id = %s", (new_status, pid))
        self.ops_conn.commit()

    def _fail_proposal(self, pid: int, reason: str):
        self.ops_cur.execute("UPDATE proposals SET status = 'failed' WHERE id = %s", (pid,))
        self.ops_conn.commit()
        self._log_route(pid, status="failed", info={"reason": reason})

    def _log_route(self, pid: int, status: str, info: Dict[str, Any]):
        try:
            self.ops_cur.execute("""
                INSERT INTO proposal_router_log (proposal_id, timestamp, status, details)
                VALUES (%s, NOW(), %s, %s)
            """, (pid, status, json.dumps(info)))
            self.ops_conn.commit()
        except Exception as e:
            logger.warning("router log insert failed p=%s: %s", pid, e)

    # ────────────────────────── Helpers ───────────────────────────────
    def _risk_ok(self, prop: Dict[str, Any]) -> bool:
        try:
            notional = float(prop["price_intent"]) * float(prop["size_intent"])
            if notional < float(self.cfg.min_trade):
                return False
            # Provide equity externally if you want stricter checks; use 0.0 here as placeholder
            return self.grayson.can_trade_pair(prop["symbol"], 0.0)
        except Exception:
            return True

    def _maybe_generate_proposals(self):
        """
        Placeholder: normally call DrCalvin to rank & insert BUY intents.
        Respect modes; skip in maintenance/halted.
        """
        m = current_mode()
        if m in ("halted", "maintenance"):
            return
        # Intentionally left minimal; Lamar handles routing → vets → approvals.

    def _safe_andi_queue(self, **kw):
        try:
            andi.queue_order(**kw)
        except Exception as e:
            logger.warning("andi.queue_order warn: %s", e)

    def _safe_julius_link(self, order_id: str, proposal_id: int):
        """
        Prefer correlation-based linking (no hold_id in proposals).
        Falls back quietly if your Julius doesn't yet support correlation_id.
        """
        try:
            # New API (recommended): correlation by proposal
            if hasattr(self.julius, "link_hold_to_order"):
                # try modern signature first
                try:
                    self.julius.link_hold_to_order(order_id=order_id, correlation_id=f"proposal:{proposal_id}")
                    return
                except TypeError:
                    pass
            # Optional alternates your Julius may expose
            if hasattr(self.julius, "link_order_to_proposal"):
                self.julius.link_order_to_proposal(proposal_id=proposal_id, order_id=order_id)
        except Exception as e:
            logger.debug("julius link skipped: %s", e)

    def _safe_julius_cancel(self, reason: str, proposal_id: int):
        try:
            if hasattr(self.julius, "on_cancel"):
                try:
                    self.julius.on_cancel(reason=reason, correlation_id=f"proposal:{proposal_id}")
                except TypeError:
                    # legacy signature: on_cancel(order_id=None, reason, hold_id=None)
                    self.julius.on_cancel(order_id=None, reason=reason, hold_id=None)
        except Exception as e:
            logger.debug("julius on_cancel skipped: %s", e)

    # ───────────────────────── Heartbeats ─────────────────────────────
    def _heartbeat(self, cycle_count: int):
        try:
            self.ops_cur.execute("""
                INSERT INTO heartbeats (process_name, last_heartbeat, status, pid, cycle_count)
                VALUES (%s, NOW(), 'ok', %s, %s)
                ON CONFLICT (process_name)
                DO UPDATE SET last_heartbeat = EXCLUDED.last_heartbeat,
                              status         = EXCLUDED.status,
                              pid            = EXCLUDED.pid,
                              cycle_count    = EXCLUDED.cycle_count
            """, (PROCESS_NAME, os.getpid(), cycle_count))
            self.ops_conn.commit()
        except Exception as e:
            logger.warning("heartbeat failed: %s", e)

    # ─────────────────────── Signal handlers ──────────────────────────
    def _sig_term(self, *_):
        self.running = False
        try:
            self.ops_conn.commit()
        except Exception:
            pass
        try:
            if PID_PATH.exists():
                PID_PATH.unlink()
        except Exception:
            pass
        logger.info("Malcolm stopped (SIGTERM/SIGINT).")

    def _sig_hup(self, *_):
        # lightweight “reload”: toggle log level; mode is read on each use via current_mode()
        new = logging.DEBUG if logger.level != logging.DEBUG else logging.INFO
        logger.setLevel(new)
        logger.info("Malcolm received SIGHUP → log level now %s", logging.getLevelName(new))

# ─────────────────────────── Entrypoint ───────────────────────────────
if __name__ == "__main__":
    Malcolm().run_forever()

================================================================================
FILE: mm/core/petra.py
================================================================================
#===================================================================
# 🍁 A R I A N D E           bot version 6.1 file build 20250917.01
#===================================================================
# last update: 2025 | Sept. 17                  Production ready ✅
#===================================================================
# Petra - Sales Manager
# mm/core/petra.py
#
# Responsible for selling inventory positions
# Identifies profitable exit opportunities  
# Places limit sell orders with proper spread
#
# [520] [741] [8]
#===================================================================
# 🜁 THE COMMANDER            ✔ PERSISTANT RUNTIME  ✔ MONIT MANAGED
#===================================================================

# 🔸 Standard Library Imports ======================================

import os
import json
import time
import select
import signal
import pathlib
import logging
import logging.handlers
from dataclasses import dataclass
from typing import Optional, Dict, Any

import psycopg2

from mm.config import marcus
from mm.core.grayson import RiskOps
from mm.conn.conn_kucoin import KucoinClient
from mm.utils.tqt import andi

# Helen: inventory manager (asset holds)
try:
    from mm.core.helen import Helen
except Exception:
    Helen = None  # graceful fallback; guarded calls below

# Inara helpers (fallbacks keep Petra runnable if inara.py is older)
try:
    from mm.utils.helpers.inara import current_mode, can_place_orders, is_live_mode
except Exception:
    def current_mode() -> str:
        return getattr(marcus, "MODE", "simulation")
    def can_place_orders() -> bool:
        return current_mode() in ("live", "simulation", "shadow")
    def is_live_mode() -> bool:
        return current_mode() == "live"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Config / Paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DSN = os.getenv("PG_DSN", "dbname=ariadne user=postgres host=localhost")
CHANNEL_READY  = "proposals.ready.petra"
CHANNEL_DENIED = "proposals.denied.petra"
PROCESS_NAME   = "petra"

LOG_PATH = pathlib.Path("mm/logs/petra.log")
PID_PATH = pathlib.Path("mm/utils/soc/petra.pid")

HEARTBEAT_SEC = 5
LOOP_SLEEP    = 0.25

logger = logging.getLogger("petra")
logger.setLevel(logging.INFO)

def _setup_logging():
    LOG_PATH.parent.mkdir(parents=True, exist_ok=True)
    fh = logging.handlers.RotatingFileHandler(LOG_PATH, maxBytes=5_000_000, backupCount=3)
    fh.setFormatter(logging.Formatter('[%(asctime)s] %(levelname)s %(message)s'))
    logger.addHandler(fh)
    ch = logging.StreamHandler()
    ch.setFormatter(logging.Formatter('%(message)s'))
    logger.addHandler(ch)

def _write_pid():
    PID_PATH.parent.mkdir(parents=True, exist_ok=True)
    PID_PATH.write_text(str(os.getpid()))

@dataclass
class PetraCfg:
    quote: str = marcus.QUOTE_CURRENCY
    min_trade: float = getattr(marcus, "MIN_TRADE_SIZE", 10.0)

class Petra:
    """
    SELL Originator:
      - Maintains two DB connections (LISTEN vs ops)
      - Reacts to proposals.ready.petra / proposals.denied.petra
      - On READY: place limit order (mode-gated); link/unwind asset holds via Helen (sim), queue to Andi
      - On DENIED/EXPIRED: log and stop progression
      - Generates lightweight sell proposals opportunistically (stub; DrCalvin/Lamar usually drive)
    """

    def __init__(self, client: Optional[KucoinClient] = None, cfg: PetraCfg = PetraCfg()):
        _setup_logging()
        _write_pid()

        self.cfg = cfg
        self.client = client or KucoinClient()
        self.grayson = RiskOps()
        self.helen = Helen(self.client) if Helen else None

        # LISTEN connection (autocommit ON)
        self.listen_conn = psycopg2.connect(DSN)
        self.listen_conn.set_session(autocommit=True)
        self.listen_cur = self.listen_conn.cursor()
        self.listen_cur.execute(f"LISTEN {CHANNEL_READY};")
        self.listen_cur.execute(f"LISTEN {CHANNEL_DENIED};")

        # OPS connection (for queries/writes)
        self.ops_conn = psycopg2.connect(DSN)
        self.ops_cur  = self.ops_conn.cursor()

        self.running = False
        signal.signal(signal.SIGINT,  self._sig_term)
        signal.signal(signal.SIGTERM, self._sig_term)
        try:
            signal.signal(signal.SIGHUP, self._sig_hup)
        except Exception:
            pass

        logger.info("Petra initialized | mode=%s | quote=%s", current_mode(), self.cfg.quote)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Main loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def run_forever(self):
        self.running = True
        last_hb = 0.0
        cycle = 0

        logger.info("Petra loop startingâ€¦")
        while self.running:
            now = time.time()
            if now - last_hb >= HEARTBEAT_SEC:
                self._heartbeat(cycle)
                last_hb = now

            # Non-blocking wait for notifications
            r, _, _ = select.select([self.listen_conn], [], [], LOOP_SLEEP)
            if r:
                self.listen_conn.poll()
                while self.listen_conn.notifies:
                    note = self.listen_conn.notifies.pop(0)
                    self._route_notify(note.channel, note.payload)

            # Opportunistic proposal generation (kept light)
            try:
                self._maybe_generate_proposals()
            except Exception as e:
                logger.debug("proposal-gen skipped: %s", e)

            cycle += 1

        logger.info("Petra loop stopped.")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Notifications routing â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _route_notify(self, channel: str, payload: str):
        data = self._parse_payload(payload)

        if channel == CHANNEL_READY:
            self._on_ready(data)
        elif channel == CHANNEL_DENIED:
            self._on_denied(data)
        else:
            logger.debug("Ignoring channel=%s payload=%r", channel, payload)

    def _parse_payload(self, payload: str) -> Dict[str, Any]:
        try:
            return json.loads(payload)
        except Exception:
            d = {"raw": payload}
            s = payload.strip()
            if s.startswith("id:"):
                try:
                    d["proposal_id"] = int(s.split(":", 1)[1])
                except Exception:
                    pass
            return d

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Handlers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _on_ready(self, data: Dict[str, Any]):
        pid = data.get("proposal_id")
        if not pid:
            logger.error("READY missing proposal_id: %r", data)
            return

        prop = self._fetch_proposal(pid)
        if not prop:
            logger.error("proposal %s not found or not in APPROVED/READY state", pid)
            return

        mode = current_mode()
        # Gate: if mode doesn't allow real placement, just record a shadow finalize
        if not can_place_orders() or mode in ("shadow", "halted", "maintenance", "drain"):
            logger.info("[mode:%s] would SELL %s @%s x%s (shadow only)",
                        mode, prop["symbol"], prop["price_intent"], prop["size_intent"])
            self._mark_finalized(pid, ghost=True)
            self._log_route(pid, status="shadow-finalized", info={"mode": mode})
            return

        # Risk sanity (light; full vet happened upstream)
        if not self._risk_ok(prop):
            self._fail_proposal(pid, reason="risk_blocked")
            return

        # Place limit order on exchange
        try:
            order_id = self.client.create_limit_order(
                symbol=prop["symbol"],
                side="sell",
                price=float(prop["price_intent"]),
                size=float(prop["size_intent"]),
            )
        except Exception as e:
            logger.error("order placement failed proposal=%s: %s", pid, e)
            # Unwind any sim asset hold via Helen (live mode â†’ no-op)
            self._safe_helen_cancel(reason="placement_failed", proposal_id=pid)
            self._fail_proposal(pid, reason="placement_failed")
            return

        # Link asset hold â†’ order in SIM (live reserves are internal to exchange)
        self._safe_helen_link(order_id=order_id, proposal_id=pid)

        # Persist order intent via Andi (TQT)
        self._safe_andi_queue(
            proposal_id=pid,
            symbol=prop["symbol"],
            side="sell",
            price=float(prop["price_intent"]),
            size=float(prop["size_intent"]),
            order_id=order_id,
            mode=mode,
            origin="petra",
        )

        # finalize
        self._mark_finalized(pid)
        self._log_route(pid, status="finalized", info={"order_id": order_id})

    def _on_denied(self, data: Dict[str, Any]):
        pid = data.get("proposal_id")
        typ = data.get("type", "denied")  # "denied" | "expired"
        if not pid:
            logger.error("DENIED missing proposal_id: %r", data)
            return
        self._log_route(pid, status=typ, info=data)
        # no further action

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Proposal plumbing â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _fetch_proposal(self, pid: int) -> Optional[Dict[str, Any]]:
        self.ops_cur.execute("""
            SELECT id, symbol, side, price_intent, size_intent, status
            FROM proposals
            WHERE id = %s AND side = 'sell' AND status IN ('approved','ready')
        """, (pid,))
        row = self.ops_cur.fetchone()
        if not row:
            return None
        keys = ("id", "symbol", "side", "price_intent", "size_intent", "status")
        return dict(zip(keys, row))

    def _mark_finalized(self, pid: int, ghost: bool = False):
        new_status = "shadow_finalized" if ghost else "finalized"
        self.ops_cur.execute("UPDATE proposals SET status = %s WHERE id = %s", (new_status, pid))
        self.ops_conn.commit()

    def _fail_proposal(self, pid: int, reason: str):
        self.ops_cur.execute("UPDATE proposals SET status = 'failed' WHERE id = %s", (pid,))
        self.ops_conn.commit()
        self._log_route(pid, status="failed", info={"reason": reason})

    def _log_route(self, pid: int, status: str, info: Dict[str, Any]):
        try:
            self.ops_cur.execute("""
                INSERT INTO proposal_router_log (proposal_id, timestamp, status, details)
                VALUES (%s, NOW(), %s, %s)
            """, (pid, status, json.dumps(info)))
            self.ops_conn.commit()
        except Exception as e:
            logger.warning("router log insert failed p=%s: %s", pid, e)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _risk_ok(self, prop: Dict[str, Any]) -> bool:
        try:
            notional = float(prop["price_intent"]) * float(prop["size_intent"])
            if notional < float(self.cfg.min_trade):
                return False
            # Provide equity externally if you want stricter checks; use 0.0 here as placeholder
            return self.grayson.can_trade_pair(prop["symbol"], 0.0)
        except Exception:
            return True

    def _maybe_generate_proposals(self):
        """
        Placeholder: normally call DrCalvin to rank & insert SELL intents based on inventory signals.
        Respect modes; skip in maintenance/halted.
        """
        m = current_mode()
        if m in ("halted", "maintenance"):
            return
        # Intentionally left minimal; Lamar handles routing â†’ vets â†’ approvals.

    def _safe_andi_queue(self, **kw):
        try:
            andi.queue_order(**kw)
        except Exception as e:
            logger.warning("andi.queue_order warn: %s", e)

    def _safe_helen_link(self, order_id: str, proposal_id: int):
        """
        Prefer correlation-based linking (no hold_id in proposals).
        Falls back quietly if your Helen expects (hold_id, order_id).
        """
        if not self.helen:
            return
        try:
            # Preferred: correlation id
            if hasattr(self.helen, "link_asset_hold_to_order"):
                try:
                    # new signature with correlation_id
                    self.helen.link_asset_hold_to_order(order_id=order_id, correlation_id=f"proposal:{proposal_id}")
                    return
                except TypeError:
                    pass
                # legacy: requires hold_id (Helen maps proposalâ†’hold on her side or ignores)
                try:
                    self.helen.link_asset_hold_to_order(None, order_id)  # best-effort
                except Exception:
                    pass
        except Exception as e:
            logger.debug("helen link skipped: %s", e)

    def _safe_helen_cancel(self, reason: str, proposal_id: int):
        if not self.helen:
            return
        try:
            if hasattr(self.helen, "on_cancel"):
                try:
                    self.helen.on_cancel(reason=reason, correlation_id=f"proposal:{proposal_id}")
                except TypeError:
                    self.helen.on_cancel(order_id=None, reason=reason)
        except Exception as e:
            logger.debug("helen on_cancel skipped: %s", e)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Heartbeats â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _heartbeat(self, cycle_count: int):
        try:
            self.ops_cur.execute("""
                INSERT INTO heartbeats (process_name, last_heartbeat, status, pid, cycle_count)
                VALUES (%s, NOW(), 'ok', %s, %s)
                ON CONFLICT (process_name)
                DO UPDATE SET last_heartbeat = EXCLUDED.last_heartbeat,
                              status         = EXCLUDED.status,
                              pid            = EXCLUDED.pid,
                              cycle_count    = EXCLUDED.cycle_count
            """, (PROCESS_NAME, os.getpid(), cycle_count))
            self.ops_conn.commit()
        except Exception as e:
            logger.warning("heartbeat failed: %s", e)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Signal handlers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _sig_term(self, *_):
        self.running = False
        try:
            self.ops_conn.commit()
        except Exception:
            pass
        try:
            if PID_PATH.exists():
                PID_PATH.unlink()
        except Exception:
            pass
        logger.info("Petra stopped (SIGTERM/SIGINT).")

    def _sig_hup(self, *_):
        # lightweight â€œreloadâ€: toggle log level; mode is read on each use via current_mode()
        new = logging.DEBUG if logger.level != logging.DEBUG else logging.INFO
        logger.setLevel(new)
        logger.info("Petra received SIGHUP â†’ log level now %s", logging.getLevelName(new))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Entrypoint â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    Petra().run_forever()

================================================================================
FILE: mm/core/naomi.py
================================================================================
#>> A R I A N D E v6
#>> last update: 2025 | Sept. 5
#>>
#>> Panic Manager
#>> mm/core/naomi.py
#>>
#>> Emergency response system for market crises
#>> Monitors for crashes, illiquidity, and extreme volatility.
#>>
#>> Auth'd -> Commander
#>>
#>> [520] [741] [8]    
#>>----------------------------------------------------------------

# Build|20250905.01

import logging
import time
from typing import Dict, List, Optional, Tuple
from datetime import datetime, timedelta

# Import config parameters
from mm.config.marcus import (
    PANIC_PRICE_MOVE_PCT, PANIC_VOLUME_SPIKE,
    PANIC_LOSS_THRESHOLD, PANIC_POSITION_LOSS_PCT,
    CONSECUTIVE_LOSS_LIMIT, QUOTE_CURRENCY,
    ALERT_EMAIL_ENABLED, ALERT_EMAIL_ADDRESS, 
    ALERT_EMAIL_RECIPIENT
)

# Email imports (conditional)
if ALERT_EMAIL_ENABLED:
    import smtplib
    from email.mime.text import MIMEText
    from mm.utils.helpers.timezone import get_email_date

# ── Logger Setup ──────────────────────────────────────────────────────
logger = logging.getLogger('ariadne.panic')

class Naomi:
    """
    Monitors for panic conditions and triggers emergency procedures
    """
    
    def __init__(self):
        """Initialize panic monitoring structures"""
        self.logger = logger
        self.price_history: Dict[str, List[Tuple[float, float]]] = {}  # symbol -> [(timestamp, price)]
        self.volume_history: Dict[str, List[Tuple[float, float]]] = {}  # symbol -> [(timestamp, volume)]
        self.consecutive_losses: int = 0
        self.last_check_time: float = 0
        self.panic_events: List[Dict] = []
        self.cooldown_until: Optional[float] = None
        
    def check_panic_conditions(self, positions: Dict[str, float], 
                             total_equity: float) -> Dict:
        """
        Check all panic conditions
        
        Args:
            positions: Current positions by symbol
            total_equity: Total account equity
            
        Returns:
            Dict with panic status and recommended actions
        """
        # Check cooldown
        if self.cooldown_until and time.time() < self.cooldown_until:
            return {
                'panic_mode': False,
                'in_cooldown': True,
                'cooldown_remaining': self.cooldown_until - time.time()
            }
        
        panic_triggers = []
        
        # Check for flash crash/spike in any position
        for symbol in positions:
            price_spike = self._check_price_spike(symbol)
            if price_spike:
                panic_triggers.append({
                    'type': 'price_spike',
                    'symbol': symbol,
                    'severity': 'critical',
                    'details': price_spike
                })
        
        # Check for abnormal volume
        volume_anomalies = self._check_volume_anomalies(positions.keys())
        if volume_anomalies:
            panic_triggers.extend(volume_anomalies)
        
        # Check position-specific losses
        position_panics = self._check_position_losses(positions, total_equity)
        if position_panics:
            panic_triggers.extend(position_panics)
        
        # Check consecutive losses
        if self.consecutive_losses >= CONSECUTIVE_LOSS_LIMIT:
            panic_triggers.append({
                'type': 'consecutive_losses',
                'severity': 'high',
                'count': self.consecutive_losses,
                'details': f'{self.consecutive_losses} consecutive losing trades'
            })
        
        # Determine action based on triggers
        if not panic_triggers:
            return {
                'panic_mode': False,
                'triggers': [],
                'reason': None
            }
        
        # Classify severity
        critical_count = sum(1 for t in panic_triggers if t['severity'] == 'critical')
        high_count = sum(1 for t in panic_triggers if t['severity'] == 'high')
        
        # Determine action
        if critical_count > 0:
            action = 'close_all'
            close_positions = True
        elif high_count >= 2:
            action = 'cancel_orders'
            close_positions = False
        else:
            action = 'monitor'
            close_positions = False
        
        # Log panic event
        panic_event = {
            'timestamp': time.time(),
            'triggers': panic_triggers,
            'action': action,
            'equity': total_equity
        }
        self.panic_events.append(panic_event)
        
        # Send alert
        if action in ['close_all', 'cancel_orders']:
            self._send_panic_alert(panic_triggers, action)
        
        # Set cooldown if taking action
        if action != 'monitor':
            self.cooldown_until = time.time() + 300  # 5 minute cooldown
        
        reason = self._format_panic_reason(panic_triggers)
        
        return {
            'panic_mode': action != 'monitor',
            'action': action,
            'close_positions': close_positions,
            'triggers': panic_triggers,
            'reason': reason
        }
    
    def _check_price_spike(self, symbol: str) -> Optional[Dict]:
        """
        Check for sudden price movements
        
        Args:
            symbol: Trading pair symbol
            
        Returns:
            Spike details or None
        """
        if symbol not in self.price_history:
            return None
            
        history = self.price_history[symbol]
        if len(history) < 2:
            return None
        
        # Get current and 1-minute ago prices
        current_price = history[-1][1]
        
        # Find price from ~60 seconds ago
        current_time = time.time()
        for timestamp, price in reversed(history[:-1]):
            if current_time - timestamp >= 60:
                minute_ago_price = price
                break
        else:
            return None
        
        # Calculate percentage change
        if minute_ago_price > 0:
            change_pct = abs((current_price - minute_ago_price) / minute_ago_price)
            
            if change_pct >= PANIC_PRICE_MOVE_PCT:
                return {
                    'move_pct': change_pct,
                    'from_price': minute_ago_price,
                    'to_price': current_price,
                    'direction': 'spike' if current_price > minute_ago_price else 'crash'
                }
        
        return None
    
    def _check_volume_anomalies(self, symbols: List[str]) -> List[Dict]:
        """
        Check for abnormal volume spikes
        
        Args:
            symbols: List of symbols to check
            
        Returns:
            List of volume anomalies
        """
        anomalies = []
        
        for symbol in symbols:
            if symbol not in self.volume_history:
                continue
            
            history = self.volume_history[symbol]
            if len(history) < 10:  # Need history for average
                continue
            
            # Calculate average volume
            recent_volumes = [v for _, v in history[-10:]]
            avg_volume = sum(recent_volumes) / len(recent_volumes)
            current_volume = history[-1][1]
            
            if avg_volume > 0:
                volume_ratio = current_volume / avg_volume
                
                if volume_ratio >= PANIC_VOLUME_SPIKE:
                    anomalies.append({
                        'type': 'volume_spike',
                        'symbol': symbol,
                        'severity': 'high',
                        'details': {
                            'ratio': volume_ratio,
                            'current': current_volume,
                            'average': avg_volume
                        }
                    })
        
        return anomalies
    
    def _check_position_losses(self, positions: Dict[str, float], 
                              total_equity: float) -> List[Dict]:
        """
        Check for significant position losses
        
        Args:
            positions: Current positions
            total_equity: Total equity
            
        Returns:
            List of position-specific panic triggers
        """
        triggers = []
        
        # This would need access to entry prices to calculate actual losses
        # For now, we'll skip this check
        # TODO: Integrate with trade history for actual P&L
        
        return triggers
    
    def update_price_history(self, symbol: str, price: float):
        """
        Update price tracking
        
        Args:
            symbol: Trading pair symbol
            price: Current price
        """
        if symbol not in self.price_history:
            self.price_history[symbol] = []
        
        # Add new price point
        self.price_history[symbol].append((time.time(), price))
        
        # Keep only last 5 minutes of history
        cutoff = time.time() - 300
        self.price_history[symbol] = [
            (t, p) for t, p in self.price_history[symbol] if t > cutoff
        ]
    
    def update_volume_history(self, symbol: str, volume: float):
        """
        Update volume tracking
        
        Args:
            symbol: Trading pair symbol
            volume: Current volume
        """
        if symbol not in self.volume_history:
            self.volume_history[symbol] = []
        
        # Add new volume point
        self.volume_history[symbol].append((time.time(), volume))
        
        # Keep only last 30 minutes of history
        cutoff = time.time() - 1800
        self.volume_history[symbol] = [
            (t, v) for t, v in self.volume_history[symbol] if t > cutoff
        ]
    
    def register_trade_result(self, profit: bool):
        """
        Register trade outcome for consecutive loss tracking
        
        Args:
            profit: True if profitable, False if loss
        """
        if profit:
            self.consecutive_losses = 0
        else:
            self.consecutive_losses += 1
            
            if self.consecutive_losses >= CONSECUTIVE_LOSS_LIMIT:
                self.logger.warning(f"Consecutive losses: {self.consecutive_losses}")
    
    def reset_cooldown(self):
        """Manually reset panic cooldown"""
        self.cooldown_until = None
        self.logger.info("Panic cooldown reset")
    
    def get_panic_report(self) -> Dict:
        """
        Get current panic monitoring status
        
        Returns:
            Dict with panic metrics
        """
        return {
            'consecutive_losses': self.consecutive_losses,
            'monitored_symbols': len(self.price_history),
            'in_cooldown': bool(self.cooldown_until and time.time() < self.cooldown_until),
            'cooldown_remaining': max(0, self.cooldown_until - time.time()) if self.cooldown_until else 0,
            'recent_events': self.panic_events[-5:],  # Last 5 panic events
            'price_history_size': {s: len(h) for s, h in self.price_history.items()},
            'volume_history_size': {s: len(h) for s, h in self.volume_history.items()}
        }
    
    def _format_panic_reason(self, triggers: List[Dict]) -> str:
        """
        Format human-readable panic reason
        
        Args:
            triggers: List of panic triggers
            
        Returns:
            Formatted reason string
        """
        if not triggers:
            return "No panic conditions"
        
        reasons = []
        for trigger in triggers[:3]:  # Top 3 reasons
            if trigger['type'] == 'price_spike':
                direction = trigger['details']['direction']
                pct = trigger['details']['move_pct'] * 100
                reasons.append(f"{trigger['symbol']} {direction} {pct:.1f}%")
            elif trigger['type'] == 'volume_spike':
                ratio = trigger['details']['ratio']
                reasons.append(f"{trigger['symbol']} volume {ratio:.1f}x normal")
            elif trigger['type'] == 'consecutive_losses':
                reasons.append(f"{trigger['count']} consecutive losses")
        
        return "; ".join(reasons)
    
    def _send_panic_alert(self, triggers: List[Dict], action: str):
        """
        Send panic alert email
        
        Args:
            triggers: List of triggers
            action: Recommended action
        """
        if not ALERT_EMAIL_ENABLED:
            return
        
        # Format message
        message = f"PANIC CONDITIONS DETECTED\n\n"
        message += f"Action: {action.upper()}\n\n"
        message += "Triggers:\n"
        
        for trigger in triggers:
            message += f"- {trigger['type']}: "
            if 'symbol' in trigger:
                message += f"{trigger['symbol']} "
            if 'details' in trigger:
                message += f"{trigger['details']}\n"
            else:
                message += "\n"
        
        try:
            msg = MIMEText(message)
            msg['Subject'] = f"🚨 PANIC ALERT - {get_email_date()}"
            msg['From'] = ALERT_EMAIL_ADDRESS
            msg['To'] = ALERT_EMAIL_RECIPIENT
            msg['Date'] = get_email_date()
            msg['X-Priority'] = '1'  # Highest priority
            
            server = smtplib.SMTP_SSL(ALERT_EMAIL_SMTP_SERVER, ALERT_EMAIL_SMTP_PORT)
            server.login(ALERT_EMAIL_ADDRESS, ALERT_EMAIL_PASSWORD)
            server.sendmail(ALERT_EMAIL_ADDRESS, [ALERT_EMAIL_RECIPIENT], msg.as_string())
            server.quit()
            
            self.logger.info("Panic alert email sent")
        except Exception as e:
            self.logger.error(f"Failed to send panic alert: {e}")

================================================================================
FILE: mm/core/julius.py
================================================================================
#>> A R I A N D E [v 6.1]
#>> last update: 2025 | Sept. 9                ❌ PRODUCTION READY
#>>
#>> The Banker
#>> mm/core/julius.py
#>>
#>> The single point of truth for cash/wallet state inside 
#>> the system.
#>>
#>> Auth'd -> Commander
#>>
#>> [520] [741] [8]        💫 PERSISTANT RUNTIME  ➰ MONIT MANAGED
#>>────────────────────────────────────────────────────────────────

# Build|20250909.01

import os
import json
import logging
from dataclasses import dataclass
from typing import Dict, Optional, Tuple

import psycopg2

from mm.config import marcus
from mm.conn.conn_kucoin import KucoinClient

# Inara (mode gating). Safe fallbacks if helpers are older.
try:
    from mm.utils.helpers.inara import current_mode, is_live_mode
except Exception:
    def current_mode() -> str:
        return getattr(marcus, "MODE", "simulation")
    def is_live_mode() -> bool:
        return current_mode() == "live"

DSN = os.getenv("PG_DSN", "dbname=ariadne user=postgres host=localhost")

logger = logging.getLogger("julius")
logger.setLevel(logging.INFO)


@dataclass
class JuliusCfg:
    quote: str = getattr(marcus, "QUOTE_CURRENCY", "USDT")
    min_trade: float = getattr(marcus, "MIN_TRADE_SIZE", 10.0)


class Julius:
    """
    Responsibilities
      • Phase 1 (vet): set proposals.bank_vet ← 'approved' | 'denied' based on QUOTE wallet availability.
      • Phase 2 (finalize for BUY):
          - SIM: reserve by incrementing sim_balances.hold for QUOTE.
          - LIVE: no DB hold (exchange will reserve at order); optional recheck.
          - Set proposals.status = 'approved' (idempotent) and log.
      • Interfaces for Malcolm:
          - link_hold_to_order(order_id, correlation_id="proposal:{id}") → log linkage (SIM reserve already done).
          - on_cancel(reason, correlation_id="proposal:{id}") → release SIM reservation + mark proposal failed (idempotent).
      • get_balances(): unified balances for UI/loop (LIVE via exchange; SIM via sim_balances).
    """

    def __init__(self, client: Optional[KucoinClient] = None, cfg: JuliusCfg = JuliusCfg()):
        self.cfg = cfg
        self.client = client or KucoinClient()
        self.conn = psycopg2.connect(DSN)
        self.cur = self.conn.cursor()
        logger.info("Julius ready | mode=%s | quote=%s", current_mode(), self.cfg.quote)

    # ───────────────────────── Public API ─────────────────────────

    def get_balances(self) -> Dict[str, Dict[str, float]]:
        """
        Returns { 'USDT': {'available': x, 'hold': y, 'total': z}, ... }.
        LIVE → KuCoin accounts; SIM → sim_balances (best-effort across common schemas).
        """
        if is_live_mode():
            return self._live_balances()
        return self._sim_balances()

    def vet_bank(self, proposal_id: int) -> str:
        """Phase 1: set bank_vet = 'approved' or 'denied' for BUY proposal."""
        prop = self._fetch_proposal(proposal_id)
        if not prop or prop["side"] != "buy":
            return "denied"

        notional = float(prop["price_intent"]) * float(prop["size_intent"])
        if notional < float(self.cfg.min_trade):
            self._set_vet(proposal_id, "bank_vet", "denied")
            self._route_log(proposal_id, "bank.denied", {"reason": "min_trade"})
            return "denied"

        ok = self._has_funds(notional)
        status = "approved" if ok else "denied"
        self._set_vet(proposal_id, "bank_vet", status)
        self._route_log(proposal_id, f"bank.{status}", {"notional": notional, "quote": self.cfg.quote})
        return status

    def finalize_for_buy(self, proposal_id: int) -> bool:
        """
        Phase 2 for Malcolm-originated proposals: after Lamar sees all vets approved,
        Lamar calls Julius to reserve funds before signaling Malcolm.
        SIM: increment sim_balances.hold atomically; LIVE: re-validate only.
        """
        prop = self._fetch_proposal(proposal_id, for_finalize=True)
        if not prop or prop["side"] != "buy":
            self._route_log(proposal_id, "bank_finalize.failed", {"reason": "not_found_or_wrong_side"})
            return False

        notional = float(prop["price_intent"]) * float(prop["size_intent"])

        if is_live_mode():
            if not self._has_funds(notional):
                self._route_log(proposal_id, "bank_finalize.denied", {"reason": "insufficient_live_funds"})
                return False
            # Approved in LIVE (exchange will reserve later)
            self._safe_set_status(proposal_id, "approved")
            self._route_log(proposal_id, "bank_finalize.live_ok", {"notional": notional, "quote": self.cfg.quote})
            return True

        # SIM: place a reservation by bumping sim_balances.hold atomically
        if not self._reserve_sim_funds(notional):
            self._route_log(proposal_id, "bank_finalize.denied", {"reason": "insufficient_sim_funds"})
            return False

        self._safe_set_status(proposal_id, "approved")
        self._route_log(proposal_id, "bank_finalize.approved", {"notional": notional, "quote": self.cfg.quote})
        return True

    # Called by Malcolm after successful order placement
    def link_hold_to_order(self, order_id: Optional[str] = None,
                           correlation_id: Optional[str] = None,
                           hold_id: Optional[str] = None) -> None:
        """
        SIM: reservation already made; just log linkage.
        LIVE: exchange manages reservation; just log.
        """
        pid = self._pid_from_corr(correlation_id)
        info = {"order_id": order_id, "hold_id": hold_id, "correlation_id": correlation_id}
        if pid:
            self._route_log(pid, "bank_linked", info)
        else:
            logger.info("Julius link (no pid): %s", info)

    # Called by Malcolm on placement failure to unwind a SIM reservation
    def on_cancel(self, order_id: Optional[str] = None, reason: str = "placement_failed",
                  correlation_id: Optional[str] = None, hold_id: Optional[str] = None) -> None:
        pid = self._pid_from_corr(correlation_id)
        prop = self._fetch_proposal(pid) if pid else None
        if not prop or prop["side"] != "buy":
            self._route_log(pid or -1, "bank_cancel.ignored", {"reason": "no_buy_proposal", "order_id": order_id})
            return

        notional = float(prop["price_intent"]) * float(prop["size_intent"])
        if not is_live_mode():
            self._release_sim_funds(notional)

        # Mark proposal failed (idempotent)
        self._safe_set_status(pid, "failed")
        self._route_log(pid, "bank_cancel", {
            "order_id": order_id, "reason": reason, "notional": notional, "quote": self.cfg.quote
        })

    # ───────────────────────── Internals ─────────────────────────

    def _fetch_proposal(self, pid: Optional[int], for_finalize: bool = False) -> Optional[Dict]:
        if not pid:
            return None
        if for_finalize:
            # ensure all vets are approved (risk/bank/invt)
            self.cur.execute("""
                SELECT id, symbol, side, price_intent, size_intent, risk_vet, bank_vet, invt_vet
                FROM proposals WHERE id=%s
            """, (pid,))
            row = self.cur.fetchone()
            if not row:
                return None
            d = dict(zip(
                ("id","symbol","side","price_intent","size_intent","risk_vet","bank_vet","invt_vet"), row
            ))
            if (d.get("risk_vet") != "approved") or (d.get("bank_vet") != "approved") or (d.get("invt_vet") != "approved"):
                return None
            return d
        else:
            self.cur.execute("""
                SELECT id, symbol, side, price_intent, size_intent
                FROM proposals WHERE id=%s
            """, (pid,))
            row = self.cur.fetchone()
            return dict(zip(("id","symbol","side","price_intent","size_intent"), row)) if row else None

    def _set_vet(self, pid: int, col: str, status: str) -> None:
        try:
            self.cur.execute(f"UPDATE proposals SET {col} = %s WHERE id = %s", (status, pid))
            self.conn.commit()
        except Exception as e:
            self.conn.rollback()
            logger.warning("vet update failed (%s) p=%s: %s", col, pid, e)

    def _safe_set_status(self, pid: int, status: str) -> None:
        try:
            self.cur.execute("UPDATE proposals SET status=%s WHERE id=%s", (status, pid))
            self.conn.commit()
        except Exception:
            self.conn.rollback()

    def _route_log(self, proposal_id: int, status: str, info: Dict) -> None:
        """Append to proposal_router_log for observability."""
        try:
            self.cur.execute("""
                INSERT INTO proposal_router_log (proposal_id, timestamp, status, details)
                VALUES (%s, NOW(), %s, %s)
            """, (proposal_id, status, json.dumps(info)))
            self.conn.commit()
        except Exception as e:
            self.conn.rollback()
            logger.warning("router log insert failed p=%s: %s", proposal_id, e)

    # ─────────────── Balances / availability helpers ───────────────

    def _has_funds(self, notional: float) -> bool:
        bals = self.get_balances() or {}
        q = bals.get(self.cfg.quote, {})
        avail = float(q.get("available", 0.0))
        return avail >= float(notional)

    def _live_balances(self) -> Dict[str, Dict[str, float]]:
        # { 'USDT': {'available': x, 'hold': y, 'total': x+y}, ... }
        try:
            d = self.client.get_account_balances_detailed()  # KuCoin private accounts
        except Exception:
            d = {}
        out: Dict[str, Dict[str, float]] = {}
        for cur, vals in (d or {}).items():
            avail = float(vals.get("available", 0.0) or 0.0)
            hold  = float(vals.get("hold", 0.0) or 0.0)
            out[cur] = {"available": avail, "hold": hold, "total": avail + hold}
        return out

    def _sim_balances(self) -> Dict[str, Dict[str, float]]:
        """
        Best-effort across common sim_balances schemas:
          (asset, available, hold)   OR (asset, total, hold)   OR (asset, balance, hold)
        """
        # Try #1: available+hold
        try:
            self.cur.execute("SELECT asset, COALESCE(available,0)::numeric, COALESCE(hold,0)::numeric FROM sim_balances")
            rows = self.cur.fetchall()
            out = {}
            for asset, avail, hold in rows:
                a = float(avail or 0.0); h = float(hold or 0.0)
                out[str(asset).upper()] = {"available": a, "hold": h, "total": a + h}
            return out
        except Exception:
            self.conn.rollback()

        # Try #2: total+hold
        try:
            self.cur.execute("SELECT asset, COALESCE(total,0)::numeric, COALESCE(hold,0)::numeric FROM sim_balances")
            rows = self.cur.fetchall()
            out = {}
            for asset, total, hold in rows:
                t = float(total or 0.0); h = float(hold or 0.0)
                out[str(asset).upper()] = {"available": max(0.0, t - h), "hold": h, "total": t}
            return out
        except Exception:
            self.conn.rollback()

        # Try #3: balance+hold
        try:
            self.cur.execute("SELECT asset, COALESCE(balance,0)::numeric, COALESCE(hold,0)::numeric FROM sim_balances")
            rows = self.cur.fetchall()
            out = {}
            for asset, bal, hold in rows:
                t = float(bal or 0.0); h = float(hold or 0.0)
                out[str(asset).upper()] = {"available": max(0.0, t - h), "hold": h, "total": t}
            return out
        except Exception:
            self.conn.rollback()
            return {}

    def _reserve_sim_funds(self, notional: float) -> bool:
        """
        Atomically bump sim_balances.hold for QUOTE if sufficient available remains.
        Tries common schemas in order.
        """
        q = self.cfg.quote

        # available+hold
        try:
            self.cur.execute("""
                UPDATE sim_balances
                   SET hold = hold + %s
                 WHERE asset = %s
                   AND (COALESCE(available,0) - COALESCE(hold,0)) >= %s
                RETURNING hold
            """, (notional, q, notional))
            if self.cur.fetchone():
                self.conn.commit()
                return True
            self.conn.rollback()
        except Exception:
            self.conn.rollback()

        # total+hold
        try:
            self.cur.execute("""
                UPDATE sim_balances
                   SET hold = hold + %s
                 WHERE asset = %s
                   AND (COALESCE(total,0) - COALESCE(hold,0)) >= %s
                RETURNING hold
            """, (notional, q, notional))
            if self.cur.fetchone():
                self.conn.commit()
                return True
            self.conn.rollback()
        except Exception:
            self.conn.rollback()

        # balance+hold
        try:
            self.cur.execute("""
                UPDATE sim_balances
                   SET hold = hold + %s
                 WHERE asset = %s
                   AND (COALESCE(balance,0) - COALESCE(hold,0)) >= %s
                RETURNING hold
            """, (notional, q, notional))
            if self.cur.fetchone():
                self.conn.commit()
                return True
            self.conn.rollback()
        except Exception:
            self.conn.rollback()

        return False

    def _release_sim_funds(self, notional: float) -> None:
        """Release a previous reservation in SIM (best-effort; never go below zero)."""
        q = self.cfg.quote
        try:
            self.cur.execute("""
                UPDATE sim_balances
                   SET hold = GREATEST(0, COALESCE(hold,0) - %s)
                 WHERE asset = %s
            """, (notional, q))
            self.conn.commit()
        except Exception:
            self.conn.rollback()

    # ───────────────────────── Utilities ─────────────────────────

    @staticmethod
    def _pid_from_corr(correlation_id: Optional[str]) -> Optional[int]:
        """Parse 'proposal:{id}' → id."""
        if not correlation_id:
            return None
        s = str(correlation_id).strip()
        if s.startswith("proposal:"):
            try:
                return int(s.split(":", 1)[1])
            except Exception:
                return None
        return None

================================================================================
FILE: mm/core/drcalvin.py
================================================================================
#===================================================================
# 🍁 A R I A N D E           bot version 6.1 file build 20250917.02
#===================================================================
# last update: 2025 | Sept. 17                  Production ready ✅
#===================================================================
# Dr. Susan Calvin
# mm/core/drcalvin.py
#
# Prefilters for USDT pairs matching volume requirements.
# Level I scoring.  
# Approving manager for proposals.
#
# [520] [741] [8]
#===================================================================
# 🜁 THE COMMANDER            ✖ PERSISTANT RUNTIME  ✖ MONIT MANAGED
#===================================================================

# 🔸 Standard Library Imports ======================================

import numpy as np
from typing import Dict, List, Tuple, Optional, Any
import time
from tqdm import tqdm
from datetime import datetime 

import logging
logger = logging.getLogger("Ariadne")

# 🔸 Application Imports ===========================================

from mm.utils.helpers import inara
from mm.config.marcus import (
    QUOTE_CURRENCY,
    MIN_24H_VOLUME,
    MAX_24H_VOLUME,
    MIN_COIN_AGE,
    SPREAD_TIGHTNESS,
    ORDER_BOOK_DEPTH,
    SLIPPAGE_RESISTANCE,
    VOLATILITY_PROFILE,
    VOLUME_CONSISTENCY,
    PRICE_STABILITY,
    FEE_EFFICIENCY,
    EXECUTION_SPEED,
    MARKET_IMPACT,
    OPPORTUNITY_MOD,
    MIN_LIQUIDITY_SCORE
)

class ValueOps:
    def __init__(self, config):
        self.cfg = config
        self.mode = inara.get_mode()
        self.client = inara.get_trading_client()
        
# 🔸 Prefilter =====================================================

    def pre_filter_pairs(self, client, all_pairs: List[str]) -> List[str]:
        try:
            all_tickers = client.get_all_tickers()
            if not all_tickers:
                return all_pairs
            
            ticker_dict = {item['symbol']: item for item in all_tickers}
            filtered_pairs = []

            for pair in all_pairs:
                if not pair.endswith(f"-{QUOTE_CURRENCY}"):
                    continue

                kucoin_symbol = client._pair(pair)
                ticker = ticker_dict.get(kucoin_symbol)

                if not ticker:
                    continue

                vol_value = float(ticker.get('volValue', 0))
                if vol_value < MIN_24H_VOLUME or vol_value > MAX_24H_VOLUME:
                    continue

                filtered_pairs.append(pair)

            mature_pairs = []
            maturity_window = int(time.time()) - (MIN_COIN_AGE * 86400)

            for pair in filtered_pairs:
                kucoin_symbol = client._pair(pair)
                candles = client.historical_ohlcv(kucoin_symbol, "1day", 8)

                if candles:
                    for candle in candles:
                        candle_timestamp = int(candle[0])
                        if maturity_window - 86400 <= candle_timestamp <= maturity_window:
                            mature_pairs.append(pair)
                            break

            return mature_pairs

        except Exception as e:
            logger.error(f"Error in prefiltering: {e}")
            return all_pairs
        
# 🔸 Scoring =======================================================

    def score_pair(self, symbol: str, client, historical_data=None) -> Dict[str, Any]:
        scores = {}
        category_breakdown = {}

        try:
            # ── Sub scores (all return 0..100) ───────────────────────────
            # LIQUIDITY (max 40 pts globally)
            spread_pct   = self._calculate_spread_score(symbol, client)            # 0..100
            depth_pct    = self._calculate_depth_score(symbol, client)             # 0..100
            slippage_pct = self._calculate_slippage_score(symbol, client)          # 0..100

            # MARKET (max 30 pts globally)
            volatility_pct = self._calculate_volatility_score(symbol, client, historical_data)  # 0..100
            volume_pct     = self._calculate_volume_consistency(symbol, client)                 # 0..100
            price_pct      = self._calculate_price_stability(symbol, client)                   # 0..100

            # TRADING (max 30 pts globally)
            fee_pct      = self._calculate_fee_efficiency(symbol, client)        # 0..100
            exec_pct     = self._calculate_execution_speed(symbol, client)       # 0..100
            impact_pct   = self._calculate_market_impact(symbol, client)         # 0..100

            # ── Convert % → global points via weights (no double-weighting) ─
            # Contribution (pts) = sub_weight * sub_pct
            spread_pts   = SPREAD_TIGHTNESS    * spread_pct
            depth_pts    = ORDER_BOOK_DEPTH    * depth_pct
            slippage_pts = SLIPPAGE_RESISTANCE * slippage_pct

            volatility_pts = VOLATILITY_PROFILE * volatility_pct
            volume_pts     = VOLUME_CONSISTENCY * volume_pct
            price_pts      = PRICE_STABILITY    * price_pct

            fee_pts    = FEE_EFFICIENCY  * fee_pct
            exec_pts   = EXECUTION_SPEED * exec_pct
            impact_pts = MARKET_IMPACT   * impact_pct

            # Category totals in points (Liquidity max 40, etc.)
            liquidity_points = spread_pts + depth_pts + slippage_pts
            market_points    = volatility_pts + volume_pts + price_pts
            trading_points   = fee_pts + exec_pts + impact_pts

            # Base score (0..100)
            base_score = liquidity_points + market_points + trading_points

            # ── Opportunity bonus (0..5 points) ──────────────────────────
            # imbalance_score is 0..100; OPPORTUNITY_MOD is 0.05 ⇒ +0..5 pts
            imbalance_score = self._calculate_order_book_imbalance(symbol, client)  # 0..100
            opportunity_boost = max(0.0, min(imbalance_score * OPPORTUNITY_MOD, 5.0))

            # Final can be 0..105 by design
            total_score = max(0.0, min(base_score + opportunity_boost, 105.0))

            # For convenience, keep simple “category scores” as points
            scores['liquidity_quality'] = liquidity_points
            scores['market_stability']  = market_points
            scores['trading_quality']   = trading_points

            # Breakdown: raw sub % and contribution pts
            category_breakdown['liquidity'] = {
                'total_points': liquidity_points,
                'subs': {
                    'spread':   {'pct': spread_pct,   'pts': spread_pts},
                    'depth':    {'pct': depth_pct,    'pts': depth_pts},
                    'slippage': {'pct': slippage_pct, 'pts': slippage_pts},
                }
            }
            category_breakdown['market'] = {
                'total_points': market_points,
                'subs': {
                    'volatility': {'pct': volatility_pct, 'pts': volatility_pts},
                    'volume':     {'pct': volume_pct,     'pts': volume_pts},
                    'price':      {'pct': price_pct,      'pts': price_pts},
                }
            }
            category_breakdown['trading'] = {
                'total_points': trading_points,
                'subs': {
                    'fee':     {'pct': fee_pct,    'pts': fee_pts},
                    'execution': {'pct': exec_pct, 'pts': exec_pts},
                    'impact': {'pct': impact_pct,  'pts': impact_pts},
                }
            }

            """
            # ── DEBUG ─────────────────────────────────────────────────────
            logger.debug(f"\n{symbol}")
            logger.debug(
                "LIQUIDITY | "
                f"{liquidity_points:5.2f} "
                f"[ Spread: {spread_pct:5.1f}% → {spread_pts:5.2f} ] "
                f"[ Depth: {depth_pct:5.1f}% → {depth_pts:5.2f} ] "
                f"[ Slippage: {slippage_pct:5.1f}% → {slippage_pts:5.2f} ]"
            )
            logger.debug(
                "MARKET    | "
                f"{market_points:5.2f} "
                f"[ Volatility: {volatility_pct:5.1f}% → {volatility_pts:5.2f} ] "
                f"[ Volume: {volume_pct:5.1f}% → {volume_pts:5.2f} ] "
                f"[ Price: {price_pct:5.1f}% → {price_pts:5.2f} ]"
            )
            logger.debug(
                "TRADING   | "
                f"{trading_points:5.2f} "
                f"[ Fee: {fee_pct:5.1f}% → {fee_pts:5.2f} ] "
                f"[ Execution: {exec_pct:5.1f}% → {exec_pts:5.2f} ] "
                f"[ Impact: {impact_pct:5.1f}% → {impact_pts:5.2f} ]"
            )
            logger.debug(f"MODIFIER  | imbalance={imbalance_score:.1f}% → +{opportunity_boost:.2f} pts")
            logger.debug(f"TOTAL     | base={base_score:.2f}  final={total_score:.2f}")
            """
            
            return {
                'total_score': total_score,                   # 0..105
                'base_score': base_score,                     # 0..100
                'opportunity_boost': opportunity_boost,       # 0..5
                'category_scores': scores,                    # per-category pts
                'category_breakdown': category_breakdown,     # detailed
                'grade': self._score_to_grade(total_score),
                'passed': total_score >= MIN_LIQUIDITY_SCORE,
            }

        except Exception as e:
            logger.error(f"❌ Error scoring {symbol}: {e}")
            return {
                'total_score': 0,
                'base_score': 0,
                'opportunity_boost': 0,
                'category_scores': {},
                'category_breakdown': {},
                'grade': 'F',
                'passed': False
            }


# 🔸 Calculations ==================================================
    
    # 🔹 Liquidity Calculations ====================================
    
    def _calculate_spread_score(self, symbol: str, client) -> float:
        try:
            orderbook = client.order_book(symbol, depth=2)
            if not orderbook['bids'] or not orderbook['asks']:
                return 0

            best_bid, best_ask = orderbook['bids'][0][0], orderbook['asks'][0][0]
            mid_price = (best_bid + best_ask) / 2
            spread_pct = (best_ask - best_bid) / mid_price

            # Tight spread = high score (0.1% spread = 100, 1% spread = 0)
            ideal_spread = 0.001  # 0.1%
            max_acceptable = 0.01  # 1%

            if spread_pct <= ideal_spread:
                return 100
            elif spread_pct >= max_acceptable:
                return 0
            else:
                return 100 * (1 - (spread_pct - ideal_spread) / (max_acceptable - ideal_spread))

        except Exception:
            return 0

    def _calculate_depth_score(self, symbol: str, client) -> float:
        try:
            orderbook = client.order_book(symbol, depth=10)
            if not orderbook['bids'] or not orderbook['asks']:
                return 0

            # Calculate total depth in USD for top 10 levels
            mid_price = (orderbook['bids'][0][0] + orderbook['asks'][0][0]) / 2
            bid_depth = sum(amount for price, amount in orderbook['bids'][:10])
            ask_depth = sum(amount for price, amount in orderbook['asks'][:10])
            total_depth = (bid_depth + ask_depth) * mid_price

            # Score based on depth (100 = $100k+, 0 = $0)
            min_depth = 0
            max_depth = 100000  # $100k
            return min(100, 100 * (total_depth - min_depth) / (max_depth - min_depth))

        except Exception:
            return 0

    def _calculate_slippage_score(self, symbol: str, client) -> float:
        try:
            orderbook = client.order_book(symbol, depth=20)
            if not orderbook['bids'] or not orderbook['asks']:
                return 0

            # Calculate slippage for a $10k order
            order_size = 10000
            mid_price = (orderbook['bids'][0][0] + orderbook['asks'][0][0]) / 2

            # Simulate buy order slippage
            filled = 0
            avg_buy_price = 0
            for price, amount in orderbook['asks']:
                fill_amount = min(amount, (order_size - filled) / price)
                avg_buy_price += fill_amount * price
                filled += fill_amount
                if filled * price >= order_size:
                    break

            buy_slippage = (avg_buy_price / filled - mid_price) / mid_price if filled > 0 else 0

            # Score based on slippage (0% = 100, 2% = 0)
            max_slippage = 0.02  # 2%
            return max(0, 100 * (1 - buy_slippage / max_slippage))

        except Exception:
            return 0
        
    # 🔹 Market Calculations =======================================
    
    def _calculate_volatility_score(self, symbol: str, client, historical_data=None) -> float:
        try:
            if not historical_data:
                historical_data = client.historical_ohlcv(symbol, "1h", 24)

            if len(historical_data) < 12:  # Need at least 12 hours of data
                return 50  # Neutral score

            closing_prices = [float(candle[2]) for candle in historical_data if float(candle[2]) > 0]
            if len(closing_prices) < 2:
                return 50

            returns = np.diff(closing_prices) / closing_prices[:-1]
            volatility = float(np.std(returns))

            # Score: 0% volatility = 100, 5% volatility = 0
            ideal_volatility = 0.00
            max_volatility = 0.05  # 5%

            if volatility <= ideal_volatility:
                return 100
            elif volatility >= max_volatility:
                return 0
            else:
                return 100 * (1 - (volatility - ideal_volatility) / (max_volatility - ideal_volatility))

        except Exception:
            return 50  # Neutral on error

    def _calculate_volume_consistency(self, symbol: str, client) -> float:
        try:
            # Get recent volume data (last 24 hours)
            candles = client.historical_ohlcv(symbol, "1h", 24)
            if len(candles) < 12:
                return 50

            volumes = [float(candle[5]) for candle in candles]  # Volume is index 5
            if not any(volumes):
                return 0

            # Calculate coefficient of variation (lower = more consistent)
            mean_volume = np.mean(volumes)
            std_volume = np.std(volumes)
            cv = std_volume / mean_volume if mean_volume > 0 else 1.0

            # Score: 0 CV = 100 (perfect consistency), 1 CV = 0 (high variation)
            return max(0, 100 * (1 - min(cv, 1.0)))

        except Exception:
            return 50

    def _calculate_price_stability(self, symbol: str, client) -> float:
        try:
            # Check for recent large price movements
            trades = client.get_recent_trades(symbol, limit=100)
            if len(trades) < 20:
                return 50

            prices = [float(trade['price']) for trade in trades]
            max_price = max(prices)
            min_price = min(prices)
            price_range = (max_price - min_price) / min_price

            # Score based on price range in recent trades
            # 0% range = 100, 10% range = 0
            max_acceptable_range = 0.10  # 10%
            return max(0, 100 * (1 - min(price_range / max_acceptable_range, 1.0)))

        except Exception:
            return 50
    
    # 🔹 Trading Calculations ======================================
    
    def _calculate_fee_efficiency(self, symbol: str, client) -> float:
        try:
            fee_info = client.maker_fee(symbol)
            maker_fee = fee_info.get('value', 0.001)  # Default 0.1%

            # Score: 0% fee = 100, 0.2% fee = 0
            ideal_fee = 0.00
            max_fee = 0.002  # 0.2%

            if maker_fee <= ideal_fee:
                return 100
            elif maker_fee >= max_fee:
                return 0
            else:
                return 100 * (1 - (maker_fee - ideal_fee) / (max_fee - ideal_fee))

        except Exception:
            return 50

    def _calculate_execution_speed(self, symbol: str, client) -> float:
        try:
            trades = client.get_recent_trades(symbol, limit=50)
            if len(trades) < 10:
                return 30  # Low activity = poor execution

            # More trades = better liquidity = faster execution
            trade_count = len(trades)
            return min(100, trade_count * 2)  # 50 trades = 100 score

        except Exception:
            return 50

    def _calculate_market_impact(self, symbol: str, client) -> float:
        try:
            orderbook = client.order_book(symbol, depth=5)
            if not orderbook['bids'] or not orderbook['asks']:
                return 0

            # Estimate impact of a $1000 order
            order_size = 1000
            mid_price = (orderbook['bids'][0][0] + orderbook['asks'][0][0]) / 2

            # Calculate price impact on ask side
            filled = 0
            total_cost = 0
            for price, amount in orderbook['asks']:
                fill_amount = min(amount, (order_size - filled) / price)
                total_cost += fill_amount * price
                filled += fill_amount
                if filled * price >= order_size:
                    break

            avg_price = total_cost / filled if filled > 0 else mid_price
            impact = (avg_price - mid_price) / mid_price

            # Score: 0% impact = 100, 1% impact = 0
            return max(0, 100 * (1 - min(impact / 0.01, 1.0)))

        except Exception:
            return 50
    
    # 🔹 Opportunity Mod Calculations ==============================

    def _calculate_order_book_imbalance(self, symbol: str, client) -> float:
        try:
            orderbook = client.order_book(symbol, depth=10)
            if not orderbook['bids'] or not orderbook['asks']:
                return 0

            bid_depth = sum(amount for price, amount in orderbook['bids'][:10])
            ask_depth = sum(amount for price, amount in orderbook['asks'][:10])

            ratio = max(bid_depth, ask_depth) / min(bid_depth, ask_depth)

            if 1.0 <= ratio <= 1.5:
                return 50     # Balanced
            elif 1.5 < ratio <= 3.0:
                return 75     # Moderate imbalance = opportunity
            elif 3.0 < ratio <= 5.0:
                return 25     # Concerning imbalance
            else:
                return 0      # Extreme imbalance = avoid

        except Exception:
            return 0

# 🔸 Scoring ==================================================

    def _score_to_grade(self, score: float) -> str:
        if score >= 90: return 'A+'
        elif score >= 85: return 'A'
        elif score >= 80: return 'A-'
        elif score >= 75: return 'B+'
        elif score >= 70: return 'B'
        elif score >= 65: return 'B-'
        elif score >= 60: return 'C+'
        elif score >= 50: return 'C'
        elif score >= 40: return 'D'
        else: return 'F'

================================================================================
FILE: mm/conn/conn_kucoin.py
================================================================================
#>> 🍁 A R I A N D E [v 6.1]
#>> last update: 2025 | Sept. 3                ✅ PRODUCTION READY
#>>
#>> KuCoin API Access
#>> mm/conn/conn_kucoin.py
#>>
#>> Live trading client for Ariadne
#>> Proxy connection required due to georestrictions by KuCoin
#>>
#>> Auth'd -> Commander
#>>
#>> [520] [741] [8]
#>>────────────────────────────────────────────────────────────────

# Build|20250903.01

import os
import time
import json
import hmac
import base64
import hashlib
import requests
from dotenv import load_dotenv
from urllib.parse import urlencode

import logging
logger = logging.getLogger("Ariadne")

# ── Config ─────────────────────────────────────────────────────────────

load_dotenv("mm/data/secrets/.env")

API_KEY        = os.getenv("KUCOIN_API", "")
API_SECRET     = os.getenv("KUCOIN_SEC", "")
API_PASSPHRASE = os.getenv("KUCOIN_PASSPHRASE", "")
PROX_USR       = os.getenv("PROXY_USERNAME", "")
PROX_PWD       = os.getenv("PROXY_PASSWORD", "")
PROX_HOST      = os.getenv("PROXY_HOST", "")

BASE_URL       = "https://api.kucoin.com"
FUTURES_URL    = "https://api-futures.kucoin.com"

PROXY_CONFIG = {
   "http":  f"socks5://{PROX_USR}:{PROX_PWD}@{PROX_HOST}",
   "https": f"socks5h://{PROX_USR}:{PROX_PWD}@{PROX_HOST}"
}

FALLBACK = {
    "maker": 0.0010,
    "taker": 0.0010,
    "withdrawal": 0.25,  # placeholder; KuCoin needs /currencies per chain
}

# ── Signing ────────────────────────────────────────────────────────────

class KcSigner:
    def __init__(self, api_key: str, api_secret: str, api_passphrase: str):
        self.api_key = api_key
        self.api_secret = api_secret
        self.api_passphrase = api_passphrase
        self.encrypted_passphrase = self._sign_bytes(
            api_passphrase.encode("utf-8"), api_secret.encode("utf-8")
        )

    def _sign_bytes(self, plain: bytes, key: bytes) -> str:
        hm = hmac.new(key, plain, hashlib.sha256)
        return base64.b64encode(hm.digest()).decode()

    def headers(self, payload_string: str) -> dict:
        ts = str(int(time.time() * 1000))
        sig = self._sign_bytes((ts + payload_string).encode("utf-8"), self.api_secret.encode("utf-8"))
        return {
            "KC-API-KEY": self.api_key,
            "KC-API-SIGN": sig,
            "KC-API-TIMESTAMP": ts,
            "KC-API-PASSPHRASE": self.encrypted_passphrase,
            "KC-API-KEY-VERSION": "2",
            "Content-Type": "application/json",
        }

_signer = KcSigner(API_KEY, API_SECRET, API_PASSPHRASE)

# ── Core request ───────────────────────────────────────────────────────

def kucoin_request(method: str, path: str, params: dict = None, body: dict = None, timeout: int = 10):
    """
    Signs and sends a KuCoin REST request.
    Returns: (json_dict, latency_ms)
    """
    method = method.upper()
    session = requests.Session()
    session.proxies = PROXY_CONFIG

    # Build URL + query
    url = BASE_URL + path
    raw_url = path
    query = ""
    if method in ("GET", "DELETE") and params:
        query = urlencode(params)
        url += f"?{query}"
        raw_url += f"?{query}"

    # Body
    body_str = ""
    data_to_send = None
    if method in ("POST", "PUT") and body:
        body_str = json.dumps(body, separators=(",", ":"))
        data_to_send = body_str

    # KuCoin sign string is: method + endpoint(+query) + body (timestamp prefixed in header builder)
    payload = method + raw_url + body_str

    # Prepare
    req = requests.Request(method, url, data=data_to_send)
    prepped = req.prepare()

    # Headers (GET/DELETE should not send Content-Type)
    hdrs = _signer.headers(payload)
    if method in ("GET", "DELETE"):
        hdrs.pop("Content-Type", None)
    prepped.headers.update(hdrs)

    # Send
    resp = session.send(prepped, timeout=timeout)
    # Return JSON regardless of HTTP code; KuCoin embeds code in JSON
    return resp.json(), int(resp.elapsed.total_seconds() * 1000)

# Convenience alias used by your tests
def kucoin_auth(method: str, path: str, params: dict = None, body: dict = None, timeout: int = 10):
    return kucoin_request(method, path, params, body, timeout)

# ── Client ─────────────────────────────────────────────────────────────

class KucoinClient:
    def __init__(self):
        self._symbols = {}  # "BTC-USDT" -> "BTC-USDT"
        data, _ = kucoin_auth("GET", "/api/v1/symbols")
        for itm in data.get("data", []):
            pair_norm = f"{itm['baseCurrency']}-{itm['quoteCurrency']}".upper()
            self._symbols[pair_norm] = itm["symbol"]

    def list_products(self):
        return list(self._symbols.keys())

    def _pair(self, symbol: str) -> str:
        return self._symbols.get(symbol.upper(), symbol.upper())

    # ── Public Market Data ────────────────────────────────────────────

    def get_all_tickers(self):
        """Get all ticker data in one batch API call"""
        try:
            d, _ = kucoin_auth("GET", "/api/v1/market/allTickers")
            return d.get('data', {}).get('ticker', [])
        except Exception as e:
            logger.error(f"Error fetching batch tickers: {e}")
            return []

    def best_bid_price(self, symbol: str) -> float:
        try:
            d, _ = kucoin_auth("GET", "/api/v1/market/orderbook/level1", {"symbol": self._pair(symbol)})
            return float((d.get("data") or {}).get("bestBid", 0.0))
        except Exception:
            return 0.0

    def best_ask_price(self, symbol: str) -> float:
        try:
            d, _ = kucoin_auth("GET", "/api/v1/market/orderbook/level1", {"symbol": self._pair(symbol)})
            return float((d.get("data") or {}).get("bestAsk", 0.0))
        except Exception:
            return 0.0

    def last_trade_price(self, symbol: str) -> float:
        try:
            d, _ = kucoin_auth("GET", "/api/v1/market/orderbook/level1", {"symbol": self._pair(symbol)})
            return float((d.get("data") or {}).get("price", 0.0))
        except Exception:
            return 0.0

    def vol_24h(self, symbol: str) -> float:
        try:
            d, _ = kucoin_auth("GET", "/api/v1/market/stats", {"symbol": self._pair(symbol)})
            return float((d.get("data") or {}).get("vol", 0.0))
        except Exception:
            return 0.0

    def high_24h(self, symbol: str) -> float:
        try:
            d, _ = kucoin_auth("GET", "/api/v1/market/stats", {"symbol": self._pair(symbol)})
            return float((d.get("data") or {}).get("high", 0.0))
        except Exception:
            return 0.0

    def low_24h(self, symbol: str) -> float:
        try:
            d, _ = kucoin_auth("GET", "/api/v1/market/stats", {"symbol": self._pair(symbol)})
            return float((d.get("data") or {}).get("low", 0.0))
        except Exception:
            return 0.0

    def historical_ohlcv(self, symbol: str, timeframe: str, limit: int):
        """
        Returns KuCoin raw candles: [[time, open, close, high, low, volume, turnover], ...]
        """
        try:
            tf_map = {
                "1m":"1min","3m":"3min","5m":"5min","15m":"15min","30m":"30min",
                "1h":"1hour","2h":"2hour","4h":"4hour","6h":"6hour","8h":"8hour","12h":"12hour",
                "1d":"1day","1w":"1week"
            }
            t = tf_map.get(timeframe, timeframe)

            # derive step seconds
            step = 3600
            if t.endswith("min"):
                step = int(t[:-3]) * 60
            elif t.endswith("hour"):
                step = int(t[:-4]) * 3600
            elif t.endswith("day"):
                step = int(t[:-3]) * 86400
            elif t.endswith("week"):
                step = int(t[:-4]) * 7 * 86400

            now = int(time.time())
            start = now - step * max(1, int(limit))
            params = {"symbol": self._pair(symbol), "type": t, "startAt": start, "endAt": now}
            d, _ = kucoin_auth("GET", "/api/v1/market/candles", params)
            return d.get("data", [])
        except Exception:
            return []

    def order_book(self, symbol: str, depth: int = 10):
        try:
            # level2_20 returns up to 20 each side; we slice to requested depth
            d, _ = kucoin_auth("GET", "/api/v1/market/orderbook/level2_20", {"symbol": self._pair(symbol)})
            data = d.get("data") or {}
            bids = [(float(b[0]), float(b[1])) for b in (data.get("bids") or [])[:depth]]
            asks = [(float(a[0]), float(a[1])) for a in (data.get("asks") or [])[:depth]]
            return {"bids": bids, "asks": asks}
        except Exception:
            return {"bids": [], "asks": []}

    def last_trade(self, symbol: str):
        try:
            d, _ = kucoin_auth("GET", "/api/v1/market/histories", {"symbol": self._pair(symbol)})
            arr = d.get("data") or []
            if not arr:
                return (0, 0.0)
            trade = arr[0]
            ts = int(trade.get("time", 0)) // 1000  # ms -> s
            price = float(trade.get("price", 0.0))
            return (ts, price)
        except Exception:
            return (0, 0.0)

    def intraday_volume(self, symbol: str, interval: str = "ONE_MINUTE", limit: int = 5):
        try:
            interval_map = {
                "ONE_MINUTE": "1min",
                "FIVE_MINUTE": "5min",
                "FIFTEEN_MINUTE": "15min",
                "ONE_HOUR": "1hour",
            }
            k_interval = interval_map.get(interval.upper(), "1min")
            d, _ = kucoin_auth("GET", "/api/v1/market/candles", {
                "symbol": self._pair(symbol),
                "type": k_interval
            })
            candles = d.get("data", [])
            # [time, open, close, high, low, volume, turnover]
            vol_data = [(str(int(c[0]) // 1000), float(c[5])) for c in candles[:limit]]
            return list(reversed(vol_data))
        except Exception:
            return []

    # ── Private / Fees / Limits ───────────────────────────────────────

    def maker_fee(self, symbol: str = None):
        """
        Fetch maker fee from KuCoin. Returns {"source":"api|fallback","value":float}.
        """
        try:
            params = {"symbols": self._pair(symbol)} if symbol else None
            d, _ = kucoin_auth("GET", "/api/v1/trade-fees", params)
            if d.get("code") == "200000" and d.get("data"):
                row = d["data"][0] or {}
                val = float(row.get("makerFeeRate"))
                # If API returns 0 or missing, treat as error → fallback
                if val > 0:
                    return {"source": "api", "value": val}
            # explicit soft-error → fallback
            return {"source": "fallback", "value": FALLBACK["maker"]}
        except Exception:
            return {"source": "fallback", "value": FALLBACK["maker"]}


    def taker_fee(self, symbol: str = None):
        """
        Fetch taker fee from KuCoin. Returns {"source":"api|fallback","value":float}.
        """
        try:
            params = {"symbols": self._pair(symbol)} if symbol else None
            d, _ = kucoin_auth("GET", "/api/v1/trade-fees", params)
            if d.get("code") == "200000" and d.get("data"):
                row = d["data"][0] or {}
                val = float(row.get("takerFeeRate"))
                if val > 0:
                    return {"source": "api", "value": val}
            return {"source": "fallback", "value": FALLBACK["taker"]}
        except Exception:
            return {"source": "fallback", "value": FALLBACK["taker"]}


    def withdrawal_fee(self, symbol: str, network: str = None):
        """
        Always fetch XRP withdraw fee (your policy). Returns fee in XRP units.
        Uses KuCoin currencies endpoint and picks the XRP/XRPL chain row.
        """
        try:
            # v3 endpoint returns one object with 'chains' list
            d, _ = kucoin_auth("GET", "/api/v3/currencies/XRP")
            data = d.get("data") or {}

            # 'chains' might be list under either data['chains'] or data (older)
            chains = data.get("chains")
            if chains is None and isinstance(data, list):
                chains = data
            if not chains:
                return {"source": "fallback", "value": FALLBACK["withdrawal"], "network": "XRP"}

            fee_val = None
            for ch in chains:
                name = (ch.get("chain") or ch.get("name") or "").upper()
                if name in ("XRP", "XRPL", "RIPPLE"):
                    # Key names vary by doc/version: withdrawFee / withdrawMinFee / withdrawalMinFee
                    raw = ch.get("withdrawFee", ch.get("withdrawMinFee", ch.get("withdrawalMinFee")))
                    if raw is not None:
                        fee_val = float(raw)
                        break

            # If we didn't find an explicit XRP row, take the smallest withdraw fee among chains
            if fee_val is None:
                fees = []
                for ch in chains:
                    raw = ch.get("withdrawFee", ch.get("withdrawMinFee", ch.get("withdrawalMinFee")))
                    if raw is not None:
                        try: fees.append(float(raw))
                        except: pass
                if fees:
                    fee_val = min(fees)
                    return {"source": "api", "value": fee_val, "network": "XRP"}
                return {"source": "fallback", "value": FALLBACK["withdrawal"], "network": "XRP"}

            return {"source": "api", "value": fee_val, "network": "XRP"}

        except Exception:
            return {"source": "fallback", "value": FALLBACK["withdrawal"], "network": "XRP"}

    def min_trade_size(self, symbol: str):
        try:
            d, _ = kucoin_auth("GET", "/api/v1/symbols")
            ex = self._pair(symbol).upper()
            for itm in d.get("data", []):
                if itm.get("symbol", "").upper() == ex:
                    v = float(itm.get("baseMinSize", 0.0))
                    return {"source": "api", "value": v}
            return {"source": "fallback", "value": 0.0}
        except Exception:
            return {"source": "fallback", "value": 0.0}
        
    def get_recent_trades(self, symbol: str, limit: int = 100):
        """
        Fetches recent trades for a symbol.
        Args:
            symbol: The trading pair (e.g., 'BTC-USDT')
            limit: Number of trades to fetch (max 100)
        Returns:
            list: A list of recent trades, newest first.
        """
        try:
            params = {"symbol": self._pair(symbol)}
            d, _ = kucoin_auth("GET", "/api/v1/market/histories", params)
            return d.get('data', [])[:limit]
        except Exception as e:
            logger.error(f"Error fetching recent trades for {symbol}: {e}")
            return []

    # ── Wallet Functions  ─────────────────────────────────────────────
    def get_account_balances(self, account_type: str = "trade"):
        try:
            # Use the private API endpoint for accounts
            params = {"type": account_type}
            d, _ = kucoin_auth("GET", "/api/v1/accounts", params)
            
            balances = {}
            if d.get("code") == "200000":
                for account in d.get("data", []):
                    currency = (account.get("currency") or "").upper()
                    # Only count available balance, not held
                    available = float(account.get("available", 0.0))
                    if available > 0:
                        balances[currency] = balances.get(currency, 0.0) + available
            return balances
        except Exception as e:
            # Log the error and return an empty dict to avoid crashing the bot
            logger.error(f"Error fetching balances: {e}")
            return {}
        
    def get_account_balances_detailed(self, account_type: str = "trade"):
        """
        Returns { 'USDT': {'available': x, 'hold': y}, 'BTC': {...}, ... }
        Uses KuCoin accounts endpoint.
        """
        try:
            params = {"type": account_type}
            d, _ = kucoin_auth("GET", "/api/v1/accounts", params)
            balances = {}
            if d.get("code") == "200000":
                for acc in d.get("data", []):
                    currency = (acc.get("currency") or "").upper()
                    if not currency:
                        continue
                    available = float(acc.get("available", 0.0) or 0.0)
                    hold = float(acc.get("hold", acc.get("holds", 0.0) or 0.0) or 0.0)
                    cur = balances.get(currency, {"available": 0.0, "hold": 0.0})
                    cur["available"] += max(0.0, available)
                    cur["hold"] += max(0.0, hold)
                    balances[currency] = cur
            return balances
        except Exception as e:
            logger.error(f"Error fetching detailed balances: {e}")
            return {}
    
    def get_orders(self, symbol: str = None, status: str = "active"):
        """Get orders from exchange."""
        endpoint = "/api/v1/orders"
        params = {"status": status}
        if symbol:
            params["symbol"] = symbol
        return self._get(endpoint, params)
    
    def get_open_sells(self):
        params = {
            "status": "active",
            "side": "sell"
        }
        d, _ = kucoin_auth("GET", "/api/v1/orders", params=params)
        return d
    
    def get_open_buys(self):
        params = {
            "status": "active",
            "side": "buy"
        }
        d, _ = kucoin_auth("GET", "/api/v1/orders", params=params)
        return d

    # ── Orders ────────────────────────────────────────────────────────

    def create_limit_order(self, symbol: str, side: str, price: float, size: float,
                           post_only: bool = False, tif: str = "GTC",
                           hidden: bool = False, iceberg: bool = False) -> str:
        """
        Places a limit order; returns orderId on success, raises on error.
        """
        path = "/api/v1/orders"
        body = {
            "clientOid": str(int(time.time() * 1000)),
            "symbol": self._pair(symbol),
            "side": side.lower(),   # "buy"/"sell"
            "type": "limit",
            "price": str(round(float(price), 8)),
            "size": str(round(float(size), 8)),
            "timeInForce": tif,
            "postOnly": bool(post_only),
            "hidden": bool(hidden),
            "iceberg": bool(iceberg),
        }
        data, _ = kucoin_auth("POST", path, None, body)
        if data.get("code") == "200000":
            od = data.get("data") or {}
            if "orderId" in od:
                return str(od["orderId"])
        raise ValueError(f"Unexpected KuCoin response: {data}")

    def get_order(self, order_id: str):
        d, _ = kucoin_auth("GET", f"/api/v1/orders/{order_id}")
        return d

    def cancel_order(self, order_id: str):
        d, _ = kucoin_auth("DELETE", f"/api/v1/orders/{order_id}")
        return d
    
    def create_market_order(self, symbol: str, side: str, size: float) -> str:
        """
        Places a market order.
        Args:
            symbol: The trading pair (e.g., 'BTC-USDT')
            side: 'buy' or 'sell'
            size: The amount of the base currency to buy/sell
        Returns:
            str: The order ID from KuCoin
        Raises:
            ValueError: If the order placement fails.
        """
        path = "/api/v1/orders"
        body = {
            "clientOid": str(int(time.time() * 1000)),
            "symbol": self._pair(symbol),
            "side": side.lower(),
            "type": "market",
            "size": str(round(float(size), 8)),
        }
        data, _ = kucoin_auth("POST", path, None, body)
        if data.get("code") == "200000":
            od = data.get("data") or {}
            if "orderId" in od:
                return str(od["orderId"])
        raise ValueError(f"Failed to place market order: {data}")

    # ── Futures (funding rate helper) ─────────────────────────────────

    def funding_rate(self, symbol: str) -> float:
        """
        Maps spot symbol like BTC-USDT to futures perpetual (e.g., XBTUSDTM or BTCUSDTM-PERP),
        then fetches current funding rate. Best-effort; returns 0.0 if not found.
        """
        try:
            # Fetch active contracts via same proxy
            r = requests.get(f"{FUTURES_URL}/api/v1/contracts/active", timeout=10, proxies=PROXY_CONFIG)
            r.raise_for_status()
            contracts = r.json().get("data", [])

            base = symbol.split("-")[0].upper()

            fut_symbol = None
            for c in contracts:
                s = (c.get("symbol") or "").upper()
                # heuristics: endswith PERP; name formats differ
                if s.startswith(base) and "PERP" in s:
                    fut_symbol = c.get("symbol")
                    break

            if not fut_symbol:
                return 0.0

            r2 = requests.get(f"{FUTURES_URL}/api/v1/funding-rate/{fut_symbol}", timeout=10, proxies=PROXY_CONFIG)
            r2.raise_for_status()
            data = r2.json().get("data") or {}
            return float(data.get("fundingRate", 0.0))
        except Exception:
            return 0.0


# ── Public helpers ─────────────────────────────────────────────────────

def public_products():
    d, _ = kucoin_auth("GET", "/api/v1/symbols")
    return [item["symbol"] for item in d.get("data", [])]

def test_proxy_connection():
    try:
        test_url = "https://api.infoip.io/"
        resp = requests.get(test_url, proxies=PROXY_CONFIG, timeout=10)
        ip_info = resp.json()
        country = ip_info.get("country", "unknown")
        logger.info("Proxy test successful. Connection from: %s", country)
        return True
    except Exception as e:
        # Include traceback so you can see DNS/proxy errors in logs
        logger.critical("Proxy test failed: %s", e, exc_info=True)
        return False

================================================================================
FILE: mm/conn/sim_kucoin.py
================================================================================
#===================================================================
# 🍁 A R I A N D E           bot version 6.1 file build 20250917.01
#===================================================================
# last update: 2025 | Sept. 17                  Production ready ✅
#===================================================================
# Simulation Client
# mm/conn/sim_kucoin.py
#
# Simulation client that mimics KucoinClient interface. 
# Uses live market data + simulated wallet/orders 
# Virtual 'paper trading'  
#
# [520] [741] [8]
#===================================================================
# 🜁 THE COMMANDER            ✔ PERSISTANT RUNTIME  ✔ MONIT MANAGED
#===================================================================

import logging
import time
import threading
from typing import Dict, Optional, Tuple, List
from decimal import Decimal, ROUND_DOWN
import psycopg2
import psycopg2.extras

# Import the live client for market data
from mm.conn.conn_kucoin import KucoinClient

logger = logging.getLogger("ariadne.sim")

# ---- DB Connection -----------------------------------------------------------
def get_db_connection():
    """
    Create a PostgreSQL connection to 'ariadne' as postgres@localhost.
    """
    return psycopg2.connect(dbname="ariadne", user="postgres", host="localhost")

# ---- Helpers ----------------------------------------------------------------
D2 = Decimal("0.01")
D8 = Decimal("0.00000001")
ZERO = Decimal("0")

def _q2(x: Decimal) -> Decimal:
    """Quantize to 2 dp (money, human)."""
    return x.quantize(D2, rounding=ROUND_DOWN)

def _q8(x: Decimal) -> Decimal:
    """Quantize to 8 dp (crypto sizes/prices)."""
    return x.quantize(D8, rounding=ROUND_DOWN)

def _split_symbol(symbol: str) -> Tuple[str, str]:
    base, quote = symbol.upper().split("-", 1)
    return base, quote

# ---- Sim Client --------------------------------------------------------------
class SimClient:
    """
    Simulator backed by PostgreSQL tables:
      - tickstick (partitioned): market snapshot source for pricing/fees
      - sim_balances(asset, available, hold)
      - sim_orders(id, symbol, side, price, size, filled_size, status, created_at, updated_at, filled_at, deleted, deleted_at)
      - sim_trades(timestamp, symbol, side, price, size, sim_order_id, fee)
    """
    
    def __init__(self, db_path: str = None, db_lock=None):
        """Initialize with live client for market data and optional db lock for thread safety."""
        self.live_client = KucoinClient()
        self._symbols = self.live_client._symbols

    # ── Public Market Data (use live data) ────────────────────────────
    
    def get_all_tickers(self):
        """Delegate to live client for real market data."""
        return self.live_client.get_all_tickers()

    def best_bid_price(self, symbol: str) -> float:
        """Use live market data."""
        return self.live_client.best_bid_price(symbol)

    def best_ask_price(self, symbol: str) -> float:
        """Use live market data."""
        return self.live_client.best_ask_price(symbol)

    def last_trade_price(self, symbol: str) -> float:
        """Use live market data."""
        return self.live_client.last_trade_price(symbol)

    def vol_24h(self, symbol: str) -> float:
        """Use live market data."""
        return self.live_client.vol_24h(symbol)

    def high_24h(self, symbol: str) -> float:
        """Use live market data."""
        return self.live_client.high_24h(symbol)

    def low_24h(self, symbol: str) -> float:
        """Use live market data."""
        return self.live_client.low_24h(symbol)

    def historical_ohlcv(self, symbol: str, timeframe: str, limit: int):
        """Use live market data."""
        return self.live_client.historical_ohlcv(symbol, timeframe, limit)

    def order_book(self, symbol: str, depth: int = 10):
        """Use live market data."""
        return self.live_client.order_book(symbol, depth)

    def get_recent_trades(self, symbol: str, limit: int = 100):
        """Use live market data."""
        return self.live_client.get_recent_trades(symbol, limit)

    def list_products(self):
        """Use live market data."""
        return self.live_client.list_products()

    def _pair(self, symbol: str) -> str:
        """Use live client's pair method."""
        return self.live_client._pair(symbol)

    # ── Fees (simple/simulated) ───────────────────────────────────────

    def maker_fee(self, symbol: str = None):
        """Return simulated maker fee."""
        return {"source": "simulation", "value": 0.0010}

    def taker_fee(self, symbol: str = None):
        """Return simulated taker fee."""
        return {"source": "simulation", "value": 0.0010}

    def withdrawal_fee(self, symbol: str, network: str = None):
        """Return simulated withdrawal fee."""
        return {"source": "simulation", "value": 0.25, "network": "XRP"}

    def min_trade_size(self, symbol: str):
        """Get minimum trade size from live client."""
        live_result = self.live_client.min_trade_size(symbol)
        return {"source": "simulation", "value": live_result["value"]}

    # ── Internal fee helper (for PostgreSQL-based fees if needed) ────────
    
    def _get_fees_from_db(self, conn, symbol: str) -> Tuple[float, float]:
        """
        Return (maker, taker) as fractions (e.g., 0.001 = 0.1%).
        Falls back to default 0.0010 if not in tickstick.
        """
        MAKER_KEYS = ("maker_fee", "maker_fee_bps", "maker_bps", "maker_rate", "makerFeeRate")
        TAKER_KEYS = ("taker_fee", "taker_fee_bps", "taker_bps", "taker_rate", "takerFeeRate")
        MAKER_COEFF_KEYS = ("maker_coeff", "maker_coef", "maker_coefficient", "makerCoefficient")
        TAKER_COEFF_KEYS = ("taker_coeff", "taker_coef", "taker_coefficient", "takerCoefficient")

        try:
            cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
            cur.execute(
                """
                SELECT *
                FROM tickstick
                WHERE symbol = %s
                ORDER BY timestamp DESC
                LIMIT 1
                """,
                (symbol,),
            )
            row = cur.fetchone() or {}
            cur.close()

            def pick(rr: Dict, keys, default=None):
                for k in keys:
                    if k in rr and rr[k] is not None:
                        try:
                            return float(rr[k])
                        except Exception:
                            continue
                return default

            maker_raw = pick(row, MAKER_KEYS, 0.0010)
            taker_raw = pick(row, TAKER_KEYS, 0.0010)
            m_coeff = pick(row, MAKER_COEFF_KEYS, 1.0)
            t_coeff = pick(row, TAKER_COEFF_KEYS, 1.0)

            # Support bps if raw > 1 (e.g., 10 = 10 bps = 0.0010)
            maker = (maker_raw / 10000.0) if maker_raw > 1 else maker_raw
            taker = (taker_raw / 10000.0) if taker_raw > 1 else taker_raw

            maker *= (m_coeff or 1.0)
            taker *= (t_coeff or 1.0)

            # Clamp to sane range [0, 1%]
            maker = max(0.0, min(maker, 0.01))
            taker = max(0.0, min(taker, 0.01))
            return (maker, taker)
        except Exception:
            return (0.0010, 0.0010)

    # ── Wallet Functions (simulation only) ─────────────────────────────

    def get_account_balances(self, account_type: str = "trade") -> Dict[str, float]:
        """Available balances only (compat with live)."""
        with self.db_lock:
            return self._get_balances()
    
    def _get_balances(self) -> Dict[str, float]:
        """Internal method to get balances (available only)."""
        try:
            with get_db_connection() as conn:
                cur = conn.cursor()
                cur.execute("SELECT asset, available FROM sim_balances")
                balances = {row[0].upper(): float(row[1] or 0.0) for row in cur.fetchall()}
                cur.close()
                return balances
        except Exception as e:
            logger.error(f"Error reading balances: {e}")
            return {}

    def get_account_balances_detailed(self, account_type: str = "trade") -> Dict[str, Dict[str, float]]:
        """Available + hold, mirroring live client detailed call."""
        with self.db_lock:
            return self._get_balances_detailed()
    
    def _get_balances_detailed(self) -> Dict[str, Dict[str, float]]:
        """Internal method to get detailed balances."""
        try:
            with get_db_connection() as conn:
                cur = conn.cursor()
                cur.execute("SELECT asset, available, hold FROM sim_balances")
                result = {}
                for asset, available, hold in cur.fetchall():
                    result[asset.upper()] = {
                        "available": float(available or 0.0),
                        "hold": float(hold or 0.0)
                    }
                cur.close()
                return result
        except Exception as e:
            logger.error(f"Error fetching detailed balances: {e}")
            return {}

    def _update_balance(self, currency: str, delta_available: float = 0.0, delta_hold: float = 0.0):
        """Adjust available/hold for a currency."""
        with self.db_lock:
            self._update_balance_internal(currency, delta_available, delta_hold)
    
    def _update_balance_internal(self, currency: str, delta_available: float, delta_hold: float):
        """Internal method to update balance."""
        currency = currency.upper()
        try:
            with get_db_connection() as conn:
                cur = conn.cursor()
                # Ensure row exists
                cur.execute(
                    "INSERT INTO sim_balances (asset, available, hold) VALUES (%s, 0, 0) ON CONFLICT (asset) DO NOTHING",
                    (currency,)
                )
                
                # Update balances
                cur.execute(
                    """
                    UPDATE sim_balances 
                    SET available = GREATEST(0, available + %s),
                        hold = GREATEST(0, hold + %s)
                    WHERE asset = %s
                    """,
                    (delta_available, delta_hold, currency)
                )
                conn.commit()
                cur.close()
        except Exception as e:
            logger.error(f"Error updating balance for {currency}: {e}")
            raise

    def _get_balance_row(self, currency: str) -> Tuple[float, float]:
        """Get balance row with lock support."""
        with self.db_lock:
            return self._get_balance_row_internal(currency)
    
    def _get_balance_row_internal(self, currency: str) -> Tuple[float, float]:
        """Internal method to get balance row."""
        currency = currency.upper()
        try:
            with get_db_connection() as conn:
                cur = conn.cursor()
                cur.execute("SELECT available, hold FROM sim_balances WHERE asset = %s", (currency,))
                row = cur.fetchone()
                cur.close()
                return (float(row[0]), float(row[1])) if row else (0.0, 0.0)
        except Exception:
            return (0.0, 0.0)

    # ── Orders (simulation only) ─────────────────────────────────────

    def create_limit_order(
        self,
        symbol: str,
        side: str,
        price: float,
        size: float,
        post_only: bool = False,
        tif: str = "GTC",
        hidden: bool = False,
        iceberg: bool = False,
    ) -> str:
        """Place a simulated limit order. Applies/creates holds atomically."""
        with self.db_lock:
            return self._create_limit_order_internal(symbol, side, price, size, post_only, tif, hidden, iceberg)
    
    def _create_limit_order_internal(
        self,
        symbol: str,
        side: str,
        price: float,
        size: float,
        post_only: bool,
        tif: str,
        hidden: bool,
        iceberg: bool
    ) -> str:
        """Internal method to create limit order."""
        side = side.lower()
        if side not in ("buy", "sell"):
            raise ValueError("side must be 'buy' or 'sell'")

        order_id = f"SIM_{side.upper()}_{int(time.time() * 1000)}"
        base, quote = _split_symbol(symbol)

        d_price = _q8(Decimal(str(price)))
        d_size = _q8(Decimal(str(size)))

        try:
            with get_db_connection() as conn:
                cur = conn.cursor()

                # Ensure balance rows exist
                cur.execute("INSERT INTO sim_balances (asset, available, hold) VALUES (%s, 0, 0) ON CONFLICT (asset) DO NOTHING", (quote,))
                cur.execute("INSERT INTO sim_balances (asset, available, hold) VALUES (%s, 0, 0) ON CONFLICT (asset) DO NOTHING", (base,))

                if side == "buy":
                    # Hold quote equal to notional
                    notional = float(_q8(d_price * d_size))
                    
                    # Check available balance
                    cur.execute("SELECT available FROM sim_balances WHERE asset = %s", (quote,))
                    row = cur.fetchone()
                    available = float(row[0]) if row else 0.0
                    
                    if available < notional:
                        raise ValueError(
                            f"Insufficient {quote} available to place BUY hold: need {notional:.8f}, have {available:.8f}"
                        )
                    
                    cur.execute(
                        """
                        UPDATE sim_balances
                        SET available = available - %s,
                            hold = hold + %s
                        WHERE asset = %s
                        """,
                        (notional, notional, quote)
                    )
                else:
                    # Hold base units equal to size
                    size_float = float(d_size)
                    
                    # Check available balance
                    cur.execute("SELECT available FROM sim_balances WHERE asset = %s", (base,))
                    row = cur.fetchone()
                    available = float(row[0]) if row else 0.0
                    
                    if available < size_float:
                        raise ValueError(
                            f"Insufficient {base} available to place SELL hold: need {size_float:.8f}, have {available:.8f}"
                        )
                    
                    cur.execute(
                        """
                        UPDATE sim_balances
                        SET available = available - %s,
                            hold = hold + %s
                        WHERE asset = %s
                        """,
                        (size_float, size_float, base)
                    )

                # Insert order row
                cur.execute(
                    """
                    INSERT INTO sim_orders (id, symbol, side, price, size, filled_size, status, created_at, updated_at)
                    VALUES (%s, %s, %s, %s, %s, %s, 'open', NOW(), NOW())
                    """,
                    (order_id, symbol, side, float(d_price), float(d_size), 0.0)
                )

                conn.commit()
                cur.close()
                logger.info(f"[ORDER] Created {side} {d_size} {symbol} @ {d_price} (id {order_id})")
                return order_id
        except Exception as e:
            logger.error(f"[ERROR] create_limit_order failed: {e}")
            raise

    def create_market_order(self, symbol: str, side: str, size: float) -> str:
        """Simulate market order placement using current best price. Abort if price is 0."""
        price = self.best_ask_price(symbol) if side.lower() == "buy" else self.best_bid_price(symbol)
        price = float(price or 0.0)
        if price <= 0.0:
            raise ValueError(f"Cannot place market {side} for {symbol}: no valid price (got {price}).")
        return self.create_limit_order(symbol, side, price, size)

    def cancel_order(self, order_id: str) -> Dict:
        """Cancel an open/partial order and release remaining holds."""
        with self.db_lock:
            return self._cancel_order_internal(order_id)
    
    def _cancel_order_internal(self, order_id: str) -> Dict:
        """Internal method to cancel order."""
        try:
            with get_db_connection() as conn:
                cur = conn.cursor()
                
                # Lock the order row
                cur.execute(
                    """
                    SELECT symbol, side, price, size, COALESCE(filled_size,0)
                    FROM sim_orders
                    WHERE id = %s
                    FOR UPDATE
                    """,
                    (order_id,)
                )
                r = cur.fetchone()
                
                if not r:
                    cur.close()
                    return {"code": "400000", "msg": "Order not found"}

                symbol, side, price, size, filled = r
                base, quote = _split_symbol(symbol)
                d_price = Decimal(str(price))
                d_size = Decimal(str(size))
                d_filled = Decimal(str(filled))
                remaining_qty = max(ZERO, d_size - d_filled)

                if side == "buy":
                    remaining_notional = float(_q8(d_price * remaining_qty))
                    # Release quote hold: hold -> available
                    cur.execute(
                        """
                        UPDATE sim_balances
                        SET available = available + %s,
                            hold = GREATEST(hold - %s, 0)
                        WHERE asset = %s
                        """,
                        (remaining_notional, remaining_notional, quote)
                    )
                else:
                    # Release base hold: hold -> available
                    remaining_float = float(remaining_qty)
                    cur.execute(
                        """
                        UPDATE sim_balances
                        SET available = available + %s,
                            hold = GREATEST(hold - %s, 0)
                        WHERE asset = %s
                        """,
                        (remaining_float, remaining_float, base)
                    )

                # Soft delete + status
                cur.execute(
                    """
                    UPDATE sim_orders
                    SET status = 'cancelled',
                        updated_at = NOW(),
                        deleted = TRUE,
                        deleted_at = NOW()
                    WHERE id = %s
                    """,
                    (order_id,)
                )

                conn.commit()
                cur.close()
                logger.info(f"[CANCEL] Order {order_id} cancelled; holds released.")
                return {"code": "200000", "data": {"cancelledOrderId": order_id}}
        except Exception as e:
            logger.error(f"[ERROR] cancel_order failed: {e}")
            return {"code": "400000", "msg": "Cancel failed"}

    def get_order(self, order_id: str) -> Dict:
        """Get simulated order details."""
        with self.db_lock:
            return self._get_order_internal(order_id)
    
    def get_orders(self, status: str = "active"):
        # 🔹 Gets all active orders for the opening risk check.
        with get_db_connection() as conn:
            cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
            cur.execute(f"SELECT * FROM sim_orders WHERE status = '{status}'")
            rows = cur.fetchall()
            cur.close()
            return rows
    
    def _get_order_internal(self, order_id: str) -> Dict:
        """Internal method to get order."""
        try:
            with get_db_connection() as conn:
                cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
                cur.execute(
                    """
                    SELECT id, symbol, side, price, size, filled_size, status, created_at, updated_at, filled_at, deleted, deleted_at
                    FROM sim_orders
                    WHERE id = %s
                    """,
                    (order_id,)
                )
                row = cur.fetchone()
                cur.close()
                
                if not row:
                    return {"code": "400000", "msg": "Order not found"}
                
                data = {
                    "id": row["id"],
                    "symbol": row["symbol"],
                    "side": row["side"],
                    "price": float(row["price"]),
                    "size": float(row["size"]),
                    "filledSize": float(row["filled_size"] or 0.0),
                    "status": row["status"],
                }
                return {"code": "200000", "data": data}
        except Exception as e:
            logger.error(f"Error getting order {order_id}: {e}")
            return {"code": "400000", "msg": "Order not found"}

    # ── Fills (keep the enhanced PostgreSQL version) ─────────────────
    
    def fill_order(self, order_id: str, fill_price: float, fill_qty: float, fee_amount: float = 0.0, role: Optional[str] = None) -> None:
        """
        Execute an order fill atomically:
          - Insert sim_trades
          - Update sim_orders (partial/filled, timestamps, soft-delete on full)
          - Update sim_balances (deduct/add with fee)
        """
        with self.db_lock:
            self._fill_order_internal(order_id, fill_price, fill_qty, fee_amount, role)
    
    def _fill_order_internal(self, order_id: str, fill_price: float, fill_qty: float, fee_amount: float, role: Optional[str]) -> None:
        """Internal method to fill order."""
        try:
            with get_db_connection() as conn:
                cur = conn.cursor()

                # 1) Lock and fetch the order state
                cur.execute(
                    """
                    SELECT status, COALESCE(filled_size,0), size, side, symbol, price
                    FROM sim_orders
                    WHERE id = %s
                    FOR UPDATE
                    """,
                    (order_id,)
                )
                row = cur.fetchone()
                if not row:
                    cur.close()
                    return
                
                status_db, filled_db, size_db, side_db, symbol_db, price_db = row

                if status_db not in ("open", "partial"):
                    cur.close()
                    return

                d_price = _q8(Decimal(str(fill_price)))
                d_qty = _q8(Decimal(str(fill_qty)))
                d_fee = _q8(Decimal(str(fee_amount))) if fee_amount is not None else ZERO

                # Clamp to remaining
                remaining = _q8(Decimal(str(size_db)) - Decimal(str(filled_db)))
                if remaining <= ZERO:
                    cur.close()
                    return
                d_qty = min(d_qty, remaining)

                # 2) Insert trade
                cur.execute(
                    """
                    INSERT INTO sim_trades (timestamp, symbol, side, price, size, sim_order_id, fee)
                    VALUES (NOW(), %s, %s, %s, %s, %s, %s)
                    """,
                    (symbol_db, side_db, float(d_price), float(d_qty), order_id, float(d_fee))
                )

                # 3) Update order
                new_filled = _q8(Decimal(str(filled_db)) + d_qty)
                if new_filled < Decimal(str(size_db)):
                    cur.execute(
                        """
                        UPDATE sim_orders
                        SET filled_size = %s,
                            status = 'partial',
                            updated_at = NOW()
                        WHERE id = %s
                        """,
                        (float(new_filled), order_id)
                    )
                else:
                    cur.execute(
                        """
                        UPDATE sim_orders
                        SET filled_size = size,
                            status = 'filled',
                            filled_at = NOW(),
                            updated_at = NOW(),
                            deleted = TRUE,
                            deleted_at = NOW()
                        WHERE id = %s
                        """,
                        (order_id,)
                    )

                # 4) Balances
                base_asset, quote_asset = symbol_db.split("-")
                notional = _q8(d_price * d_qty)
                
                if side_db == "buy":
                    # BUY: reduce quote hold by filled notional+fee, add base available
                    total_quote = float(_q8(notional + d_fee))
                    cur.execute(
                        """
                        UPDATE sim_balances
                        SET hold = GREATEST(0, hold - %s)
                        WHERE asset = %s
                        """,
                        (total_quote, quote_asset)
                    )
                    cur.execute(
                        """
                        INSERT INTO sim_balances (asset, available, hold)
                        VALUES (%s, %s, 0)
                        ON CONFLICT (asset) DO UPDATE
                        SET available = sim_balances.available + EXCLUDED.available
                        """,
                        (base_asset, float(d_qty))
                    )
                else:
                    # SELL: reduce base hold by filled qty, add quote available (less fee)
                    quote_credit = float(_q8(notional - d_fee))
                    cur.execute(
                        """
                        UPDATE sim_balances
                        SET hold = GREATEST(0, hold - %s)
                        WHERE asset = %s
                        """,
                        (float(d_qty), base_asset)
                    )
                    cur.execute(
                        """
                        UPDATE sim_balances
                        SET available = available + %s
                        WHERE asset = %s
                        """,
                        (quote_credit, quote_asset)
                    )

                conn.commit()
                cur.close()
                logger.info(f"[FILL] {side_db.upper()} {d_qty} {symbol_db} @ {d_price} (fee {d_fee})")
        except Exception as e:
            logger.error(f"[ERROR] fill_order failed: {e}")
            raise

================================================================================
FILE: mm/config/marcus.py
================================================================================
#===================================================================
# 🍁 A R I A N D E           bot version 6.1 file build 20250917.01
#===================================================================
# last update: 2025 | Sept. 17                  Production ready ✅
#===================================================================
# Marcus
# mm/config/marcus.py
#
# Config parameters for the market making bot, Ariadne.
# Trading limits & tolerances.
# Operation mode toggle..
#
# [520] [741] [8]
#===================================================================
# 🜁 THE COMMANDER            ✖ PERSISTANT RUNTIME  ✖ MONIT MANAGED
#===================================================================


from typing import List

# 🔸 Standard Library Imports ======================================

MODE: str = "simulation"
# Options
# >> simulation  | full simulation of live operations.
# >> live        | production environment, real trades, real money.
# >> halted      | no new orders, existing bids/asks are allowed to complete.
# >> drain       | no new orders, existing bids cancelled, positions are liquidated profitable or neutral.
# >> maintenance | trading actions disabled, background processes continue.
# >> shadow      | runs the full decision loop in parallel with live but only logs intents (no orders).
    
# 🔸 Dashboard Progress Bar  =======================================

SHOW_PROGRESS: bool = False
    
# 🔸 Base Standard Parameters  =====================================

INITIAL_CAPITAL: float = 2500.0             # CAD
QUOTE_CURRENCY: str = "USDT"
    
# 🔸 Filters and Scoring ===========================================

MIN_24H_VOLUME: float = 5000000             # 5M USDT minimum volume
MAX_24H_VOLUME: float = 200000000           # 200M USDT maximum volume
MIN_COIN_AGE: int = 7                       # Minimum days of trading required
SPREAD_TIGHTNESS: float = 0.15              # 15% component in Liquidity scoring
ORDER_BOOK_DEPTH: float = 0.15              # 15% component in Liquidity scoring
SLIPPAGE_RESISTANCE: float = 0.10           # 10% component in Liquidity scoring
VOLATILITY_PROFILE: float = 0.15            # 15% component in Market scoring
VOLUME_CONSISTENCY: float = 0.10            # 10% component in Market scoring 
PRICE_STABILITY: float = 0.05               #  5% component in Market scoring
FEE_EFFICIENCY: float = 0.10                # 10% component in Trading scoring 
EXECUTION_SPEED: float = 0.10               # 10% component in Trading scoring 
MARKET_IMPACT: float = 0.10                 # 10% component in Trading scoring
OPPORTUNITY_MOD: float = 0.05               #  5% bonus applied to the overall score
MIN_LIQUIDITY_SCORE: float = 50.0           # Minimum score (0-100) to consider a pair

# 🔸 Cancellation Considerations ===================================

ORDER_REFRESH_SECONDS = 900                 # 15 minutes
STALE_ORDER_HOURS = 6                       # 6 hours
MAX_SPREAD_DRIFT_PCT = 0.02                 # 2%
PRICE_STAGNANT_MINUTES = 10                 # 10 minutes


MAX_EXPOSURE_PER_PAIR: float = 0.1          # 10% of total capital

INVENTORY_DRAWDOWN_LIMIT: float = 0.1       # 10% loss on inventory triggers defense
    
# ── Market Making Parameters ────────────────────────────────────────────────────────────────────
TARGET_SPREAD_PCT: float = 0.01             # 1% spread (was 0.002)
MAX_SPREAD_PCT: float = 0.03                # 3% max spread (was 0.01)
MIN_SPREAD_PCT: float = 0.005               # 0.5% minimum spread
    
# ── Order Management ────────────────────────────────────────────────────────────────────────────
ORDER_REFRESH_SECONDS: int = 600            # Cancel and replace orders older than this
MAX_ORDERS_PER_PAIR: int = 2                # Max concurrent orders per side per pair
ORDER_DISTANCE_FROM_MID: float = 0.005      # Minimum 0.5% distance from mid price
    
# ── Position Management ─────────────────────────────────────────────────────────────────────────
MAX_POSITION_AGE_HOURS: int = 4             # Liquidate positions older than this
POSITION_REDUCTION_SPREAD: float = 0.005    # Tighter spread when reducing position
MAX_ASSET_PCT: float = 0.10                 # 10% per-asset cap (share of portfolio value, ex-cash)
CAP_MARGIN:    float = 0.01                 # +1% tolerance band (allows slight overage before hard-deny)
    
# ── Pair Selection ──────────────────────────────────────────────────────────────────────────────
MIN_PAIR_VOLUME_24H: float = 100000         # Minimum 24h volume in USDT
MAX_PAIRS_TO_TRADE: int = 5                 # Focus on top 5 pairs only
PAIR_ROTATION_INTERVAL: int = 300           # Re-evaluate pairs every 5 minutes

# ── Profitability Targets ───────────────────────────────────────────────────────────────────────
TARGET_DAILY_RETURN: float = 0.0445         # 4.45% daily target
MIN_PROFIT_PER_TRADE: float = 0.001         # 0.1% minimum profit after fees
FEE_BUFFER_MULTIPLIER: float = 2.5          # Spread must be 2.5x the round-trip fees

# ── Advanced Risk Parameters ────────────────────────────────────────────────────────────────────
MAX_DRAWDOWN_PCT: float = 0.1               # 10% maximum equity drawdown
DAILY_LOSS_LIMIT: float = 0.05              # 5% maximum daily loss
MAX_LEVERAGE: float = 1.0                   # No leverage (1.0 = spot only)
MIN_TRADE_SIZE: float = 10.0                # $10 minimum trade size
POSITION_TIMEOUT_HOURS: int = 24            # Hours before considering position stale

# ── Market Making Strategy ──────────────────────────────────────────────────────────────────────
# TARGET_SPREAD_PCT: float = 0.001          # 0.1% target spread in calm markets (was 10 bps)
# MAX_SPREAD_PCT: float = 0.005             # 0.5% max spread during volatility (was 50 bps)
DYNAMIC_SPREAD_MULTIPLIER: float = 1.5      # 1.5x market spread for opportunistic trades

# ── Volatility & Danger Detection ───────────────────────────────────────────────────────────────
VOLATILITY_THRESHOLD: float = 0.02          # 2% price move triggers wider spreads
PANIC_THRESHOLD: float = 0.05               # 5% price move triggers emergency shutdown
PANIC_LOOKBACK_WINDOW: int = 60             # Duration in seconds to measure the panic threshold

# ── Market Selection & Filters ──────────────────────────────────────────────────────────────────


MIN_BOOK_DEPTH_USD: float = 1000.0          # Minimum order book depth in USD
MAX_TOP_WALL_SHARE: float = 0.3             # Maximum allowed top order dominance (30%)

MAX_ACTIVE_PAIRS: int = 10                  # Maximum simultaneous positions

# ── Pre-existing, but not called anywhere ───────────────────────────────────────────────────────
# LIQUIDITY: float = 0.40                     # 40% weight in overall scoring    
# MARKET: float = 0.30                        # 30% weight in overall scoring 
# TRADING: float = 0.30                       # 30% weight in overall scoring 


# ── Operational Settings ────────────────────────────────────────────────────────────────────────
LOOP_DELAY: int = 5                         # Seconds to wait between main loop iterations
LOG_LEVEL: str = "DEBUG"                    # DEBUG, INFO, WARNING, ERROR
CACHE_TTL: int = 30                         # Seconds to cache market data

# ── Emergency Settings ──────────────────────────────────────────────────────────────────────────
MAX_API_RETRIES: int = 3                    # Maximum API retry attempts
HEARTBEAT_TIMEOUT: int = 30                 # Seconds before considering system unresponsive
EMERGENCY_TIMEOUT: int = 300                # Seconds to wait after emergency stop

# ── Exchange & Connection Settings ──────────────────────────────────────────────────────────────
EXCHANGE: str = "kucoin"                    # Currently only kucoin is implemented
API_TIMEOUT: int = 10                       # Seconds before API requests time out
    
# ── Email Server Configuration Settings ─────────────────────────────────────────────────────────
ALERT_EMAIL_ENABLED: bool = True            # Set to False to disable email alerts
ALERT_EMAIL_SMTP_SERVER: str = "smtp.hostinger.com"  
ALERT_EMAIL_SMTP_PORT: int = 465
ALERT_EMAIL_ENCRYPT: str = "SSL"
ALERT_EMAIL_RECIPIENT: str = "james@hodlcorp.io"
# !! Sign-in credentials are located in the dotenv file.
    
# ── Data Manager Windows (in ms) ────────────────────────────────────────────────────────────────
DM_TICK_RETENTION_SECONDS: int = 3600
DM_CANDLE_RETENTION_SECONDS: int = 86400

================================================================================
FILE: mm/config/monit/alma
================================================================================
check process alma with pidfile /root/Echelon/valentrix/mm/utils/tickersticker/alma.pid
  start program = "/bin/bash -lc 'nohup /root/Echelon/valentrix/mm/config/bash/alma.sh >/dev/null 2>&1 &'"
    with timeout 60 seconds
  stop program  = "/bin/bash -lc 'kill -TERM $(cat /root/Echelon/valentrix/mm/utils/tickersticker/alma.pid)'"
  working directory "/root/Echelon/valentrix"
  if does not exist then restart

================================================================================
FILE: mm/config/monit/edith
================================================================================
check process edith with pidfile /root/Echelon/valentrix/mm/utils/partition_manager/edith.pid
  start program = "/bin/bash -lc 'nohup /root/Echelon/valentrix/mm/config/bash/edith.sh >/dev/null 2>&1 &'"
    with timeout 60 seconds
  stop program  = "/bin/bash -lc 'kill -TERM $(cat /root/Echelon/valentrix/mm/utils/partition_manager/edith.pid)'"
  if does not exist then start

================================================================================
FILE: mm/config/monit/laurel
================================================================================
check process laurel with pidfile /root/Echelon/valentrix/mm/utils/canary/laurel.pid
  start program = "/bin/bash -lc 'nohup /root/Echelon/valentrix/mm/config/bash/laurel.sh >/dev/null 2>&1 &'"
    with timeout 60 seconds
  stop program  = "/bin/bash -lc 'kill -TERM $(cat /root/Echelon/valentrix/mm/utils/canary/laurel.pid)'"
  if does not exist then start

================================================================================
FILE: mm/config/bash/laurel.sh
================================================================================
#!/usr/bin/env bash 
set -Eeuo pipefail 
umask 077 
export PYTHONUNBUFFERED=1 

cd /root/Echelon/valentrix 

exec /root/Echelon/bin/python3 -u -m mm.utils.canary.laurel \ 
 >> mm/utils/canary/laurel.log 2>&1

================================================================================
FILE: mm/config/bash/edith.sh
================================================================================
#!/usr/bin/env bash
set -Eeuo pipefail
umask 077
export PYTHONUNBUFFERED=1

cd /root/Echelon/valentrix

exec /root/Echelon/bin/python3 -u -m mm.utils.partition_manager.edith \
  >> mm/utils/partition_manager/partition_manager.log 2>&1

================================================================================
FILE: mm/config/bash/alma.sh
================================================================================
#!/usr/bin/env bash
set -Eeuo pipefail
umask 077
export PYTHONUNBUFFERED=1

cd /root/Echelon/valentrix

exec /root/Echelon/bin/python3 -u -m mm.utils.tickersticker.alma \
  >> mm/utils/tickersticker/alma.log 2>&1

================================================================================
FILE: mm/data/state/state.json
================================================================================
{
  "meta": {
    "version": "1.0",
    "start_time": null,
    "cycle_count": 0
  },
  "equity": {},
  "positions": {},
  "open_orders": {},
  "daily_stats": {},
  "risk_metrics": {},
  "last_actions": {}
}

================================================================================
FILE: mm/data/state/sim_state.json
================================================================================
{
  "meta": {
    "version": "1.0",
    "simulation_start": null,
    "total_cycles": 0
  },
  "balances": {
    "USDT": {
      "available": 2500.00,
      "hold": 0.0
    }
  },
  "performance": {
    "starting_capital": 2500.00,
    "current_equity": 2500.00
  },
  "active_orders": [],
  "position_history": {},
  "realism_stats": {},
  "config_overrides": {}
}

================================================================================
FILE: mm/data/secrets/.env
================================================================================
# [520] [741] [8]

#BestProxy.net
PROXY_USERNAME=figidactual
PROXY_PASSWORD=E8D55D5C814605834F9A71D331ED8ED0
PROXY_HOST=198.1.204.30:12100

# KUCOIN
KUCOIN_API=684815c155fba5000107ef4b
KUCOIN_SEC=0bf6af04-b1b0-4ed3-b8c8-d57c121064f0
KUCOIN_PASSPHRASE=syntient01

# Email Server Sign-in Credentials
# All email accounts are receive restricted - outgoing only.
ARI_USR=ariadne@hodlcorp.io
ARI_PWD=#M4rk3t[-]M4k3r^a01
ARI_NAME= Ariadne Lamarr
DRC_USR=drcalvin@hodlcorp.io
DRC_PWD=M4rk3t[-]M4k3r^d02 
DRC_NAME=Dr. Susan Calvin, B.Ot.
GRA_USR=grayson@hodlcorp.io
GRA_PWD=M4rk3t[-]M4k3r^g03 
GRA_NAME=Grayson Moorcock
NAO_USR=naomi@hodlcorp.io
NAO_PWD=M4rk3t[-]M4k3r^n04 
NAO_NAME=Naomi Nagata
ALE_USR=alec@hodlcorp.io
ALE_PWD=M4rk3t[-]M4k3r^a05 
ALE_NAME=Alec Sadler
MAL_USR=mal@hodlcorp.io
MAL_PWD=M4rk3t[-]M4k3r^m06 
MAL_NAME=Malcolm Reynolds
PET_USR=petra@hodlcorp.io
PET_PWD=M4rk3t[-]M4k3r^p07 
PET_NAME=Petra Arkanian
HEL_USR=helen@hodlcorp.io
HEL_PWD=M4rk3t[-]M4k3r^h08 
HEL_NAME=Helen Magnus
VER_USR=verity@hodlcorp.io
VER_PWD=M4rk3t[-]M4k3r^v09 
VER_NAME=Verity Danton
HAR_USR=hari@hodlcorp.io
HAR_PWD=M4rk3t[-]M4k3r^h10 
HAR_NAME=Hari Seldon
JUL_USR=julius@hodlcorp.io 
JUL_PWD=M4rk3t[-]M4k3r^j11 
JUL_NAME=Julius Deane
CHR_USR=chris@hodlcorp.io
CHR_PWD=M4rk3t[-]M4k3r^c12 
CHR_NAME=Christian Wolff
LAU_USR=laurel@hodlcorp.io
LAU_PWD=M4rk3t[-]M4k3r^l13 
LAU_NAME=Laurel Lance
AND_USR=andi@hodlcorp.io
AND_PWD=M4rk3t[-]M4k3r^a14 
AND_NAME=Andi Clarke
EDI_USR=edith@hodlcorp.io
EDI_PWD=M4rk3t[-]M4k3r^e15 
EDI_NAME=Edith Hathaway
ALM_USR=alma@hodlcorp.io
ALM_PWD=M4rk3t[-]M4k3r^a16 
ALM_NAME=Alma Lane
AGN_USR=agnes@hodlcorp.io
AGN_PWD=M4rk3t[-]M4k3r^a17 
AGN_NAME=Agnus Jurati
MAR_USR=marcus@hodlcorp.io
MAR_PWD=M4rk3t[-]M4k3r^m18 
MAR_NAME=Marcus Eaton
WIN_USR=wintermute@hodlcorp.io
WIN_PWD=M4rk3t[-]M4k3r^w19
WIN_NAME=Wintermute
INA_USR=inara@hodlcorp.io
INA_PWD=M4rk3t[-]M4k3r^i20
INA_NAME=Inara Serra
KAR_USR=karin@hodlcorp.io
KAR_PWD=M4rk3t[-]M4k3r^k21
KAR_NAME=K∙A∙R∙I∙N
#ASH_USR=ash@hodlcorp.io
#ASH_PWD=M4rk3t[-]M4k3r^a22
#ASH_NAME=Ash
#QUO_USR=quorra@hodlcorp.io
#QUO_PWD=M4rk3t[-]M4k3r^q23
#QUO_NAME=Quorra
#VIK_USR=viki@hodlcorp.io
#VIK_PWD=M4rk3t[-]M4k3r^v24
#VIK_NAME=V∙I∙K∙I
#LAM_USR=lamar@hodlcorp.io
#LAM_PWD=M4rk3t[-]M4k3r^l25
#LAM_NAME=Lamar Burgess







================================================================================
FILE: mm/utils/tickersticker/alma.pid
================================================================================
3496182

================================================================================
FILE: mm/utils/tickersticker/alma.py
================================================================================
#===================================================================
# 🍁 A R I A N D E           bot version 6.1 file build 20250905.01
#===================================================================
# last update: 2025 | Sept. 5                   Production ready ✅
#===================================================================
# Alma
# mm/utils/tickersticker/alma.py             
#
#🔺 THIS FILE IS MISSION CRITICAL 🔺
#
# Captures market data every 3 seconds and stores to db.
#
# [520] [741] [8]                   
#===================================================================
# 🔰 THE COMMANDER            ✔ PERSISTANT RUNTIME  ✔ MONIT MANAGED
#===================================================================

# 🔸 Standard Library Imports ======================================

import importlib
import smtplib
import ssl
import uuid 
import time
import psycopg2
import os
import logging
import signal
import sys
import atexit
from datetime import datetime
from mm.conn.conn_kucoin import KucoinClient
from mm.utils.helpers.wintermute import update_heartbeat
from mm.utils.helpers.wintermute import now_local  # local tz helper 
from email.message import EmailMessage
from email.utils import formataddr
from zoneinfo import ZoneInfo
    
# 🔸 third-party imports ===========================================

from dotenv import load_dotenv

# 🔸 local application imports =====================================

import mm.config.marcus as marcus

# 🔸 load env for this process =====================================

load_dotenv("mm/data/secrets/.env")

# 🔸 heartbeat tracer logger =======================================

_hb_logger = logging.getLogger("alma_hb")
_hb_logger.setLevel(logging.INFO)
_log_path = "mm/utils/tickersticker/alma_heartbeats.log"
os.makedirs(os.path.dirname(_log_path), exist_ok=True)
if not _hb_logger.handlers:
    _fh = logging.FileHandler(_log_path)
    _fh.setFormatter(logging.Formatter('%(message)s'))
    _hb_logger.addHandler(_fh)

# 🔸 Global shutdown flag ==========================================

shutdown_requested = False

def signal_handler(signum, frame):
    """Handle SIGTERM and SIGINT for graceful shutdown"""
    global shutdown_requested
    print(f"[SHUTDOWN] Received signal {signum}, shutting down gracefully...")
    shutdown_requested = True

signal.signal(signal.SIGTERM, signal_handler)
signal.signal(signal.SIGINT,  signal_handler)

# 🔸 PID support for MONIT =========================================

PID_FILE = "/root/Echelon/valentrix/mm/utils/tickersticker/alma.pid"

def _cleanup_pidfile():
    try:
        if os.path.exists(PID_FILE):
            os.remove(PID_FILE)
    except Exception:
        pass

# 🔸 remove stale pid if dead ======================================

if os.path.exists(PID_FILE):
    try:
        with open(PID_FILE) as f:
            old = f.read().strip()
        if old.isdigit() and not os.path.exists(f"/proc/{old}"):
            os.remove(PID_FILE)
    except Exception:
        pass

# 🔸 write our pid =================================================

try:
    with open(PID_FILE, "w") as f:
        f.write(str(os.getpid()))
except Exception as e:
    print(f"[PID ERROR] {e}", file=sys.stderr)
    sys.exit(1)

atexit.register(_cleanup_pidfile)

DB_NAME = "ariadne"
TABLE_NAME = "tickstick"
INTERVAL = 3

client = KucoinClient()

FIELDS = [
    "symbol", "symbol_name", "buy", "sell", "last", "best_bid_size", "best_ask_size",
    "change_rate", "change_price", "high", "low", "vol", "vol_value",
    "average_price", "taker_fee_rate", "maker_fee_rate", "taker_coefficient", "maker_coefficient"
]

def get_connection():
    """Create PostgreSQL connection"""
    try:
        conn = psycopg2.connect(database=DB_NAME, user="postgres", host="localhost")
        return conn
    except psycopg2.Error as e:
        print(f"[DB ERROR] Failed to connect: {e}")
        sys.exit(1)

def insert_rows(conn, timestamp, tickers):
    """Insert ticker data into PostgreSQL"""
    
    # Get current UTC timestamp
    timestamp = int(datetime.utcnow().timestamp())

    rows = []
    for t in tickers:
        try:
            symbol = t.get("symbol", "")
            if not symbol.endswith("-USDT"):
                continue

            vol_value = float(t.get("volValue", 0.0))
            if not (2_000_000 <= vol_value < 200_000_000):
                continue

            # Map API fields to DB columns
            row = [
                timestamp,
                t.get("symbol"),
                t.get("symbolName"),
                float(t.get("buy", 0)) if t.get("buy") else None,
                float(t.get("sell", 0)) if t.get("sell") else None,
                float(t.get("last", 0)) if t.get("last") else None,
                float(t.get("bestBidSize", 0)) if t.get("bestBidSize") else None,
                float(t.get("bestAskSize", 0)) if t.get("bestAskSize") else None,
                float(t.get("changeRate", 0)) if t.get("changeRate") else None,
                float(t.get("changePrice", 0)) if t.get("changePrice") else None,
                float(t.get("high", 0)) if t.get("high") else None,
                float(t.get("low", 0)) if t.get("low") else None,
                float(t.get("vol", 0)) if t.get("vol") else None,
                float(t.get("volValue", 0)) if t.get("volValue") else None,
                float(t.get("averagePrice", 0)) if t.get("averagePrice") else None,
                float(t.get("takerFeeRate", 0)) if t.get("takerFeeRate") else None,
                float(t.get("makerFeeRate", 0)) if t.get("makerFeeRate") else None,
                float(t.get("takerCoefficient", 0)) if t.get("takerCoefficient") else None,
                float(t.get("makerCoefficient", 0)) if t.get("makerCoefficient") else None
            ]
            
            # Bounds check for numeric overflow
            MAX_VAL = 999999999999
            skip = False
            for i, val in enumerate(row):
                if isinstance(val, (int, float)) and val > MAX_VAL:
                    print(f"[OVERFLOW SKIP] {symbol} field index {i} = {val}")
                    skip = True
                    break

            if not skip:
                rows.append(tuple(row))
        
        except Exception as e:
            print(f"[PARSE ERROR] {t.get('symbol', '?')} → {e}")

    if rows:
        cursor = conn.cursor()
        try:
            placeholders = ",".join(["%s"] * 19)  # 19 columns total
            sql = f"""
            INSERT INTO {TABLE_NAME} 
            (timestamp, symbol, symbol_name, buy, sell, last, best_bid_size, best_ask_size,
             change_rate, change_price, high, low, vol, vol_value, average_price,
             taker_fee_rate, maker_fee_rate, taker_coefficient, maker_coefficient)
            VALUES ({placeholders})
            """
            
            #--- temp debug tracer ---
            for row in rows:
                for i, val in enumerate(row):
                    if isinstance(val, float) and abs(val) >= 1e12:
                        print(f"[DEBUG] Overflow risk → index: {i}, value: {val}")
            #--- end debug tracer ---
            
            cursor.executemany(sql, rows)
            conn.commit()
            return len(rows)
        except psycopg2.Error as e:
            print(f"[INSERT ERROR] {e}")
            conn.rollback()
            return 0
        finally:
            cursor.close()
    return 0

# 🔸 Drop-in Email Sender ==========================================

def send_email(subject: str, status: str, title: str, message: str) -> str:

    importlib.reload(marcus)
    if not bool(getattr(marcus, "ALERT_EMAIL_ENABLED", False)):
        return "disabled"
    if str(getattr(marcus, "ALERT_EMAIL_ENCRYPT", "SSL")).upper() != "SSL":
        return "Simple Mail Transfer Protocol not established. No conn."

    host = getattr(marcus, "ALERT_EMAIL_SMTP_SERVER", None)
    port = getattr(marcus, "ALERT_EMAIL_SMTP_PORT", None)
    recipient = getattr(marcus, "ALERT_EMAIL_RECIPIENT", None)

    USERCODE = "ALM"  # hardcode per file

    # ---- Edit Sender Info (per file) ----
    user = os.getenv(f"{USERCODE}_USR")
    pwd = os.getenv(f"{USERCODE}_PWD")
    sender_email = user
    sender_name = os.getenv(f"{USERCODE}_NAME")
    # -------------------------------------

    # status color map
    STATUS_COLORS = {
        "STATCON3": "#F1C232",	# on the first missing heartbeat 
        "STATCON2": "#E69138",	# on the second missing heartbeat
        "STATCON1": "#CC0000",	# on the third missing heartbeat
        "SIGCON1": 	"#FB6D8B",	# Process never started
		"OPSCON5": 	"#F5F5F5",	# Normal, all systems nominal
        "OPSCON1": 	"#990000",	# Issues detected
    }
    status_text = str(status).upper()
    status_color = STATUS_COLORS.get(status_text, "#BE644C")

    msg = EmailMessage()
    domain = sender_email.split("@")[1] if "@" in sender_email else "hodlcorp.io"
    msg_id = f"<{uuid.uuid4()}@{domain}>"
    msg["Message-ID"] = msg_id
    msg["From"] = formataddr((sender_name, sender_email))
    msg["To"] = recipient
    msg["Subject"] = subject
    msg["X-Priority"] = "1"
    msg["X-MSMail-Priority"] = "High"
    msg["Importance"] = "High"

    # footer fields
    now_tz = datetime.now(ZoneInfo("America/Toronto"))
    sent_str = now_tz.strftime("%Y-%m-%d %H:%M:%S America/Toronto")
    epoch_ms = int(now_tz.timestamp() * 1000)
    mid_clean = msg_id.strip("<>").split("@", 1)[0]

    # full HTML body (single block)
    html_body = f"""
<div style="font-family: monospace;">
  <table role="presentation" width="100%" height="20px" cellpadding="8px" cellspacing="0" border="0">
    <!-- Top Banner -->
    <tr style="font-family: Georgia, 'Times New Roman', Times, serif;font-size:20px;font-weight:600;background-color:#333;">
      <td align="left" style="color:#EFEFEF;letter-spacing:12px;">INTCOMM</td>
      <td align="right" style="color:{status_color};letter-spacing:4px;">{status_text}</td>
    </tr>

    <!-- Message Title -->
    <tr width="100%" cellpadding="6px" style="font-family: Tahoma, Geneva, sans-serif;text-align:left;font-size:14px;font-weight:600;color:#333;">
      <td colspan="2">
        {title}
      </td>
    </tr>

    <!-- Message Content -->
    <tr width="100%" cellpadding="6px" style="font-family: Tahoma, Geneva, sans-serif;text-align:left;font-size:11px;font-weight:400;line-height:1.5;color:#333;">
      <td colspan="2">
        {message}
      </td>
    </tr>

    <!-- UNUSED SPACER ROW -->
    <tr width="100%" height="25px"><td colspan="2">&nbsp;</td></tr>
  </table>

  <!-- Footer -->
  <table role="presentation" width="400px" height="20px" cellpadding="4" cellspacing="0" border="0" style="font-family: Tahoma, Geneva, sans-serif;">
    <!-- DOCINT -->
    <tr style="background-color:#333;">
      <td colspan="2" style="color:#efefef;font-size:12px;font-weight:600;">DOCINT</td>
    </tr>

    <tr style="background-color:#E9E9E5;">
      <td width="30px" style="color:#333;font-size:10px;font-weight:600;">SENT</td>

      <td width="10px" style="color:#333;font-size:10px;font-weight:600;">&rarr;</td>
      <td style="color:#333;font-size:11px;font-weight:400;">{sent_str}</td>
    </tr>

    <tr style="background-color:#F2F2F0;">
      <td width="30px" style="color:#333;font-size:10px;font-weight:600;">EPOCH</td>
      <td width="10px" style="color:#333;font-size:10px;font-weight:600;">&rarr;</td>
      <td style="color:#333;font-size:11px;font-weight:400;">{epoch_ms} (ms since 1970/01/01 0:00 UTC)</td>
    </tr>

    <tr style="background-color:#E9E9E5;">
      <td width="30px" style="color:#333;font-size:10px;font-weight:600;">m.ID</td>
      <td width="10px" style="color:#333;font-size:10px;font-weight:600;">&rarr;</td>
      <td style="color:#333;font-size:11px;font-weight:400;">{mid_clean}</td>
    </tr>
  </table>
</div>
"""

    msg.add_alternative(html_body, subtype="html")

    ctx = ssl.create_default_context()
    with smtplib.SMTP_SSL(host, port, context=ctx, timeout=10) as s:
        if user and pwd:
            s.login(user, pwd)
        s.send_message(msg)

    return msg_id

def main():
    """Main loop"""
    
    conn = get_connection()
    
    cycle_count = 0

    try:
        while not shutdown_requested:
            start = time.time()
            start_dt = now_local()  # for tracer line

            try:
                now_ts = int(start)
                tickers = client.get_all_tickers()

                inserted = insert_rows(conn, now_ts, tickers)

                cycle_count += 1

                # Update heartbeat every 20 cycles (every minute at 3s intervals)
                hbs_str = "-"
                if cycle_count % 10 == 0:
                    update_heartbeat("alma", conn)
                    hbs_str = now_local().strftime("%H:%M:%S")

            except psycopg2.OperationalError as e:
                print(f"[DB FATAL] Database connection lost: {e}")
                try:
                    send_email(
                        subject="[ STATCON1 ] Alma executed a corrective exit.",
                        status="STATCON1",
                        title="Database Connection Error Triggering a Corrective Exit",
                        message=f"<p><b>Alma was unable to connect to the database, the reported error was:</b><br><i>{e}</i></p><p>This exit was coded in to prevent stalling, infinite loops, and other outcomes that prevent Monit from knowing Alma is stuck. Monit <b><i>should</i></b> restart Alma.</p><p>Please ensure that this is the case by logging onto the server and using the command:<br><i>sudo monit status alma</i></p>",
                    )
                except:
                    pass  # Don't let email failure prevent exit
                sys.exit(1)
            except psycopg2.InterfaceError as e:
                print(f"[DB FATAL] Database interface error: {e}")
                try:
                    send_email(
                        subject="[ STATCON1 ] Alma executed a corrective exit.",
                        status="STATCON1",
                        title="Database Interface Error Triggering a Corrective Exit",
                        message=f"<p><b>Alma was unable to interface with the database, the reported error was:</b><br><i>{e}</i></p><p>This exit was coded in to prevent stalling, infinite loops, and other outcomes that prevent Monit from knowing Alma is stuck. Monit <b><i>should</i></b> restart Alma.</p><p>Please ensure that this is the case by logging onto the server and using the command:<br><i>sudo monit status alma</i></p>",
                    )
                except:
                    pass
                sys.exit(1)
            except Exception as e:
                # Check for KuCoin connection errors
                error_str = str(e)
                if "SOCKSHTTPSConnectionPool" in error_str or "Max retries exceeded" in error_str:
                    print(f"[KUCOIN FATAL] API connection failed: {e}")
                    try:
                        send_email(
                            subject="[ STATCON1 ] Alma executed a corrective exit.",
                            status="STATCON1",
                            title="API Connection to KuCoin Failed",
                            message=f"<p><b>Alma was unable to fetch data from KuCoin via the API, the reported error was:</b><br><i>{e}</i></p><p>This exit was coded in to prevent stalling, infinite loops, and other outcomes that prevent Monit from knowing Alma is stuck. Monit <b><i>should</i></b> restart Alma.</p><p>Please ensure that this is the case by logging onto the server and using the command:<br><i>sudo monit status alma</i></p>",
                        )
                    except:
                        pass
                    sys.exit(1)
                elif "connection already closed" in error_str:
                    print(f"[DB RECONNECT] Connection closed, attempting reconnect: {e}")
                    try:
                        conn.close()  # Clean up the dead connection
                    except:
                        pass

                    try:
                        conn = get_connection()  # Get new connection
                        print("[DB RECONNECT] Successfully reconnected to database")
                    except Exception as reconnect_error:
                        print(f"[DB FATAL] Reconnection failed: {reconnect_error}")
                        try:
                            send_email(
                                subject="[ STATCON1 ] Alma executed a corrective exit.",
                                status="STATCON1",
                                title="Database Connection was Closed, Reconnection Failed.",
                                message=f"<p><b>The connection to the database that Alma was using was closed. She attempted to open a new connection, but that failed as well. The reported error was:</b><br><i>{e}</i></p><p>This exit was coded in to prevent stalling, infinite loops, and other outcomes that prevent Monit from knowing Alma is stuck. Monit <b><i>should</i></b> restart Alma.</p><p>Please ensure that this is the case by logging onto the server and using the command:<br><i>sudo monit status alma</i></p>"
                            )
                        except:
                            pass
                    sys.exit(1)
                else:
                    print(f"[LOOP ERROR] {e}")

            # Fixed 3s cycle, sleep accounts for execution time (no incremental sleep)
            elapsed = (time.time() - start)
            sleep_time = max(0.0, INTERVAL - elapsed)  # INTERVAL = 3.0s

            # Tracer line: Cycle No. | Start | Runtime | Sleep | HBS
            runtime_ms = int(round(elapsed * 1000))
            sleep_ms = int(round(sleep_time * 1000))
            
            if sleep_time > 0.0:
                time.sleep(sleep_time)

    except Exception as e:
        print(f"[FATAL ERROR] {e}")
    finally:
        _cleanup_pidfile()
        conn.close()
        
if __name__ == "__main__":
    main()

================================================================================
FILE: mm/utils/market_state/viki.py
================================================================================
#>> A R I A N D E v6
#>> last update: 2025 | Sept. 4
#>>
#>> Viki
#>> mm/utils/market_state/viki.py
#>>
#>> Classifies the current market regime for a given 
#>> symbol using exchange data (candles, order book) 
#>> and derives a label + confidence.
#>> Designed to be called by Quorra (risk scoring) 
#>> and Hari (simulation realism).
#>>
#>> Auth'd -> Commander
#>>
#>> [520] [741] [8]
#>>────────────────────────────────────────────────────

# Build|20250904.01

from __future__ import annotations
import math
import statistics
import logging
import json
from typing import Dict, List, Tuple

from mm.conn.conn_kucoin import KucoinClient
from mm.utils.market_state.viki_config import (
    LOGGING,
    LOG_PATH,
    RSI_LEN,
    BOLL_LEN,
    ATR_LEN,
    LOOKBACK_SHORT,
    LOOKBACK_LONG,
    PANIC_THRESH,
    CAPITULATION_THRESH,
    EUPHORIA_THRESH,
    STALE_THRESH,
    WOLF_THRESH,
    CORRECTION_DROP_PCT,
)
from mm.utils.helpers.inara import get_mode

logger = logging.getLogger("MarketState")

# KuCoin candle format: [time, open, close, high, low, volume, turnover]
OPEN=1; CLOSE=2; HIGH=3; LOW=4; VOL=5


class MarketState:
    """
    Determines market regime for a symbol from recent data.

    Methods
    -------
    classify(symbol: str) -> Dict
        Returns { 'symbol', 'mode', 'state', 'confidence', 'evidence': {...} }
    """

    def __init__(self):
        self.client = KucoinClient()

    # ── Public API ───────────────────────────────────────────────

    def classify(self, symbol: str) -> Dict:
        """Compute market state and confidence for a symbol.
        Returns a dict suitable for logging and programmatic use.
        """
        sym = symbol.upper()

        # Data pulls
        c15 = self._get_candles(sym, "15m", max(LOOKBACK_LONG, 100))  # 15m granularity
        c1h = self._get_candles(sym, "1h",  max(LOOKBACK_LONG, 100))  # hourly context
        c1d = self._get_candles(sym, "1d",  60)                       # multi-day context
        book = self.client.order_book(sym, depth=10)

        # Feature extraction
        features = {}
        features.update(self._price_features(c15, c1h, c1d))
        features.update(self._vol_features(c15))
        features.update(self._band_features(c15))
        features.update(self._orderbook_features(book))

        # State scoring
        scores = {
            "panic":        self._score_panic(features),
            "capitulation": self._score_capitulation(features),
            "euphoria":     self._score_euphoria(features),
            "stale":        self._score_stale(features),
            "wolf":         self._score_wolf(features),
            "correction":   self._score_correction(features),
            "normal":       0.0,  # filled below as 100 - max others if low signal
        }
        max_label, max_score = max(scores.items(), key=lambda kv: kv[1])
        if max_score < 25:
            # If nothing is strong, treat as normal; confidence = 100 - sum of variances
            scores["normal"] = 60.0
            max_label, max_score = "normal", 60.0

        out = {
            "symbol": sym,
            "mode": get_mode(),
            "state": max_label,
            "confidence": round(float(max_score), 1),
            "evidence": {**features, "state_scores": scores},
        }

        if LOGGING:
            self._log(out)
        return out

    # ── Data helpers ─────────────────────────────────────────────

    def _get_candles(self, symbol: str, timeframe: str, limit: int) -> List[List[float]]:
        arr = self.client.historical_ohlcv(symbol, timeframe, limit)
        # KuCoin returns newest first; reverse to chronological
        return list(reversed(arr))

    # ── Feature engineering ──────────────────────────────────────

    def _price_features(self, c15: List[List[float]], c1h: List[List[float]], c1d: List[List[float]]) -> Dict:
        f: Dict[str, float] = {}

        def closes(c):
            return [float(x[CLOSE]) for x in c]

        def highs(c):
            return [float(x[HIGH]) for x in c]

        def lows(c):
            return [float(x[LOW]) for x in c]

        cl15 = closes(c15)
        cl1h = closes(c1h)
        cl1d = closes(c1d)

        # Returns
        def pct(a,b):
            return 0.0 if b==0 else (a/b - 1.0) * 100.0

        f["ret_15m"] = pct(cl15[-1], cl15[max(0, -LOOKBACK_SHORT)])
        f["ret_1h"]  = pct(cl1h[-1],  cl1h[max(0, -LOOKBACK_SHORT)])
        f["ret_4h"]  = pct(cl1h[-1],  cl1h[max(0, -min(4, len(cl1h)-1))])

        # Drawdown from recent highs
        recent_high_1d = max(highs(c1d)) if c1d else cl1h[-1]
        f["drop_from_1d_high_pct"] = pct(cl1h[-1], recent_high_1d)  # negative if below high

        # RSI 14 on 15m
        f["rsi_15m"] = self._rsi(cl15, RSI_LEN)

        # Reversal frequency (sign flips) last N
        flips = 0
        for i in range(1, min(LOOKBACK_SHORT, len(cl15))):
            if (cl15[i] - cl15[i-1]) * (cl15[i-1] - cl15[i-2] if i-2>=0 else 0) < 0:
                flips += 1
        f["sign_flips_15m"] = float(flips)

        # True range proxy over 15m
        tr = []
        prev_close = None
        for x in c15:
            high, low, close = float(x[HIGH]), float(x[LOW]), float(x[CLOSE])
            if prev_close is None:
                tr.append(high - low)
            else:
                tr.append(max(high - low, abs(high - prev_close), abs(low - prev_close)))
            prev_close = close
        f["atr_15m"] = statistics.fmean(tr[-ATR_LEN:]) if len(tr) >= ATR_LEN else statistics.fmean(tr) if tr else 0.0

        # Net change tightness (for wolf/stale discrimination)
        f["net_change_15m"] = cl15[-1] - cl15[-LOOKBACK_SHORT] if len(cl15) > LOOKBACK_SHORT else cl15[-1] - cl15[0]

        return f

    def _vol_features(self, c15: List[List[float]]) -> Dict:
        f: Dict[str, float] = {}
        vols = [float(x[VOL]) for x in c15]
        if not vols:
            f.update({"vol_z": 0.0, "vol_surge_ratio": 0.0})
            return f
        m = statistics.fmean(vols)
        s = statistics.pstdev(vols) or 1.0
        f["vol_z"] = (vols[-1] - m) / s
        base = statistics.fmean(vols[-max(5, min(20, len(vols)//3)):]) or 1.0
        f["vol_surge_ratio"] = vols[-1] / base
        return f

    def _band_features(self, c15: List[List[float]]) -> Dict:
        f: Dict[str, float] = {}
        closes = [float(x[CLOSE]) for x in c15]
        if len(closes) < BOLL_LEN:
            f.update({"bb_pos": 0.0, "bb_width_pct": 0.0})
            return f
        ma = statistics.fmean(closes[-BOLL_LEN:])
        std = statistics.pstdev(closes[-BOLL_LEN:]) or 1e-9
        upper = ma + 2*std
        lower = ma - 2*std
        last = closes[-1]
        # Position inside bands: <0 below lower, >1 above upper
        bb_pos = (last - lower) / (upper - lower)
        f["bb_pos"] = bb_pos
        f["bb_width_pct"] = (upper - lower) / last * 100.0
        return f

    def _orderbook_features(self, book: Dict) -> Dict:
        f: Dict[str, float] = {}
        bids = book.get("bids", [])
        asks = book.get("asks", [])
        bid_px = float(bids[0][0]) if bids else 0.0
        ask_px = float(asks[0][0]) if asks else 0.0
        mid = (bid_px + ask_px) / 2 if bid_px and ask_px else 0.0
        spread = (ask_px - bid_px) / mid * 100.0 if mid else 0.0
        bid_vol = sum(v for _, v in bids)
        ask_vol = sum(v for _, v in asks)
        imb = (bid_vol - ask_vol) / (bid_vol + ask_vol + 1e-9)
        f["spread_pct"] = spread
        f["book_imbalance"] = imb
        return f

    # ── Indicator calcs ──────────────────────────────────────────

    def _rsi(self, closes: List[float] | List[List[float]], length: int = 14) -> float:
        if closes and isinstance(closes[0], list):
            closes = [float(x[CLOSE]) for x in closes]  # type: ignore
        if len(closes) <= length:
            return 50.0
        gains, losses = [], []
        for i in range(1, len(closes)):
            delta = closes[i] - closes[i-1]
            if delta >= 0:
                gains.append(delta)
                losses.append(0.0)
            else:
                gains.append(0.0)
                losses.append(-delta)
        avg_gain = statistics.fmean(gains[-length:]) or 1e-9
        avg_loss = statistics.fmean(losses[-length:]) or 1e-9
        rs = avg_gain / avg_loss if avg_loss else float('inf')
        rsi = 100.0 - (100.0 / (1.0 + rs))
        return rsi

    # ── State scorers ────────────────────────────────────────────

    def _score_panic(self, f: Dict[str, float]) -> float:
        score = 0.0
        # Strong negative returns + volume spike + widening spread
        if f.get("ret_15m", 0.0) <= PANIC_THRESH["ret_15m"]:
            score += min(40.0, abs(f["ret_15m"]) * 4)
        if f.get("vol_z", 0.0) >= PANIC_THRESH["vol_z_min"]:
            score += min(30.0, (f["vol_z"] - PANIC_THRESH["vol_z_min"]) * 8)
        if f.get("spread_pct", 0.0) >= PANIC_THRESH["spread_pct_min"]:
            score += min(20.0, (f["spread_pct"] - PANIC_THRESH["spread_pct_min"]) * 2)
        if f.get("book_imbalance", 0.0) < -0.2:
            score += 10.0
        return max(0.0, min(100.0, score))

    def _score_capitulation(self, f: Dict[str, float]) -> float:
        score = 0.0
        if f.get("ret_1h", 0.0) <= CAPITULATION_THRESH["ret_1h"]:
            score += min(40.0, abs(f["ret_1h"]) * 3)
        if f.get("drop_from_1d_high_pct", 0.0) <= -CORRECTION_DROP_PCT:
            score += 20.0
        if f.get("vol_z", 0.0) >= CAPITULATION_THRESH["vol_z_min"]:
            score += min(25.0, (f["vol_z"] - CAPITULATION_THRESH["vol_z_min"]) * 6)
        if f.get("rsi_15m", 50.0) <= CAPITULATION_THRESH["rsi_max"]:
            score += 15.0
        if f.get("bb_pos", 0.5) < 0.0:  # below lower band decisively
            score += 10.0
        return max(0.0, min(100.0, score))

    def _score_euphoria(self, f: Dict[str, float]) -> float:
        score = 0.0
        if f.get("ret_1h", 0.0) >= EUPHORIA_THRESH["ret_1h"]:
            score += min(40.0, f["ret_1h"] * 3)
        if f.get("vol_z", 0.0) >= EUPHORIA_THRESH["vol_z_min"]:
            score += min(25.0, (f["vol_z"] - EUPHORIA_THRESH["vol_z_min"]) * 6)
        if f.get("rsi_15m", 50.0) >= EUPHORIA_THRESH["rsi_min"]:
            score += 15.0
        if f.get("bb_pos", 0.5) > 1.0:  # above upper band
            score += 10.0
        return max(0.0, min(100.0, score))

    def _score_stale(self, f: Dict[str, float]) -> float:
        score = 0.0
        # Low volatility, narrow band width, low volume, tight spread
        if abs(f.get("ret_15m", 0.0)) < STALE_THRESH["ret_abs_max"]:
            score += 25.0
        if f.get("bb_width_pct", 100.0) < STALE_THRESH["bb_width_max"]:
            score += 25.0
        if f.get("vol_surge_ratio", 1.0) < STALE_THRESH["vol_ratio_max"]:
            score += 25.0
        if f.get("spread_pct", 100.0) < STALE_THRESH["spread_pct_max"]:
            score += 25.0
        return max(0.0, min(100.0, score))

    def _score_wolf(self, f: Dict[str, float]) -> float:
        score = 0.0
        # High intraday volatility (ATR or band width), many sign flips, small net change
        if f.get("bb_width_pct", 0.0) >= WOLF_THRESH["bb_width_min"]:
            score += min(35.0, (f["bb_width_pct"] - WOLF_THRESH["bb_width_min"]) * 1.0)
        if f.get("sign_flips_15m", 0.0) >= WOLF_THRESH["flips_min"]:
            score += min(35.0, (f["sign_flips_15m"] - WOLF_THRESH["flips_min"]) * 4)
        if abs(f.get("net_change_15m", 0.0)) <= WOLF_THRESH["net_change_abs_max"]:
            score += 30.0
        return max(0.0, min(100.0, score))

    def _score_correction(self, f: Dict[str, float]) -> float:
        score = 0.0
        # 10%+ off recent highs and negative return context
        if f.get("drop_from_1d_high_pct", 0.0) <= -CORRECTION_DROP_PCT:
            score += 60.0
        if f.get("ret_4h", 0.0) < 0:
            score += min(40.0, abs(f["ret_4h"]))
        return max(0.0, min(100.0, score))

    # ── Logging ─────────────────────────────────────────────────

    def _log(self, payload: Dict) -> None:
        try:
            with open(LOG_PATH, "a") as f:
                f.write(json.dumps(payload) + "\n")
        except Exception as e:
            logger.warning(f"MarketState logging error: {e}")

================================================================================
FILE: mm/utils/market_state/viki_config.py
================================================================================
#>> A R I A N D E v6
#>> last update: 2025 | Sept. 4
#>>
#>> Viki Config
#>> mm/utils/market_state/viki_config.py
#>>
#>> Thresholds and toggles for the Market State Oracle.
#>>
#>> [520] [741] [8]
#>>────────────────────────────────────────────────────────────────

# Build|20250904.01

# Logging toggle and path
LOGGING: bool = True
LOG_PATH: str = "mm/utils/market_state/viki_log.json"

# Indicator lookbacks
RSI_LEN: int = 14
BOLL_LEN: int = 20
ATR_LEN: int = 14

# Returns lookback (index offsets on fetched arrays)
LOOKBACK_SHORT: int = 8    # ~ last 8 bars of given TF
LOOKBACK_LONG: int = 48    # context size for TF series

# Panic thresholds
PANIC_THRESH = {
    "ret_15m": -3.0,         # <= -3% in 15m
    "vol_z_min": 2.0,        # >= 2 std devs above mean
    "spread_pct_min": 0.25,  # >= 0.25% spread
}

# Capitulation thresholds (harsher than panic)
CAPITULATION_THRESH = {
    "ret_1h": -8.0,          # <= -8% in 1h
    "vol_z_min": 3.0,        # >= 3 std devs
    "rsi_max": 20.0,         # RSI <= 20
}

# Euphoria thresholds
EUPHORIA_THRESH = {
    "ret_1h": 6.0,           # >= +6% in 1h
    "vol_z_min": 2.0,        # >= 2 std devs
    "rsi_min": 70.0,         # RSI >= 70
}

# Stale thresholds
STALE_THRESH = {
    "ret_abs_max": 0.5,      # abs 15m return < 0.5%
    "bb_width_max": 1.0,     # band width < 1% of price
    "vol_ratio_max": 0.9,    # last vol lower than recent avg
    "spread_pct_max": 0.15,  # tight spread
}

# Wolf thresholds (rangebound but choppy)
WOLF_THRESH = {
    "bb_width_min": 2.0,      # band width >= 2% of price
    "flips_min": 3.0,         # >= 3 sign flips over short window
    "net_change_abs_max": 0.7 # <= 0.7 units net px change over short window
}

# Correction: % below recent high considered a correction
CORRECTION_DROP_PCT: float = 10.0

================================================================================
FILE: mm/utils/tqdm/agnes.py
================================================================================
#===================================================================
# 🍁 A R I A N D E           bot version 6.1 file build 20250905.01
#===================================================================
# last update: 2025 | Sept. 5                   Production ready ✅
#===================================================================
# Agnes - TQDM Logger
# mm/utils/tqdm/agnes.py
#
# A logging handler that safely outputs to a terminal.
# Ensures human-freindly readability.    
# Formats [DEBUG][INFO][WARNING][ERROR][CRITICAL] 
#
# [520] [741] [8]
#===================================================================
# 🔰 THE COMMANDER            ✖ PERSISTANT RUNTIME  ✖ MONIT MANAGED
#===================================================================

# 🔸 Standard Library Imports ======================================

import logging
import tqdm
from typing import Optional

class TqdmLogHandler(logging.Handler):
    """
    A logging handler that safely outputs to a terminal being used by tqdm progress bars.
    Ensures log messages are printed on new lines without interfering with progress bars.
    """
    def __init__(self, level: int = logging.NOTSET):
        super().__init__(level)

    def emit(self, record):
        try:
            msg = self.format(record)
            tqdm.tqdm.write(msg)
        except (KeyboardInterrupt, SystemExit):
            raise
        except:
            self.handleError(record)

def setup_logger(name: str, level: int = logging.INFO, log_format: Optional[str] = None) -> logging.Logger:
    """
    A helper function to quickly create and configure a logger that uses the TqdmLogHandler.

    Args:
        name (str): The name of the logger (e.g., __name__).
        level (int): The logging level (e.g., logging.INFO).
        log_format (str, optional): A format string for the log messages.
                                    Uses a default with timestamp if none provided.

    Returns:
        logging.Logger: A configured logger instance.
    """
    if log_format is None:
        log_format = '%(asctime)s    %(message)s'

    formatter = logging.Formatter(log_format, datefmt='%Y-%m-%d %H:%M:%S')

    handler = TqdmLogHandler()
    handler.setFormatter(formatter)

    logger = logging.getLogger(name)
    logger.setLevel(level)
    # Avoid adding multiple handlers if this function is called more than once for the same logger
    if not logger.handlers:
        logger.addHandler(handler)
    # Prevent the log messages from being propagated to the root logger and appearing twice
    logger.propagate = False

    return logger

================================================================================
FILE: mm/utils/helpers/inara.py
================================================================================
#===================================================================
# 🍁 A R I A N D E           bot version 6.1 file build 20250917.01
#===================================================================
# last update: 2025 | Sept. 17                  Production ready ✅
#===================================================================
# Inara
# mm/utils/helpers/inara.py
#
# Determines what mode to operate in - simulation or live
# Central logic disseminating her decision to the other files
#
# [520] [741] [8]
#===================================================================
# 🜁 THE COMMANDER            ✖ PERSISTANT RUNTIME  ✖ MONIT MANAGED
#===================================================================

# 🔸 Standard Library Imports ======================================

import logging
import os
import json
import time
import importlib
import smtplib
import ssl
import uuid
from email.message import EmailMessage
from email.utils import formataddr
from datetime import datetime
from zoneinfo import ZoneInfo
from typing import Literal, Optional
from functools import lru_cache
from dotenv import load_dotenv

# 🔸 local application imports =====================================

import mm.config.marcus as marcus

# 🔸 load env for this process =====================================
load_dotenv("mm/data/secrets/.env")

logger = logging.getLogger("ariadne")

USERCODE = "INA"  # this file's identity

# 🔸 Allowed operational modes =====================================

ALLOWED_MODES: set[str] = {
    "simulation",
    "live",
    "halted",
    "drain",
    "maintenance",
    "shadow",
}

# 🔸 Current runtime mode (mutable) ================================
_mode: str = "simulation"

STATE_DIR = os.path.join(os.path.dirname(__file__), "../../data/state")
MODE_FILE = os.path.join(STATE_DIR, "mode.json")


def send_email(subject: str, status: str, title: str, message: str) -> str:
    importlib.reload(marcus)
    if not bool(getattr(marcus, "ALERT_EMAIL_ENABLED", False)):
        return "disabled"
    if str(getattr(marcus, "ALERT_EMAIL_ENCRYPT", "SSL")).upper() != "SSL":
        return "Simple Mail Transfer Protocol not established. No conn."

    host = getattr(marcus, "ALERT_EMAIL_SMTP_SERVER", None)
    port = getattr(marcus, "ALERT_EMAIL_SMTP_PORT", None)
    recipient = getattr(marcus, "ALERT_EMAIL_RECIPIENT", None)

    user = os.getenv(f"{USERCODE}_USR")
    pwd = os.getenv(f"{USERCODE}_PWD")
    sender_email = user
    sender_name = os.getenv(f"{USERCODE}_NAME")

    STATUS_COLORS = {
        "STATCON3": "#F1C232",
        "STATCON2": "#E69138",
        "STATCON1": "#CC0000",
        "SIGCON1":  "#FB6D8B",
        "OPSCON5":  "#F5F5F5",
        "OPSCON1":  "#990000",
    }
    status_text = str(status).upper()
    status_color = STATUS_COLORS.get(status_text, "#BE644C")

    msg = EmailMessage()
    domain = sender_email.split("@")[1] if "@" in sender_email else "hodlcorp.io"
    msg_id = f"<{uuid.uuid4()}@{domain}>"
    msg["Message-ID"] = msg_id
    msg["From"] = formataddr((sender_name, sender_email))
    msg["To"] = recipient
    msg["Subject"] = subject
    msg["X-Priority"] = "1"
    msg["X-MSMail-Priority"] = "High"
    msg["Importance"] = "High"

    now_tz = datetime.now(ZoneInfo("America/Toronto"))
    sent_str = now_tz.strftime("%Y-%m-%d %H:%M:%S America/Toronto")
    epoch_ms = int(now_tz.timestamp() * 1000)
    mid_clean = msg_id.strip("<>").split("@", 1)[0]

    html_body = f"""
<div style="font-family: monospace;">
  <table role="presentation" width="100%" height="20px" cellpadding="8px" cellspacing="0" border="0">
    <tr style="font-family: Georgia, 'Times New Roman', Times, serif;font-size:20px;font-weight:600;background-color:#333;">
      <td align="left" style="color:#EFEFEF;letter-spacing:12px;">INTCOMM</td>
      <td align="right" style="color:{status_color};letter-spacing:4px;">{status_text}</td>
    </tr>
    <tr width="100%" cellpadding="6px" style="font-family: Tahoma, Geneva, sans-serif;text-align:left;font-size:14px;font-weight:600;color:#333;">
      <td colspan="2">{title}</td>
    </tr>
    <tr width="100%" cellpadding="6px" style="font-family: Tahoma, Geneva, sans-serif;text-align:left;font-size:11px;font-weight:400;line-height:1.5;color:#333;">
      <td colspan="2">{message}</td>
    </tr>
    <tr width="100%" height="25px"><td colspan="2">&nbsp;</td></tr>
  </table>

  <table role="presentation" width="400px" height="20px" cellpadding="4" cellspacing="0" border="0" style="font-family: Tahoma, Geneva, sans-serif;">
    <tr style="background-color:#333;">
      <td colspan="2" style="color:#efefef;font-size:12px;font-weight:600;">DOCINT</td>
    </tr>
    <tr style="background-color:#E9E9E5;">
      <td width="30px" style="color:#333;font-size:10px;font-weight:600;">SENT</td>
      <td width="10px">→</td>
      <td style="color:#333;font-size:11px;">{sent_str}</td>
    </tr>
    <tr style="background-color:#F2F2F0;">
      <td width="30px" style="color:#333;font-size:10px;font-weight:600;">EPOCH</td>
      <td width="10px">→</td>
      <td style="color:#333;font-size:11px;">{epoch_ms} (ms since 1970/01/01 0:00 UTC)</td>
    </tr>
    <tr style="background-color:#E9E9E5;">
      <td width="30px" style="color:#333;font-size:10px;font-weight:600;">m.ID</td>
      <td width="10px">→</td>
      <td style="color:#333;font-size:11px;">{mid_clean}</td>
    </tr>
  </table>
</div>
"""
    msg.add_alternative(html_body, subtype="html")
    ctx = ssl.create_default_context()
    with smtplib.SMTP_SSL(host, port, context=ctx, timeout=10) as s:
        if user and pwd:
            s.login(user, pwd)
        s.send_message(msg)
    return msg_id


def _write_prev_mode(mode: str):
    try:
        os.makedirs(STATE_DIR, exist_ok=True)
        if mode not in ("simulation", "live"):
            return
        tmpfile = MODE_FILE + ".tmp"
        with open(tmpfile, "w") as f:
            json.dump({"prev_mode": mode}, f)
        os.replace(tmpfile, MODE_FILE)
        logger.debug(f"prev_mode updated to {mode}")
    except Exception as e:
        logger.error(f"Failed to write prev_mode file: {e}")


def _read_prev_mode() -> str:
    try:
        with open(MODE_FILE, "r") as f:
            data = json.load(f)
            prev = data.get("prev_mode")
            if prev in ("simulation", "live"):
                return prev
    except FileNotFoundError:
        logger.warning("mode.json not found, defaulting prev_mode=simulation")
    except Exception as e:
        logger.error(f"Failed to read prev_mode file: {e}")
    return "simulation"


@lru_cache(maxsize=1)
def get_mode() -> str:
    global _mode
    if _mode:
        return _mode
    try:
        from mm.config.marcus import MODE
        m = str(MODE).lower()
        if m in ALLOWED_MODES:
            _mode = m
            if _mode in ("simulation", "live"):
                _write_prev_mode(_mode)
            return _mode
    except Exception:
        logger.exception("Error importing marcus.MODE")

    _mode = "halted"
    try:
        send_email(
            subject="[Ariadne] Mode defaulted to HALTED",
            status="OPSCON1",
            title="Mode Fallback Triggered",
            message=("Inara could not determine a valid MODE. "
                     "System has defaulted to HALTED. "
                     "Please investigate marcus.py and restart."),
        )
    except Exception as e:
        logger.error("Failed to send halt alert: %s", e)

    return _mode


def require_mode(required_mode: str) -> None:
    current = get_mode()
    if current != required_mode:
        logger.error(f"[ABORT] This process requires {required_mode} mode but system is in {current} mode")
        raise RuntimeError(f"Mode mismatch: requires {required_mode}, got {current}")


def override_mode(new_mode: str, origin: Optional[str] = None, reason: Optional[str] = None) -> None:
    global _mode
    nm = str(new_mode).lower()
    if nm not in ALLOWED_MODES:
        logger.error(f"Invalid override mode: {new_mode}")
        return

    get_mode.cache_clear()
    _mode = nm
    get_mode()

    try:
        send_email(
            subject=f"[Ariadne] Mode overridden to {new_mode.upper()}",
            status="STATCON2",
            title="Manual Mode Override",
            message=f"Mode changed by {origin or 'unknown'} for reason: {reason or 'unspecified'}",
        )
    except Exception as e:
        logger.error("Failed to send override alert: %s", e)


def get_trading_client():
    counter = 0
    last_exc = None

    while counter < 3:
        counter += 1
        try:
            mode = get_mode()
            if mode == "simulation":
                from mm.conn.sim_kucoin import SimClient as TradingClient
                client = TradingClient()
                logger.info("Trading client initialized (simulation)")
                return client
            elif mode == "live":
                from mm.conn.conn_kucoin import KucoinClient as TradingClient
                client = TradingClient()
                logger.info("Trading client initialized (live)")
                return client
            elif mode == "halted":
                prev_mode = _read_prev_mode()
                if prev_mode == "simulation":
                    from mm.conn.sim_kucoin import SimClient as TradingClient
                    client = TradingClient()
                    logger.info("Trading client initialized in halted mode (prev=simulation)")
                    return client
                elif prev_mode == "live":
                    from mm.conn.conn_kucoin import KucoinClient as TradingClient
                    client = TradingClient()
                    logger.info("Trading client initialized in halted mode (prev=live)")
                    return client
                else:
                    from mm.conn.sim_kucoin import SimClient as TradingClient
                    client = TradingClient()
                    logger.warning("prev_mode could not be determined, defaulting to simulation")
                    send_email("inara.get_trading_client", "STATCON3", "Fallback to Simulation", "prev_mode could not be determined, defaulting to simulation")
                    return client
            else:
                logger.warning(f"Mode {mode} does not support client initialization")
                return None

        except Exception as e:
            last_exc = e
            logger.error(f"Client init failed (attempt {counter}/3): {e}")
            time.sleep(2)

    send_email("inara.get_trading_client", "STATCON1", "Trading Client Failure", f"Trading client failed after retries: {last_exc}")
    logger.critical("Trading client failed after retries, forcing halted mode")
    override_mode("halted", origin="inara.get_trading_client", reason="client init failure")

    try:
        from mm.conn.sim_kucoin import SimClient as TradingClient
        client = TradingClient()
        logger.warning("Returning simulation client as safe fallback")
        return client
    except Exception as e:
        logger.error(f"Failed to init simulation client fallback: {e}")
        return None

================================================================================
FILE: mm/utils/helpers/wintermute.py
================================================================================
#>> A R I A N D E [v 6.1]
#>> last update: 2025 | Sept. 03
#>>
#>> wintermute
#>> mm/utils/helpers/wintermute.py
#>>
#>> Central hub for all common helpers, methods, and facilitators
#>> The one source of truth for shared functionality
#>>
#>> Auth'd -> Commander
#>>
#>> [520] [741] [8]
#>>────────────────────────────────────────────────────────────────

# Build|20250903.01

from __future__ import annotations

import os
import ssl
import json
import signal
import smtplib
import logging
import hashlib
import uuid
import random
from dataclasses import dataclass
from decimal import Decimal, getcontext, ROUND_DOWN, ROUND_UP, InvalidOperation
from email.message import EmailMessage
from logging.handlers import RotatingFileHandler
from typing import Any, Callable, Dict, Iterable, Optional, Tuple, Protocol, List
from datetime import datetime, timedelta
from zoneinfo import ZoneInfo
from pathlib import Path
from dotenv import load_dotenv

import psycopg2
import psycopg2.extras
import psycopg2.pool

# Explicitly load secrets file
dotenv_path = os.path.join(os.path.dirname(__file__), "../../data/secrets/.env")
load_dotenv(dotenv_path=dotenv_path)

# ──────────────────────────────────────────────────────────────────────────
# Global settings
# ──────────────────────────────────────────────────────────────────────────
DEFAULT_TZ = "America/Toronto"  # DST-aware local default
getcontext().prec = 28          # safe precision for crypto P&L/fees

# Database connection pool (lazy-initialized)
_db_pool = None

# 🔶 Logging =======================================================

import logging
import tqdm

class TqdmLogHandler(logging.Handler):
    def emit(self, record):
        try:
            msg = self.format(record)
            tqdm.tqdm.write(msg)
        except (KeyboardInterrupt, SystemExit):
            raise
        except Exception:
            self.handleError(record)

def init_logging(LOG_SELF=True, LOG_MAIN=True, SCREEN_OUT=True, LOGGER="Julius"):
    fmt = '%(asctime)s    [ %s ] [%%(levelname)s] %%(message)s' % LOGGER
    formatter = logging.Formatter(fmt, datefmt='%Y-%m-%d %H:%M:%S')

    logger = logging.getLogger(LOGGER)
    logger.setLevel(logging.DEBUG)

    # 🔹 Output to the page's log ==================================
    
    if LOG_SELF and not any(isinstance(h, logging.FileHandler) and h.baseFilename.endswith(f"{LOGGER.lower()}.log") for h in logger.handlers):
        fh = logging.FileHandler(f"mm/logs/{LOGGER.lower()}.log")
        fh.setFormatter(formatter)
        fh.setLevel(logging.DEBUG)
        logger.addHandler(fh)

    # 🔹 Output to the main log (ariadne.log) ======================
    
    if LOG_MAIN:
        ariadne_logger = logging.getLogger("ariadne")
        ariadne_logger.setLevel(logging.DEBUG)
        if not any(isinstance(h, logging.FileHandler) and h.baseFilename.endswith("ariadne.log") for h in ariadne_logger.handlers):
            fh2 = logging.FileHandler("mm/logs/ariadne.log")
            fh2.setFormatter(formatter)
            fh2.setLevel(logging.DEBUG)
            ariadne_logger.addHandler(fh2)
    else:
        ariadne_logger = None

    # 🔹 Output to the screen ======================================
    
    if SCREEN_OUT and not any(isinstance(h, TqdmLogHandler) for h in logger.handlers):
        sh = TqdmLogHandler()
        sh.setFormatter(formatter)
        sh.setLevel(logging.DEBUG)
        logger.addHandler(sh)

    logger.propagate = False
    if LOG_MAIN and ariadne_logger:
        ariadne_logger.propagate = False

    return logger

    #🛑

# 🔶 Email Sender ==================================================

import os
import importlib
import smtplib
import ssl
import uuid
from datetime import datetime
from email.message import EmailMessage
from email.utils import formataddr
from zoneinfo import ZoneInfo
from dotenv import load_dotenv

import mm.config.marcus as marcus

# Load .env (only once per process, safe here)
load_dotenv("mm/data/secrets/.env")

def send_email(subject: str, status: str, title: str, message: str, USERCODE: str = "ARI") -> str:
    """
    Send a styled HTML alert email via SMTP, using config/marcus and env for secrets.

    Params:
        subject:   Email subject
        status:    Status code/tag (for coloring: STATCON1, STATCON2, etc.)
        title:     Big bold message in the email
        message:   Main body text
        USERCODE:  3-letter user/app code (for env vars: ARI_USR, ARI_PWD, ARI_NAME)
    Returns:
        msg_id: Message-ID string (on success), or string error/"disabled" as fallback
    """

    importlib.reload(marcus)
    if not bool(getattr(marcus, "ALERT_EMAIL_ENABLED", False)):
        return "disabled"
    if str(getattr(marcus, "ALERT_EMAIL_ENCRYPT", "SSL")).upper() != "SSL":
        return "Simple Mail Transfer Protocol not established. No conn."

    host = getattr(marcus, "ALERT_EMAIL_SMTP_SERVER", None)
    port = getattr(marcus, "ALERT_EMAIL_SMTP_PORT", None)
    recipient = getattr(marcus, "ALERT_EMAIL_RECIPIENT", None)

    # Sender Info from env
    user = os.getenv(f"{USERCODE}_USR")
    pwd = os.getenv(f"{USERCODE}_PWD")
    sender_email = user
    sender_name = os.getenv(f"{USERCODE}_NAME", USERCODE)

    STATUS_COLORS = {
        "STATCON3": "#F1C232",
        "STATCON2": "#E69138",
        "STATCON1": "#CC0000",
        "SIGCON1":  "#FB6D8B",
        "OPSCON5":  "#F5F5F5",
        "OPSCON1":  "#990000",
    }
    status_text = str(status).upper()
    status_color = STATUS_COLORS.get(status_text, "#BE644C")

    msg = EmailMessage()
    domain = sender_email.split("@")[1] if "@" in sender_email else "hodlcorp.io"
    msg_id = f"<{uuid.uuid4()}@{domain}>"
    msg["Message-ID"] = msg_id
    msg["From"] = formataddr((sender_name, sender_email))
    msg["To"] = recipient
    msg["Subject"] = subject
    msg["X-Priority"] = "1"
    msg["X-MSMail-Priority"] = "High"
    msg["Importance"] = "High"

    now_tz = datetime.now(ZoneInfo("America/Toronto"))
    sent_str = now_tz.strftime("%Y-%m-%d %H:%M:%S America/Toronto")
    epoch_ms = int(now_tz.timestamp() * 1000)
    mid_clean = msg_id.strip("<>").split("@", 1)[0]

    html_body = f"""
<div style="font-family: monospace;">
  <table role="presentation" width="100%" height="20px" cellpadding="8px" cellspacing="0" border="0">
    <tbody><tr style="font-family: Georgia, 'Times New Roman', Times, serif;font-size:20px;font-weight:600;background-color:#333;">
      <td align="left" style="color:#EFEFEF;letter-spacing:12px;">INTCOMM</td>
      <td align="right" style="color:{status_color};letter-spacing:4px;">{status_text}</td>
    </tr>
    <tr width="100%" cellpadding="6px" style="font-family: Tahoma, Geneva, sans-serif;text-align:left;font-size:14px;font-weight:600;color:#333;">
      <td colspan="2">{title}</td>
    </tr>
    <tr width="100%" cellpadding="6px" style="font-family: Tahoma, Geneva, sans-serif;text-align:left;font-size:11px;font-weight:400;line-height:1.5;color:#333;">
      <td colspan="2">{message}</td>
    </tr>
    <tr width="100%" height="25px"><td colspan="2">&nbsp;</td></tr>
  </tbody></table>
  <table role="presentation" width="400px" height="20px" cellpadding="4" cellspacing="0" border="0" style="font-family: Tahoma, Geneva, sans-serif;">
    <tbody><tr style="background-color:#333;">
      <td colspan="2" style="color:#efefef;font-size:12px;font-weight:600;">DOCINT</td>
    </tr>
    <tr style="background-color:#E9E9E5;">
      <td width="30px" style="color:#333;font-size:10px;font-weight:600;">SENT</td>
      <td width="10px" style="color:#333;font-size:10px;font-weight:600;">→</td>
      <td style="color:#333;font-size:11px;font-weight:400;">{sent_str}</td>
    </tr>
    <tr style="background-color:#F2F2F0;">
      <td width="30px" style="color:#333;font-size:10px;font-weight:600;">EPOCH</td>
      <td width="10px" style="color:#333;font-size:10px;font-weight:600;">→</td>
      <td style="color:#333;font-size:11px;font-weight:400;">{epoch_ms} (ms since 1970/01/01 0:00 UTC)</td>
    </tr>
    <tr style="background-color:#E9E9E5;">
      <td width="30px" style="color:#333;font-size:10px;font-weight:600;">m.ID</td>
      <td width="10px" style="color:#333;font-size:10px;font-weight:600;">→</td>
      <td style="color:#333;font-size:11px;font-weight:400;">{mid_clean}</td>
    </tr>
  </tbody></table>
</div>
"""

    msg.add_alternative(html_body, subtype="html")
    ctx = ssl.create_default_context()
    try:
        with smtplib.SMTP_SSL(host, port, context=ctx, timeout=10) as s:
            if user and pwd:
                s.login(user, pwd)
            s.send_message(msg)
        return msg_id
    except Exception as e:
        return f"email_failed: {e}"

    #🛑

# ══════════════════════════════════════════════════════════════════════════
#  SECTION 1: TIME & TIMEZONE
# ══════════════════════════════════════════════════════════════════════════

def _tz(tz: Optional[str]) -> ZoneInfo:
    return ZoneInfo(tz or DEFAULT_TZ)

def _offset_colon(dt: datetime) -> str:
    # Convert -0400 -> -04:00 for DB-friendly formatting
    z = dt.strftime("%z")
    return f"{z[:3]}:{z[3:]}" if z and len(z) == 5 else z

@dataclass(frozen=True)
class TimePack:
    """
    Multi-format timestamp bundle at the *local* timezone (default America/Toronto).

    Fields:
      dt       : timezone-aware datetime (local tz)
      epoch_ms : integer milliseconds since Unix epoch
      iso      : ISO-8601 with local offset (e.g., 2025-09-02T01:23:45-04:00)
      human    : "YYYY-MM-DD HH:MM:SS America/Toronto"
      db       : "YYYY-MM-DD HH:MM:SS-04:00"
    """
    dt: datetime
    epoch_ms: int
    iso: str
    human: str
    db: str

def now_local(tz: Optional[str] = None) -> datetime:
    """Current time as timezone-aware datetime in the given tz (default: America/Toronto)."""
    return datetime.now(_tz(tz))

def to_pack(dt: Optional[datetime] = None, tz: Optional[str] = None) -> TimePack:
    """Build a TimePack from a datetime at local tz."""
    Z = _tz(tz)
    if dt is None:
        dt = datetime.now(Z)
    else:
        dt = dt if dt.tzinfo is not None else dt.replace(tzinfo=Z)
        dt = dt.astimezone(Z)
    epoch_ms = int(dt.timestamp() * 1000)
    iso = dt.isoformat()
    human = f"{dt.strftime('%Y-%m-%d %H:%M:%S')} {Z.key}"
    db = f"{dt.strftime('%Y-%m-%d %H:%M:%S')}{_offset_colon(dt)}"
    return TimePack(dt=dt, epoch_ms=epoch_ms, iso=iso, human=human, db=db)

def now_pack(tz: Optional[str] = None) -> TimePack:
    """Convenience wrapper returning a local TimePack (default America/Toronto)."""
    return to_pack(None, tz)

def parse_epoch_ms(epoch_ms: int, tz: Optional[str] = None) -> TimePack:
    """Convert epoch ms to a local TimePack (default America/Toronto)."""
    Z = _tz(tz)
    dt = datetime.fromtimestamp(epoch_ms / 1000, tz=Z)
    return to_pack(dt, tz)

def get_utc_timestamp() -> int:
    """Consistent UTC timestamp generation (seconds since epoch)."""
    return int(datetime.utcnow().timestamp())

def get_email_date() -> str:
    """RFC 2822 formatted date for email headers."""
    from email.utils import formatdate
    return formatdate(localtime=True)

def is_market_hours(symbol: str = None) -> bool:
    """Check if within trading hours (crypto trades 24/7, this is for future use)."""
    # For crypto, always True. For stocks, implement market hours logic
    return True

# ══════════════════════════════════════════════════════════════════════════
#  SECTION 2: DECIMAL MATH & FORMATTING
# ══════════════════════════════════════════════════════════════════════════

def _coerce_str_no_sci(v: Any, dp: int) -> str:
    """Turn inputs into a fixed-point string with EXACT dp decimals and NO scientific notation."""
    if isinstance(v, Decimal):
        s = format(v, f"f")
    elif isinstance(v, int):
        s = f"{v:.{dp}f}"
    elif isinstance(v, float):
        s = f"{v:.{dp}f}"
    elif isinstance(v, str):
        if "e" in v.lower():
            raise ValueError(f"Scientific notation not allowed: {v!r}")
        try:
            d = Decimal(v)
        except InvalidOperation as e:
            raise ValueError(f"Invalid decimal string: {v!r}") from e
        s = format(d, f"f")
    else:
        s = format(Decimal(str(v)), f"f")
    d = Decimal(s)
    q = Decimal("1." + "0" * dp)
    return format(d.quantize(q, rounding=ROUND_DOWN), f"f")

def dec2(v: Any) -> Decimal:
    """Return Decimal with exactly 2 dp; no sci-notation accepted."""
    return Decimal(_coerce_str_no_sci(v, 2))

def dec8(v: Any) -> Decimal:
    """Return Decimal with exactly 8 dp; no sci-notation accepted."""
    return Decimal(_coerce_str_no_sci(v, 8))

def fmt2(d: Any) -> str:
    """String with exactly 2 dp, non-scientific."""
    return _coerce_str_no_sci(d, 2)

def fmt8(d: Any) -> str:
    """String with exactly 8 dp, non-scientific."""
    return _coerce_str_no_sci(d, 8)

def quantize_step(d: Decimal, step: Decimal, mode: str = "down") -> Decimal:
    """Round to tradable increment (tick/lot)."""
    d = Decimal(str(d)); step = Decimal(str(step))
    q = (ROUND_DOWN if mode == "down" else ROUND_UP)
    return (d / step).to_integral_value(rounding=q) * step

# ══════════════════════════════════════════════════════════════════════════
#  SECTION 3: SYMBOL OPERATIONS
# ══════════════════════════════════════════════════════════════════════════

def parse_symbol(sym: str) -> Tuple[str, str]:
    """
    Parse symbol into (BASE, QUOTE) tuple.
    Accepts: 'BTC-USDT', 'BTC/USDT', 'btcusdt', 'BTC_USDT'
    This is the primary symbol parser - norm_symbol is an alias.
    """
    s = sym.strip().upper().replace("/", "-").replace("_", "-")
    if "-" in s:
        base, quote = s.split("-", 1)
        return base, quote
    # Try common quote currencies
    for q in ("USDT", "USDC", "BTC", "ETH", "USD", "EUR", "CAD"):
        if s.endswith(q) and len(s) > len(q):
            return s[:-len(q)], q
    raise ValueError(f"Unrecognized symbol format: {sym!r}")

# Alias for compatibility
norm_symbol = parse_symbol

def format_pair(base: str, quote: str) -> str:
    """Format base/quote into standard symbol format (BASE-QUOTE)."""
    return f"{base.upper()}-{quote.upper()}"

# Alias for compatibility
join_symbol = format_pair

def validate_symbol(symbol: str, valid_symbols: Optional[List[str]] = None) -> bool:
    """
    Validate symbol format and optionally check against list of valid symbols.
    """
    try:
        base, quote = parse_symbol(symbol)
        if not base or not quote:
            return False
        if valid_symbols:
            formatted = format_pair(base, quote)
            return formatted in valid_symbols
        return True
    except ValueError:
        return False

def get_symbol_info(conn, symbol: str) -> Dict[str, Any]:
    """
    Get symbol trading rules from symbol_info table.
    Returns dict with base/quote increments, min sizes, etc.
    """
    try:
        cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        cur.execute("""
            SELECT * FROM symbol_info WHERE symbol = %s
        """, (symbol,))
        row = cur.fetchone()
        cur.close()
        if row:
            return dict(row)
        else:
            # Return defaults if not found
            return {
                'symbol': symbol,
                'base_increment': 0.00000001,
                'price_increment': 0.00000001, 
                'base_min_size': 0.00000001,
                'quote_min_size': 0.00001
            }
    except Exception as e:
        log.error(f"Error getting symbol info for {symbol}: {e}")
        return {}

def safe_filename(s: str) -> str:
    """Convert string to safe filename."""
    return "".join(c if c.isalnum() or c in ("-", "_", ".") else "_" for c in s)

def sha256_hex(s: str) -> str:
    """SHA256 hash of string."""
    return hashlib.sha256(s.encode("utf-8")).hexdigest()

# ══════════════════════════════════════════════════════════════════════════
#  SECTION 4: PRICE CALCULATIONS
# ══════════════════════════════════════════════════════════════════════════

def mid_price(bid: Decimal, ask: Decimal) -> Decimal:
    """Mid price = (bid + ask) / 2"""
    bid = Decimal(str(bid)); ask = Decimal(str(ask))
    return (bid + ask) / Decimal("2")

# Alias for compatibility
calculate_mid_price = mid_price

def microprice(bid: Decimal, ask: Decimal, bid_sz: Decimal, ask_sz: Decimal) -> Decimal:
    """Queue-depth weighted price leaning toward the thinner side."""
    bid = Decimal(str(bid)); ask = Decimal(str(ask))
    bid_sz = Decimal(str(bid_sz)); ask_sz = Decimal(str(ask_sz))
    denom = bid_sz + ask_sz
    return mid_price(bid, ask) if denom == 0 else (ask * bid_sz + bid * ask_sz) / denom

def calculate_spread(bid: float, ask: float) -> float:
    """
    Calculate bid-ask spread as percentage.
    Returns: (ask - bid) / mid * 100
    """
    if bid <= 0 or ask <= 0:
        return 0.0
    mid = (bid + ask) / 2
    return ((ask - bid) / mid) * 100 if mid > 0 else 0.0

def round_to_tick(price: float, tick_size: float) -> float:
    """Round price to valid tick increment."""
    if tick_size <= 0:
        return price
    return round(price / tick_size) * tick_size

def round_size(size: float, increment: float) -> float:
    """Round order size to valid increment."""
    if increment <= 0:
        return size
    return round(size / increment) * increment

def notional(qty: Decimal, price: Decimal) -> Decimal:
    """Notional = qty * price (quote-currency value)."""
    return Decimal(str(qty)) * Decimal(str(price))

# ══════════════════════════════════════════════════════════════════════════
#  SECTION 5: FEE CALCULATIONS
# ══════════════════════════════════════════════════════════════════════════

def apply_fee(amount: Decimal, rate: Decimal, coefficient: Decimal = Decimal("1")) -> Tuple[Decimal, Decimal]:
    """
    Apply fee with coefficient. Returns (fee, net_amount).
    """
    amount = Decimal(str(amount))
    eff = Decimal(str(rate)) * Decimal(str(coefficient))
    fee = (amount * eff).quantize(Decimal("0.00000001"), rounding=ROUND_DOWN)
    net = (amount - fee).quantize(Decimal("0.00000001"), rounding=ROUND_DOWN)
    return fee, net

def calculate_fees(symbol: str, size: float, price: float, side: str, 
                  maker_rate: float = 0.001, taker_rate: float = 0.001) -> float:
    """
    Calculate maker/taker fees for order.
    Returns fee amount in quote currency.
    """
    notional_value = size * price
    # Assume taker unless specified otherwise
    fee_rate = maker_rate if side == 'maker' else taker_rate
    return notional_value * fee_rate

def get_fee_rate(symbol: str, side: str, conn=None) -> float:
    """
    Get fee rate for symbol/side from database or default.
    """
    if conn:
        try:
            cur = conn.cursor()
            cur.execute("""
                SELECT maker_fee_rate, taker_fee_rate 
                FROM symbol_info 
                WHERE symbol = %s
            """, (symbol,))
            row = cur.fetchone()
            cur.close()
            if row:
                return float(row[0] if side == 'maker' else row[1])
        except:
            pass
    # Default rates
    return 0.001  # 0.1%

def breakeven(entry_price: Decimal, fee_rate: Decimal, coefficient: Decimal = Decimal("1")) -> Decimal:
    """Approx round-trip breakeven with fees on buy and sell."""
    entry = Decimal(str(entry_price))
    eff = Decimal(str(fee_rate)) * Decimal(str(coefficient))
    return entry * (Decimal("1") + (Decimal("2") * eff))

# ══════════════════════════════════════════════════════════════════════════
#  SECTION 6: ORDER VALIDATION
# ══════════════════════════════════════════════════════════════════════════

def validate_min_order_size(symbol: str, size: float, conn=None) -> bool:
    """Check if order size meets minimum requirements."""
    if conn:
        info = get_symbol_info(conn, symbol)
        min_size = float(info.get('base_min_size', 0.00000001))
        return size >= min_size
    # Without connection, assume valid
    return size > 0

def validate_order_value(symbol: str, size: float, price: float, conn=None) -> bool:
    """Check if order meets minimum USDT value requirement."""
    notional_value = size * price
    if conn:
        info = get_symbol_info(conn, symbol)
        min_value = float(info.get('quote_min_size', 0.00001))
        return notional_value >= min_value
    # Default minimum
    return notional_value >= 0.00001

def count_open_orders(symbol: str, conn) -> int:
    """Count active orders for a symbol."""
    try:
        cur = conn.cursor()
        cur.execute("""
            SELECT COUNT(*) FROM sim_orders 
            WHERE symbol = %s 
            AND status IN ('open', 'partial')
            AND COALESCE(deleted, FALSE) = FALSE
        """, (symbol,))
        count = cur.fetchone()[0]
        cur.close()
        return count
    except Exception as e:
        log.error(f"Error counting open orders for {symbol}: {e}")
        return 0

# ══════════════════════════════════════════════════════════════════════════
#  SECTION 7: BALANCE OPERATIONS
# ══════════════════════════════════════════════════════════════════════════

def get_available_balance(currency: str, conn) -> float:
    """Get tradeable balance for currency."""
    try:
        cur = conn.cursor()
        cur.execute("""
            SELECT available FROM sim_balances WHERE asset = %s
        """, (currency.upper(),))
        row = cur.fetchone()
        cur.close()
        return float(row[0]) if row else 0.0
    except Exception as e:
        log.error(f"Error getting balance for {currency}: {e}")
        return 0.0

def calculate_total_equity(conn, base_currency: str = 'USDT') -> float:
    """
    Sum portfolio value in base currency.
    Uses last prices from tickstick table.
    """
    try:
        cur = conn.cursor()
        cur.execute("""
            SELECT asset, available + hold as total FROM sim_balances
        """)
        balances = cur.fetchall()
        
        total_equity = 0.0
        for asset, balance in balances:
            if balance <= 0:
                continue
            if asset == base_currency:
                total_equity += balance
            else:
                # Get conversion rate
                symbol = f"{asset}-{base_currency}"
                cur.execute("""
                    SELECT last FROM tickstick 
                    WHERE symbol = %s 
                    ORDER BY timestamp DESC LIMIT 1
                """, (symbol,))
                price_row = cur.fetchone()
                if price_row:
                    total_equity += balance * float(price_row[0])
        
        cur.close()
        return total_equity
    except Exception as e:
        log.error(f"Error calculating total equity: {e}")
        return 0.0

def can_afford(currency: str, amount: float, conn) -> bool:
    """Check if sufficient balance for purchase."""
    available = get_available_balance(currency, conn)
    return available >= amount

# ══════════════════════════════════════════════════════════════════════════
#  SECTION 8: DATABASE OPERATIONS
# ══════════════════════════════════════════════════════════════════════════

def get_db_connection() -> psycopg2.extensions.connection:
    """
    Get PostgreSQL connection to ariadne database.
    Uses connection pooling for efficiency.
    """
    global _db_pool
    if _db_pool is None:
        _db_pool = psycopg2.pool.SimpleConnectionPool(
            1, 20,  # min 1, max 20 connections
            dbname="ariadne",
            user="postgres",
            host="localhost"
        )
    return _db_pool.getconn()

def release_db_connection(conn):
    """Return connection to pool."""
    global _db_pool
    if _db_pool:
        _db_pool.putconn(conn)

def execute_query(query: str, params: tuple = None, conn=None) -> list:
    """
    Safe query execution with error handling.
    Returns list of dict rows.
    """
    own_conn = False
    try:
        if conn is None:
            conn = get_db_connection()
            own_conn = True
        
        cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        cur.execute(query, params)
        
        # If SELECT query, fetch results
        if query.strip().upper().startswith('SELECT'):
            results = [dict(row) for row in cur.fetchall()]
        else:
            conn.commit()
            results = []
        
        cur.close()
        return results
    except Exception as e:
        log.error(f"Query execution failed: {e}")
        if conn:
            conn.rollback()
        raise
    finally:
        if own_conn and conn:
            release_db_connection(conn)

def bulk_insert(table: str, data: list, columns: list = None, conn=None) -> bool:
    """
    Batch insert with rollback protection.
    data: list of tuples/lists to insert
    columns: column names (if not provided, assumes all columns)
    """
    own_conn = False
    try:
        if conn is None:
            conn = get_db_connection()
            own_conn = True
        
        cur = conn.cursor()
        
        if columns:
            placeholders = ','.join(['%s'] * len(columns))
            cols = ','.join(columns)
            query = f"INSERT INTO {table} ({cols}) VALUES ({placeholders})"
        else:
            # Assumes data matches all columns in order
            placeholders = ','.join(['%s'] * len(data[0]))
            query = f"INSERT INTO {table} VALUES ({placeholders})"
        
        cur.executemany(query, data)
        conn.commit()
        cur.close()
        return True
    except Exception as e:
        log.error(f"Bulk insert failed: {e}")
        if conn:
            conn.rollback()
        return False
    finally:
        if own_conn and conn:
            release_db_connection(conn)

# ══════════════════════════════════════════════════════════════════════════
#  SECTION 9: RISK CALCULATIONS
# ══════════════════════════════════════════════════════════════════════════

def calculate_position_size(capital: float, risk_pct: float, 
                           stop_loss_pct: float = None) -> float:
    """
    Kelly/fixed fractional position sizing.
    capital: total available capital
    risk_pct: percentage of capital to risk (e.g., 0.02 for 2%)
    stop_loss_pct: stop loss percentage (optional, for position sizing)
    """
    risk_amount = capital * risk_pct
    
    if stop_loss_pct and stop_loss_pct > 0:
        # Position size = risk_amount / stop_loss_pct
        return risk_amount / stop_loss_pct
    else:
        # Simple fixed fractional
        return risk_amount

def check_exposure_limits(symbol: str, size: float, conn, 
                         max_position_pct: float = 0.1) -> bool:
    """
    Validate against max exposure limits.
    max_position_pct: max percentage of portfolio in single position
    """
    total_equity = calculate_total_equity(conn)
    if total_equity <= 0:
        return False
    
    # Get current position if any
    base, quote = parse_symbol(symbol)
    current_balance = get_available_balance(base, conn)
    
    # Get price for position value calculation
    try:
        cur = conn.cursor()
        cur.execute("""
            SELECT last FROM tickstick 
            WHERE symbol = %s 
            ORDER BY timestamp DESC LIMIT 1
        """, (symbol,))
        row = cur.fetchone()
        cur.close()
        
        if row:
            price = float(row[0])
            new_position_value = (current_balance + size) * price
            position_pct = new_position_value / total_equity
            return position_pct <= max_position_pct
    except:
        pass
    
    return True

def calculate_drawdown(peak: float, current: float) -> float:
    """
    Compute current drawdown percentage.
    Returns positive percentage (e.g., 10.5 for 10.5% drawdown)
    """
    if peak <= 0:
        return 0.0
    drawdown = ((peak - current) / peak) * 100
    return max(0.0, drawdown)

# ══════════════════════════════════════════════════════════════════════════
#  SECTION 10: MARKET DATA & ANALYSIS
# ══════════════════════════════════════════════════════════════════════════

def get_latest_ticker(symbol: str, conn) -> dict:
    """
    Fetch most recent ticker data from tickstick table.
    Returns dict with bid, ask, last, volume, etc.
    """
    try:
        cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        cur.execute("""
            SELECT * FROM tickstick 
            WHERE symbol = %s 
            ORDER BY timestamp DESC 
            LIMIT 1
        """, (symbol,))
        row = cur.fetchone()
        cur.close()
        return dict(row) if row else {}
    except Exception as e:
        log.error(f"Error fetching ticker for {symbol}: {e}")
        return {}

def get_order_book_imbalance(symbol: str, conn) -> float:
    """
    Calculate bid/ask volume ratio.
    Returns: -1 to +1 (-1 = all ask pressure, +1 = all bid pressure)
    """
    ticker = get_latest_ticker(symbol, conn)
    if not ticker:
        return 0.0
    
    bid_size = float(ticker.get('best_bid_size', 0))
    ask_size = float(ticker.get('best_ask_size', 0))
    
    total_size = bid_size + ask_size
    if total_size == 0:
        return 0.0
    
    return (bid_size - ask_size) / total_size

def calculate_volatility(symbol: str, period: int, conn) -> float:
    """
    Historical volatility calculation over period minutes.
    Returns annualized volatility percentage.
    """
    try:
        cutoff = datetime.utcnow() - timedelta(minutes=period)
        cur = conn.cursor()
        cur.execute("""
            SELECT last FROM tickstick 
            WHERE symbol = %s AND timestamp > %s 
            ORDER BY timestamp
        """, (symbol, cutoff))
        
        prices = [float(row[0]) for row in cur.fetchall()]
        cur.close()
        
        if len(prices) < 2:
            return 0.0
        
        # Calculate returns
        returns = []
        for i in range(1, len(prices)):
            if prices[i-1] > 0:
                ret = (prices[i] - prices[i-1]) / prices[i-1]
                returns.append(ret)
        
        if not returns:
            return 0.0
        
        # Standard deviation of returns
        mean_return = sum(returns) / len(returns)
        variance = sum((r - mean_return) ** 2 for r in returns) / len(returns)
        std_dev = variance ** 0.5
        
        # Annualize (crypto trades 24/7)
        periods_per_year = 365 * 24 * 60 / period
        annualized_vol = std_dev * (periods_per_year ** 0.5) * 100
        
        return annualized_vol
    except Exception as e:
        log.error(f"Error calculating volatility for {symbol}: {e}")
        return 0.0

# ══════════════════════════════════════════════════════════════════════════
#  SECTION 11: STATE MANAGEMENT
# ══════════════════════════════════════════════════════════════════════════

def save_state(state: dict, filepath: str) -> bool:
    """
    Persist bot state to JSON file.
    Creates backup of existing file before overwriting.
    """
    try:
        path = Path(filepath)
        # Backup existing file
        if path.exists():
            backup_path = path.with_suffix('.json.bak')
            path.rename(backup_path)
        
        # Write new state
        with open(filepath, 'w') as f:
            json.dump(state, f, indent=2, default=str)
        
        log.info(f"State saved to {filepath}")
        return True
    except Exception as e:
        log.error(f"Failed to save state: {e}")
        # Restore backup if save failed
        backup_path = Path(filepath).with_suffix('.json.bak')
        if backup_path.exists():
            backup_path.rename(filepath)
        return False

def load_state(filepath: str) -> dict:
    """
    Restore bot state from JSON file.
    Returns empty dict if file doesn't exist or is corrupted.
    """
    try:
        with open(filepath, 'r') as f:
            state = json.load(f)
        log.info(f"State loaded from {filepath}")
        return state
    except FileNotFoundError:
        log.info(f"No state file found at {filepath}, starting fresh")
        return {}
    except json.JSONDecodeError as e:
        log.error(f"State file corrupted: {e}")
        # Try backup
        backup_path = Path(filepath).with_suffix('.json.bak')
        if backup_path.exists():
            try:
                with open(backup_path, 'r') as f:
                    state = json.load(f)
                log.info(f"State loaded from backup")
                return state
            except:
                pass
        return {}
    except Exception as e:
        log.error(f"Failed to load state: {e}")
        return {}

def update_heartbeat(process_name: str, conn=None) -> None:
    """
    Update process heartbeat in database.
    Creates heartbeat row if it doesn't exist.
    """
    own_conn = False
    try:
        if conn is None:
            conn = get_db_connection()
            own_conn = True
        
        cur = conn.cursor()
        cur.execute("""
            INSERT INTO heartbeats (process_name, last_heartbeat, status, pid)
            VALUES (%s, NOW(), 'ok', %s)
            ON CONFLICT (process_name)
            DO UPDATE SET 
                last_heartbeat = NOW(),
                status = 'ok',
                pid = %s
        """, (process_name, os.getpid(), os.getpid()))
        
        conn.commit()
        cur.close()
    except Exception as e:
        log.error(f"Failed to update heartbeat for {process_name}: {e}")
        if conn:
            conn.rollback()
    finally:
        if own_conn and conn:
            release_db_connection(conn)

# ══════════════════════════════════════════════════════════════════════════
#  SECTION 12: PROCESS MANAGEMENT
# ══════════════════════════════════════════════════════════════════════════

def write_pid_file(filepath: str) -> None:
    """
    Create PID file for process monitoring.
    Used by Monit and other process managers.
    """
    try:
        # Ensure directory exists
        Path(filepath).parent.mkdir(parents=True, exist_ok=True)
        
        with open(filepath, 'w') as f:
            f.write(str(os.getpid()))
        
        log.info(f"PID {os.getpid()} written to {filepath}")
    except Exception as e:
        log.error(f"Failed to write PID file: {e}")

def cleanup_pid_file(filepath: str) -> None:
    """Remove PID file on shutdown."""
    try:
        if os.path.exists(filepath):
            os.remove(filepath)
            log.info(f"PID file {filepath} removed")
    except Exception as e:
        log.error(f"Failed to remove PID file: {e}")

def check_process_alive(pid: int) -> bool:
    """
    Verify process is running via PID.
    Cross-platform compatible.
    """
    try:
        os.kill(pid, 0)  # Signal 0 = check if process exists
        return True
    except OSError:
        return False

# ══════════════════════════════════════════════════════════════════════════
#  SECTION 13: RETRY & BACKOFF
# ══════════════════════════════════════════════════════════════════════════

def time_sleep(sec: float) -> None:
    import time as _t; _t.sleep(sec)

def retry(fn: Callable[[], Any],
          max_tries: int = 3,
          base_delay: float = 0.25,
          jitter: bool = True,
          exceptions: Tuple[type, ...] = (Exception,),
          logger: Optional[logging.Logger] = None) -> Any:
    """Simple exponential backoff with optional jitter."""
    logger = logger or log
    delay = base_delay
    for i in range(1, max_tries + 1):
        try:
            return fn()
        except exceptions as e:
            if i == max_tries:
                raise
            sleep_for = delay + (random.random() * delay if jitter else 0.0)
            logger.warning(f"retry {i}/{max_tries-1} after error: {e}; sleeping {sleep_for:.3f}s")
            time_sleep(sleep_for)
            delay *= 2.0

# ══════════════════════════════════════════════════════════════════════════
#  SECTION 14: TICKSTICK REPOSITORY (Alma's cache)
# ══════════════════════════════════════════════════════════════════════════

class TickstickRepo:
    """
    Read-only accessor to the latest ticker snapshot for a symbol from Alma's cache.
    Uses standard PostgreSQL connection to ariadne database.
    """
    def __init__(self):
        pass  # No initialization needed, connections created per query
    
    def _conn(self):
        """Standard PostgreSQL connection to ariadne."""
        return psycopg2.connect(dbname="ariadne", user="postgres", host="localhost")
    
    @staticmethod
    def _norm_row(d: Dict[str, Any]) -> Dict[str, Any]:
        # Collapse case/underscores for tolerant key access
        return {(k or "").lower().replace("_", ""): v for k, v in d.items()}
    
    def latest(self, symbol: str) -> Dict[str, Any]:
        """Get latest ticker data for symbol."""
        sym = symbol.upper()
        sql = """
            SELECT * FROM tickstick
            WHERE symbol = %s
            ORDER BY timestamp DESC
            LIMIT 1
        """
        conn = self._conn()
        try:
            with conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cur:
                cur.execute(sql, (sym,))
                row = cur.fetchone()
                if not row:
                    raise KeyError(f"No tickstick row for {sym}")
                return self._norm_row(dict(row))
        finally:
            conn.close()


================================================================================
FILE: mm/utils/soc/hari.py
================================================================================
#===================================================================
# 🍁 A R I A N D E           bot version 6.1 file build 20250905.01
#===================================================================
# last update: 2025 | Sept. 5                   Production ready ❌
#===================================================================
# Hari - Simulated Market Place
# mm/utils/soc/hari.py
#
# Checks simulated orders and mimicks the market process. 
# Applies "realisms" to keep me humble.
# Records trade via Andi.
#
# [520] [741] [8]
#===================================================================
# 🔰 THE COMMANDER            ✔ PERSISTANT RUNTIME  ✔ MONIT MANAGED
#===================================================================

# 🔸 Standard Library Imports ======================================

import os
import sys
import time
import signal 
import random
import psycopg2
import psycopg2.extras
import logging
from datetime import datetime, timedelta
from decimal import Decimal
from typing import Dict, List, Tuple, Optional
from logging.handlers import RotatingFileHandler

from mm.utils.helpers.wintermute import get_db_connection, release_db_connection
from mm.utils.helpers.inara import get_mode

# 🔸 Constants =====================================================

CHECK_INTERVAL_SECONDS = 5
HEARTBEAT_CYCLES = 12  # Every 60 seconds
REALISM_CHANCE = 0.15  # 15% chance
REALISM_COOLDOWN_HOURS = 12
PID_FILE = os.getenv("SOC_PID_FILE", "/root/Echelon/valentrix/mm/utils/soc/soc.pid")
LOG_FILE = os.getenv("SOC_LOG_FILE", "/root/Echelon/valentrix/mm/utils/soc/soc.log")
MAKER_REST_THRESHOLD_SECONDS = 5  # age on book to count as maker

# 🔸 Market condition thresholds ===================================

PANIC_VOLATILITY_THRESHOLD = 0.05  # 5% in 15 mins
STRESS_VOLATILITY_THRESHOLD = 0.02  # 2% in 15 mins
MOMENTUM_WINDOW_SECONDS = 5         # lookback for short-term drift
MOMENTUM_TOUCHED_THRESH = 0.001     # 0.10% move considered “moving away”
MOMENTUM_SKIP_BONUS = 0.07          # +7% skip chance when moving away

# 🔸 Logging Setup (rotating) ======================================

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# 🔸 Ensure log directory exists ===================================

try:
    os.makedirs(os.path.dirname(LOG_FILE), exist_ok=True)
except Exception:
    pass

rot = RotatingFileHandler(LOG_FILE, maxBytes=10_000_000, backupCount=5)
rot.setFormatter(logging.Formatter('[%(asctime)s] %(levelname)s: %(message)s', '%Y-%m-%d %H:%M:%S'))
stdout = logging.StreamHandler(sys.stdout)
stdout.setFormatter(logging.Formatter('[%(asctime)s] %(levelname)s: %(message)s', '%Y-%m-%d %H:%M:%S'))

# 🔸 avoid duplicate handlers if reloaded ==========================

if not logger.handlers:
    logger.addHandler(rot)
    logger.addHandler(stdout)

# 🔸 Global shutdown flag ==========================================

shutdown_requested = False

def signal_handler(signum, frame):
    """Handle shutdown signals gracefully"""
    global shutdown_requested
    logger.info(f"[SHUTDOWN] Received signal {signum}, shutting down gracefully...")
    shutdown_requested = True

# 🔸 Register signal handlers ======================================

signal.signal(signal.SIGINT, signal_handler)
signal.signal(signal.SIGTERM, signal_handler)

# 🔸 Database Connection ===========================================

def get_db_connection():
    """Get connection to ariadne database"""
    return psycopg2.connect(
        dbname="ariadne",
        user="postgres",
        host="localhost"
    )

# 🔸 PID Management ================================================

def write_pid_file():
    """Write PID file for monit"""
    try:
        os.makedirs(os.path.dirname(PID_FILE), exist_ok=True)
    except Exception:
        pass
    with open(PID_FILE, "w") as f:
        f.write(str(os.getpid()))
    logger.info(f"[INIT] PID {os.getpid()} written to {PID_FILE}")

# 🔸 Market Analysis ===============================================

def calculate_volatility(conn, symbol: str, minutes: int = 10) -> float:
    """Calculate price volatility over specified minutes"""
    try:
        cur = conn.cursor()
        cutoff_time = utc_now_ts() - (minutes * 60)
        
        cur.execute("""
            SELECT MIN(last), MAX(last), AVG(last)
            FROM tickstick
            WHERE symbol = %s AND timestamp > %s
        """, (symbol, cutoff_time))
        
        result = cur.fetchone()
        cur.close()
        
        if result and result[0] and result[1] and result[2]:
            min_price, max_price, avg_price = float(result[0]), float(result[1]), float(result[2])
            if avg_price > 0:
                return (max_price - min_price) / avg_price
        
        return 0.0
    except Exception as e:
        logger.error(f"[ERROR] Failed to calculate volatility for {symbol}: {e}")
        return 0.0

def get_market_condition(conn, symbol: str) -> str:
    """Determine market condition based on volatility"""
    volatility = calculate_volatility(conn, symbol, minutes=15)
    
		    if volatility > PANIC_VOLATILITY_THRESHOLD:
        return "panic"
    elif volatility > STRESS_VOLATILITY_THRESHOLD:
        return "stress"
    else:
        return "normal"

# 🔸 Fix UTC Seconds ===============================================

def utc_now_ts() -> int:
    """UTC 'now' as integer seconds."""
    return int(datetime.utcnow().timestamp())

# 🔸 Order Book Analysis ===========================================

def get_order_book_state(conn, symbol: str) -> Dict:
    """Get current order book state for realism calculations, normalized:
       - both bid/ask > 0
       - ask >= bid (if inverted by <= 2 ticks, clamp to 'touch')
       - sizes non-negative
    """
    try:
        cur = conn.cursor()
        cur.execute("""
            SELECT buy, sell, best_bid_size, best_ask_size, vol
            FROM tickstick
            WHERE symbol = %s
            ORDER BY timestamp DESC
            LIMIT 1
        """, (symbol,))
        row = cur.fetchone()
        cur.close()

        if not row:
            return None

        bid_raw  = float(row[0]) if row[0] is not None else 0.0
        ask_raw  = float(row[1]) if row[1] is not None else 0.0
        bid_sz   = float(row[2]) if row[2] is not None else 0.0
        ask_sz   = float(row[3]) if row[3] is not None else 0.0
        volume   = float(row[4]) if row[4] is not None else 0.0

        # Basic positivity checks
        if bid_raw <= 0.0 or ask_raw <= 0.0:
            return None

        # Enforce ask >= bid; if slightly inverted (<= 2 ticks), clamp to touch
        tick = get_price_increment(conn, symbol)
        if ask_raw < bid_raw:
            diff = bid_raw - ask_raw
            if tick and diff <= (2.0 * tick):
                # Treat as touch (micro inversion noise): clamp ask to bid
                ask_raw = bid_raw
            else:
                # Inverted beyond tolerance: discard snapshot
                logger.warning(f"[BOOK] Inverted book for {symbol}: bid={bid_raw} > ask={ask_raw} (diff={diff}); skipping snapshot")
                return None

        # Non-negative sizes
        bid_sz = max(bid_sz, 0.0)
        ask_sz = max(ask_sz, 0.0)

        return {
            'bid': bid_raw,
            'ask': ask_raw,
            'bid_size': bid_sz,
            'ask_size': ask_sz,
            'volume': volume
        }

    except Exception as e:
        logger.error(f"[ERROR] Failed to get/normalize order book for {symbol}: {e}")
        return None

# 🔸 Realism Functions =============================================

def check_realism_history(conn, symbol: str) -> bool:
    """Check if symbol had realism applied in last 12 hours"""
    try:
        cur = conn.cursor()
        cutoff_time = datetime.utcnow() - timedelta(hours=REALISM_COOLDOWN_HOURS)
        
        cur.execute("""
            SELECT COUNT(*) FROM realism_history
            WHERE symbol = %s AND applied_at > %s
        """, (symbol, cutoff_time))
        
        count = cur.fetchone()[0]
        cur.close()
        return count > 0
    except Exception as e:
        logger.error(f"[ERROR] Failed to check realism history: {e}")
        return False

def record_realism(conn, order_id: str, symbol: str, realism_type: str, details: Dict):
    """Record realism application in history"""
    try:
        cur = conn.cursor()
        cur.execute("""
            INSERT INTO realism_history (order_id, symbol, realism_type, applied_at, details)
            VALUES (%s, %s, %s, NOW(), %s::jsonb)
            ON CONFLICT (order_id) DO NOTHING
        """, (order_id, symbol, realism_type, psycopg2.extras.Json(details)))
        conn.commit()
        cur.close()
    except Exception as e:
        logger.error(f"[ERROR] Failed to record realism: {e}")
        conn.rollback()

def select_realism_type(market_condition: str) -> str:
    """Select realism type with weights based on market condition"""
    if market_condition == "panic":
        # In panic: more skips and delays
        types = ["slippage", "partial", "delay", "skip_touch"]
        weights = [0.2, 0.2, 0.3, 0.3]
    elif market_condition == "stress":
        # In stress: balanced with slight preference for delays
        types = ["slippage", "partial", "delay", "skip_touch"]
        weights = [0.3, 0.2, 0.3, 0.2]
    else:
        # Normal: focus on slippage
        types = ["slippage", "partial", "delay", "skip_touch"]
        weights = [0.4, 0.2, 0.2, 0.2]
    
    return random.choices(types, weights=weights, k=1)[0]

def apply_slippage(order: Dict, book_state: Dict, market_condition: str, quote_min_size: float) -> Dict:
    """Apply slippage realism to order"""
    # Determine slippage multiplier based on market
    if market_condition == "panic":
        multiplier = random.randint(10, 25)
    elif market_condition == "stress":
        multiplier = random.randint(5, 15)
    else:
        multiplier = random.randint(1, 5)
    
    # Compute slippage as NOTIONAL (USDT), then convert to per-unit price slip — using Decimal
    slippage_notional = Decimal(str(quote_min_size)) * Decimal(str(multiplier))

    # Remaining quantity to be filled (respect existing partials)
    remaining_qty = max(order['size'] - order.get('filled_size', 0), 0)
    if remaining_qty <= 0:
        order['skip'] = True
        order['realism_applied'] = 'slippage'
        order['slippage_amount'] = Decimal("0")
        logger.warning(f"[REALISM] Slippage: no remaining qty for order {order.get('id')} ({order['symbol']}); skipping.")
        return order

    d_remaining = Decimal(str(remaining_qty))
    price_slip_dec = slippage_notional / d_remaining  # Decimal per-unit move

    # 2% chance of favorable slippage (helps the trade)
    favorable = (random.random() < 0.02)

    # Apply slippage by side and favorability (Decimal → float for tick rounding)
    base_price = Decimal(str(order['price']))
    if order['side'] == 'buy':
        new_price_dec = (base_price - price_slip_dec) if favorable else (base_price + price_slip_dec)
    else:  # sell
        new_price_dec = (base_price + price_slip_dec) if favorable else (base_price - price_slip_dec)

    # Tick-round the final fill price
    tick = get_price_increment(conn, order['symbol'])
    order['fill_price'] = round_price_to_tick(float(new_price_dec), tick)

    # Sanity check — ensure minimum NOTIONAL at intended fill qty
    intended_fill_qty = order.get('partial_fill_qty', remaining_qty)
    if (Decimal(str(order['fill_price'])) * Decimal(str(intended_fill_qty))) < Decimal(str(quote_min_size)):
        order['skip'] = True

    order['realism_applied'] = 'slippage'
    # Positive means trader-unfavorable notional drag; negative means favorable
    order['slippage_amount'] = (-slippage_notional) if favorable else slippage_notional
    order['favorable_slippage'] = favorable

    logger.info(
        f"[REALISM] Slippage applied to {order['symbol']} "
        f"(notional={slippage_notional}, favorable={favorable})"
    )
    return order

def apply_partial_fill(order: Dict) -> Dict:
    """Apply partial fill realism"""
    # Compute partials on REMAINING qty (respect prior fills)
    already_filled = order.get('filled_size', 0) or 0
    remaining = max(order['size'] - already_filled, 0)

    if remaining <= 0:
        # Nothing to do; mark and return
        order['partial_fill_qty'] = 0
        order['remaining_qty'] = 0
        order['cancel_remainder'] = False
        order['realism_applied'] = 'partial'
        order['fill_percentage'] = 0.0
        logger.warning(f"[REALISM] Partial: no remaining qty for order {order.get('id')} ({order['symbol']}); skipping.")
        return order

    # Random fill between 40–90% of the remaining amount
    fill_percentage = random.uniform(0.4, 0.9)
    partial_qty = remaining * fill_percentage

    # Cap to remaining (numerical safety)
    partial_qty = min(partial_qty, remaining)

    # Round both quantities to base increment
    base_step = get_base_increment(conn, order['symbol'])
    partial_qty = round_qty_to_increment(partial_qty, base_step, side=order.get('side'))
    remaining_after = max(remaining - partial_qty, 0.0)
    remaining_after = round_qty_to_increment(remaining_after, base_step, side=order.get('side'))

    order['partial_fill_qty'] = partial_qty
    order['remaining_qty'] = remaining_after

    # 70% chance remainder STAYS on book, 30% CANCELLED
    order['cancel_remainder'] = (random.random() < 0.3)

    order['realism_applied'] = 'partial'
    order['fill_percentage'] = fill_percentage

    logger.info(
        f"[REALISM] Partial fill {fill_percentage:.1%} for {order['symbol']} "
        f"(filled_now={partial_qty}, remaining_after={order['remaining_qty']})"
    )
    return order

def apply_delay(order: Dict) -> Dict:
    """Apply fill delay realism"""
    delay_seconds = random.uniform(0.2, 2.5)
    order['fill_after'] = time.time() + delay_seconds
    order['realism_applied'] = 'delay'
    order['delay_seconds'] = delay_seconds
    
    logger.info(f"[REALISM] Applied {delay_seconds:.1f}s delay to {order['symbol']} order")
    return order

def apply_skip_touch(order: Dict, book_state: Dict, momentum: float = 0.0, record: bool = True) -> Dict:
    """Apply skip-on-touch realism with pressure, imbalance, and short-term momentum."""
    skip_chance = 0.10  # base 10%

    # Order-pressure vs top-of-book size
    if order['side'] == 'buy':
        top_qty = book_state.get('ask_size', 0)
    else:
        top_qty = book_state.get('bid_size', 0)

    if top_qty > 0:
        order_pressure = (order['size'] / top_qty)
        if order_pressure > 0.25:
            skip_chance += 0.05  # +5% if large vs top level

    # Book imbalance (-1..+1)
    total_vol = book_state.get('bid_size', 0) + book_state.get('ask_size', 0)
    if total_vol > 0:
        imbalance = (book_state.get('bid_size', 0) - book_state.get('ask_size', 0)) / total_vol
        if (imbalance < -0.3 and order['side'] == 'sell') or (imbalance > 0.3 and order['side'] == 'buy'):
            skip_chance += 0.05  # +5% when book bias works against your side

    # Short-term momentum away from the order price
    if (order['side'] == 'buy' and momentum > MOMENTUM_TOUCHED_THRESH) or \
       (order['side'] == 'sell' and momentum < -MOMENTUM_TOUCHED_THRESH):
        skip_chance += MOMENTUM_SKIP_BONUS  # e.g., +7%

    # Cap to a reasonable max
    if skip_chance > 0.50:
        skip_chance = 0.50

    # Roll for skip
    if random.random() < skip_chance:
        order['skip'] = True
        if record:
            order['realism_applied'] = 'skip_touch'
            order['skip_chance'] = skip_chance

    return order

# 🔸 Order Processing ==============================================

def should_fill_order(order: Dict, ticker_data: Dict) -> bool:
    """Fill predicate with safety guards:
       - require bid/ask present and > 0
       - require ask >= bid (otherwise ignore snapshot)
       - standard cross logic
    """
    if not ticker_data:
        return False

    bid = ticker_data.get('bid', 0.0) or 0.0
    ask = ticker_data.get('ask', 0.0) or 0.0

    # Basic validity
    if bid <= 0.0 or ask <= 0.0:
        return False

    # Reject inverted book (should be handled upstream, but double-guard here)
    if bid > ask:
        return False

    # Fill conditions
    if order['side'] == 'buy':
        # Buy fills when market ask <= our bid
        return ask <= order['price']
    else:
        # Sell fills when market bid >= our ask
        return bid >= order['price']

def get_quote_min_size(conn, symbol: str) -> float:
    """Get quote minimum size for a symbol"""
    try:
        cur = conn.cursor()
        cur.execute("""
            SELECT quote_min_size FROM symbol_info
            WHERE symbol = %s
        """, (symbol,))
        
        result = cur.fetchone()
        cur.close()
        
        if result:
            return float(result[0])
        else:
            # Default fallback
            return 0.00001
    except:
        return 0.00001
    
def get_price_increment(conn, symbol: str) -> float:
    """Get price increment (tick size) for a symbol"""
    try:
        cur = conn.cursor()
        cur.execute("""
            SELECT price_increment 
            FROM symbol_info
            WHERE symbol = %s
        """, (symbol,))
        row = cur.fetchone()
        cur.close()
        if row and row[0] is not None:
            return float(row[0])
    except Exception:
        pass
    # Fallback to a small tick if unknown (will be refined in Step #5 rounding)
    return 1e-8


def prices_touch(p1: float, p2: float, tick: float) -> bool:
    """
    Consider prices 'touching' if they are within half a tick.
    Avoids fragile float equality.
    """
    if tick <= 0:
        # Defensive fallback; treat exact equality as touch
        return abs(p1 - p2) == 0.0
    return abs(p1 - p2) <= (tick * 0.5)

def get_short_term_momentum(conn, symbol: str, seconds: int = MOMENTUM_WINDOW_SECONDS) -> float:
    """
    Return short-term mid-price return over ~`seconds`:
      (mid_now - mid_prev) / mid_prev
    Uses latest mid and the last mid at or before the cutoff.
    If data missing or invalid, returns 0.0.
    """
    try:
        cutoff = utc_now_ts() - int(seconds)

        # prev mid at/just before cutoff
        cur = conn.cursor()
        cur.execute("""
            SELECT buy, sell
            FROM tickstick
            WHERE symbol = %s AND timestamp <= %s
            ORDER BY timestamp DESC
            LIMIT 1
        """, (symbol, cutoff))
        prev = cur.fetchone()

        # latest mid now
        cur.execute("""
            SELECT buy, sell
            FROM tickstick
            WHERE symbol = %s
            ORDER BY timestamp DESC
            LIMIT 1
        """, (symbol,))
        curr = cur.fetchone()
        cur.close()

        if not prev or not curr or not prev[0] or not prev[1] or not curr[0] or not curr[1]:
            return 0.0

        mid_prev = (float(prev[0]) + float(prev[1])) / 2.0
        mid_now  = (float(curr[0]) + float(curr[1])) / 2.0
        if mid_prev <= 0:
            return 0.0

        return (mid_now - mid_prev) / mid_prev
    except Exception:
        return 0.0

def has_realism_for_order(conn, order_id: str) -> bool:
    """Return True if this order already had any realism applied."""
    try:
        cur = conn.cursor()
        cur.execute(
            "SELECT 1 FROM realism_history WHERE order_id = %s LIMIT 1",
            (order_id,)
        )
        found = cur.fetchone() is not None
        cur.close()
        return found
    except Exception:
        # Defensive: if in doubt, do not double-apply realism
        return True
    
def get_base_increment(conn, symbol: str) -> float:
    """Get minimum increment for base asset quantity (size step)."""
    try:
        cur = conn.cursor()
        cur.execute("""
            SELECT base_increment
            FROM symbol_info
            WHERE symbol = %s
        """, (symbol,))
        row = cur.fetchone()
        cur.close()
        if row and row[0] is not None:
            return float(row[0])
    except Exception:
        pass
    return 1e-8  # defensive fallback


def get_quote_increment(conn, symbol: str) -> float:
    """Get minimum increment for quote (USDT) amounts, if needed for validations."""
    try:
        cur = conn.cursor()
        cur.execute("""
            SELECT quote_increment
            FROM symbol_info
            WHERE symbol = %s
        """, (symbol,))
        row = cur.fetchone()
        cur.close()
        if row and row[0] is not None:
            return float(row[0])
    except Exception:
        pass
    return 1e-8  # defensive fallback


# ---- Rounding helpers (Decimal-based for correctness; return float) ----
from decimal import Decimal, getcontext, ROUND_FLOOR, ROUND_CEILING, ROUND_HALF_UP
getcontext().prec = 28  # high precision for crypto math


def _to_dec(x) -> Decimal:
    # Accept float/str/Decimal safely
    return x if isinstance(x, Decimal) else Decimal(str(x))


def round_to_increment(value: float, increment: float, mode: str = "NEAREST") -> float:
    """
    Round value to a multiple of increment.
    mode: "NEAREST" (half-up), "FLOOR", "CEIL"
    """
    v = _to_dec(value)
    inc = _to_dec(increment) if increment else _to_dec(0)
    if inc <= 0:
        # No increment info; return as-is
        return float(v)

    q = v / inc
    if mode == "FLOOR":
        q = q.to_integral_value(rounding=ROUND_FLOOR)
    elif mode == "CEIL":
        q = q.to_integral_value(rounding=ROUND_CEILING)
    else:  # NEAREST half-up
        q = q.to_integral_value(rounding=ROUND_HALF_UP)

    return float(q * inc)


def round_price_to_tick(price: float, tick: float) -> float:
    return round_to_increment(price, tick, mode="NEAREST")


def round_qty_to_increment(qty: float, step: float, side: str = None) -> float:
    """
    Round qty to base increment. By default, NEAREST.
    If you prefer conservative rounding by side, you can use:
      - buys: FLOOR (avoid exceeding funds)
      - sells: FLOOR (avoid selling more than held)
    For now we use NEAREST to keep behavior consistent, can revisit later.
    """
    return round_to_increment(qty, step, mode="NEAREST")

def _clear_cap_if_empty(asset: str) -> None:
    """
    If total remaining position for `asset` is zero (or dust), clear the cap in inventory_limits.
    Mode-aware: checks positions (live) or sim_positions (simulation).
    """
    conn = None
    try:
        try:
            mode = get_mode()
        except Exception:
            mode = "simulation"

        table = "positions" if mode == "live" else "sim_positions"
        dust = Decimal("0.00000001")

        conn = get_db_connection()
        cur = conn.cursor()
        cur.execute(f"SELECT COALESCE(SUM(quantity),0) FROM {table} WHERE asset = %s", (asset.upper(),))
        remaining = Decimal(str(cur.fetchone()[0] or 0))

        if remaining <= dust:
            cur.execute("""
                UPDATE inventory_limits
                   SET capped = FALSE,
                       reason = NULL,
                       capped_at = NULL,
                       updated_by = 'Hari',
                       updated_at = NOW()
                 WHERE asset = %s
            """, (asset.upper(),))
            conn.commit()
        cur.close()
    except Exception as e:
        logger.warning(f"[CAP] Clear check failed for {asset}: {e}")
        if conn:
            conn.rollback()
    finally:
        if conn:
            conn.close()


def get_fee_rates_from_tick(conn, symbol: str) -> tuple[float, float]:
    """
    Returns (maker_rate_dec, taker_rate_dec) from latest tickstick row.
    Accepts either decimals (0.0010) or bps (10) and coerces to decimals.
    Optional coeff columns are applied if present.
    """
    MAKER_KEYS = ("maker_fee", "maker_fee_bps", "maker_bps", "maker_rate")
    TAKER_KEYS = ("taker_fee", "taker_fee_bps", "taker_bps", "taker_rate")
    MAKER_COEFF_KEYS = ("maker_coeff", "maker_coef", "maker_coefficient")
    TAKER_COEFF_KEYS = ("taker_coeff", "taker_coef", "taker_coefficient")

    try:
        cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        cur.execute("""
            SELECT *
            FROM tickstick
            WHERE symbol = %s
            ORDER BY timestamp DESC
            LIMIT 1
        """, (symbol,))
        row = cur.fetchone() or {}
        cur.close()

        def pick(row, keys, default=None):
            for k in keys:
                if k in row and row[k] is not None:
                    return float(row[k])
            return default

        maker_raw = pick(row, MAKER_KEYS, 0.0010)
        taker_raw = pick(row, TAKER_KEYS, 0.0010)
        m_coeff = pick(row, MAKER_COEFF_KEYS, 1.0)
        t_coeff = pick(row, TAKER_COEFF_KEYS, 1.0)

        maker = (maker_raw / 10000.0) if maker_raw > 1 else maker_raw
        taker = (taker_raw / 10000.0) if taker_raw > 1 else taker_raw

        maker *= m_coeff
        taker *= t_coeff

        maker = max(0.0, min(maker, 0.01))
        taker = max(0.0, min(taker, 0.01))
        return (maker, taker)
    except Exception:
        return (0.0010, 0.0010)

def process_fill(conn, order: Dict, fill_price: float, fill_qty: float, fee_amount: float = 0.0, role: str = None):
    """Atomic fill with row lock on sim_orders to prevent double-counting."""
    try:
        cur = conn.cursor()

        # 1) Lock the order row and fetch status/filled/size
        cur.execute("""
            SELECT status, COALESCE(filled_size, 0), size, side, symbol
            FROM sim_orders
            WHERE id = %s
            FOR UPDATE
        """, (order['id'],))
        row = cur.fetchone()
        if not row:
            cur.close()
            return
        status_db, filled_db, size_db, side_db, symbol_db = row

        # If already closed, bail
        if status_db not in ('open', 'partial'):
            cur.close()
            return

        # 2) Decimal-safe scalars
        d_price = Decimal(str(fill_price))
        d_qty   = Decimal(str(fill_qty))
        d_fee   = Decimal(str(fee_amount)) if fee_amount is not None else Decimal("0")
        d_zero  = Decimal("0")

        # 3) Clamp to remaining
        remaining = Decimal(str(size_db)) - Decimal(str(filled_db))
        if remaining <= d_zero:
            cur.close()
            return
        d_qty = min(d_qty, remaining)  # cannot overfill

        # 4) Insert trade
        cur.execute("""
            INSERT INTO sim_trades (timestamp, symbol, side, price, size, sim_order_id, fee)
            VALUES (NOW(), %s, %s, %s, %s, %s, %s)
        """, (symbol_db, side_db, d_price, d_qty, order['id'], d_fee))

        # 5) Update order status/filled
        new_filled = Decimal(str(filled_db)) + d_qty
        if new_filled + d_zero < Decimal(str(size_db)):
            # still partial
            cur.execute("""
                UPDATE sim_orders
                SET filled_size = %s,
                    status = 'partial',
                    updated_at = NOW()
                WHERE id = %s
            """, (new_filled, order['id']))
        else:
            # full
            cur.execute("""
                UPDATE sim_orders
                SET filled_size = size,
                    status = 'filled',
                    filled_at = NOW(),
                    updated_at = NOW(),
                    deleted = TRUE,
                    deleted_at = NOW()
                WHERE id = %s
            """, (order['id'],))

        # 6) Balances
        base_asset = symbol_db.split('-')[0]
        notional = d_price * d_qty

        if side_db == 'buy':
            usdt_deduct = notional + d_fee
            # deduct USDT (available & hold)
            cur.execute("""
                UPDATE sim_balances
                SET available = available - %s,
                    hold = hold - %s
                WHERE asset = 'USDT'
            """, (usdt_deduct, usdt_deduct))
            # credit base asset
            cur.execute("""
                INSERT INTO sim_balances (asset, available, hold)
                VALUES (%s, %s, 0)
                ON CONFLICT (asset)
                DO UPDATE SET available = sim_balances.available + %s
            """, (base_asset, d_qty, d_qty))
        else:
            usdt_credit = notional - d_fee
            # debit base asset (available & hold)
            cur.execute("""
                UPDATE sim_balances
                SET available = available - %s,
                    hold = hold - %s
                WHERE asset = %s
            """, (d_qty, d_qty, base_asset))
            # credit USDT (available)
            cur.execute("""
                UPDATE sim_balances
                SET available = available + %s
                WHERE asset = 'USDT'
            """, (usdt_credit,))

        conn.commit()
        cur.close()

        logger.info(f"[FILL] Processed {side_db} {d_qty} {symbol_db} @ {d_price}")
        logger.info(f"[FEE] {symbol_db} {side_db} role={role} fee={d_fee} notional={notional}")
        
        # If this was a SELL, check whether to clear asset cap
        if side_db == 'sell':
            _clear_cap_if_empty(base_asset)

    except Exception as e:
        logger.error(f"[ERROR] Failed to process fill atomically: {e}")
        conn.rollback()

# 🔸 Main Order Checker ============================================

class SimOrderChecker:
    def __init__(self):
        self.cycle_count = 0
        self.delayed_orders = {}  # Track orders with delay realism
        
    def check_orders(self):
        """Main order checking logic"""
        conn = get_db_connection()
        
        try:
            # Get all open orders
            cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
            cur.execute("""
                SELECT id, symbol, side, price, size, 
                       COALESCE(filled_size, 0) as filled_size,
                       created_at
                FROM sim_orders
                WHERE status IN ('open', 'partial')
                  AND COALESCE(deleted, FALSE) = FALSE
                ORDER BY created_at
            """)
            
            orders = cur.fetchall()
            cur.close()
            
            # Process each order
            for order in orders:
                # Check delayed orders
                if order['id'] in self.delayed_orders:
                    plan = self.delayed_orders[order['id']]
                    if time.time() < plan['fill_after']:
                        continue  # Still delayed
                    else:
                        # Mark for post-delay re-validation; delete AFTER we recheck market
                        order['_delay_expired'] = True
                        order['_delay_plan'] = plan
                
                # Get current market data
                book_state = get_order_book_state(conn, order['symbol'])
                if not book_state:
                    continue
                
                # Check if order should fill
                if should_fill_order(order, book_state):
                    # If a delay just expired, re-validate execution conditions at the moment of fill
                    if order.get('_delay_expired'):
			# Ensure the order is still open/partial at delay expiry
			_cur = conn.cursor()
			_cur.execute("SELECT status FROM sim_orders WHERE id = %s", (order['id'],))
			_row = _cur.fetchone()
			_cur.close()
                            if not _row or _row[0] not in ('open', 'partial'):
			    try:
 				del self.delayed_orders[order['id']]
			    except KeyError:
                                pass
 			    continue
                        # Optional: if the touch is razor-thin, allow a skip check without re-tagging realism
                        tick = get_price_increment(conn, order['symbol'])
                        do_touch_check = (
                            (order['side'] == 'buy'  and prices_touch(order['price'], book_state['ask'], tick)) or
                            (order['side'] == 'sell' and prices_touch(order['price'], book_state['bid'], tick))
                        )
                        if do_touch_check:
                            mom = get_short_term_momentum(conn, order['symbol'], seconds=MOMENTUM_WINDOW_SECONDS)
                            _before = order.get('skip', False)
                            order = apply_skip_touch(order, book_state, momentum=mom, record=False)
                            # If we decided to skip due to touch after delay, honor it and clear the delay plan
                            if order.get('skip', False) and not _before:
                                # Delay is consumed; remove from queue and move on
                                try:
                                    del self.delayed_orders[order['id']]
                                except KeyError:
                                    pass
                                continue  # skip this order this cycle
                        # Delay is consumed regardless; remove the plan
                        try:
                            del self.delayed_orders[order['id']]
                        except KeyError:
                            pass

                    # Determine if realism should apply (per-order, not per-symbol)
                    apply_realism = False
                    realism_type = None

                    if not has_realism_for_order(conn, order['id']) and (random.random() < REALISM_CHANCE):
                        apply_realism = True
                        market_condition = get_market_condition(conn, order['symbol'])
                        realism_type = select_realism_type(market_condition)
                    
                    # Apply realism if selected
                    if apply_realism:
                        quote_min_size = get_quote_min_size(conn, order['symbol'])
                        
                        if realism_type == 'slippage':
                            order = apply_slippage(order, book_state, market_condition, quote_min_size)
                        elif realism_type == 'partial':
                            order = apply_partial_fill(order)
                        elif realism_type == 'delay':
                            order = apply_delay(order)
                            # Store a lightweight plan (timestamp only) to avoid stale order dicts
                            self.delayed_orders[order['id']] = {
                                'fill_after': order['fill_after']
                            }
                            continue  # Don't process now
                        elif realism_type == 'skip_touch':
                            # Only apply if price "touches" within half a tick
                            tick = get_price_increment(conn, order['symbol'])
                            if (order['side'] == 'buy' and prices_touch(order['price'], book_state['ask'], tick)) or \
                               (order['side'] == 'sell' and prices_touch(order['price'], book_state['bid'], tick)):
                                mom = get_short_term_momentum(conn, order['symbol'], seconds=MOMENTUM_WINDOW_SECONDS)
                                order = apply_skip_touch(order, book_state, momentum=mom)
                        
                        # Record realism application
                        if 'realism_applied' in order:
                            details = {
                                'market_condition': market_condition,
                                'book_state': book_state,
                                'realism_details': {k: v for k, v in order.items() 
                                                  if k in ['slippage_amount', 'fill_percentage', 
                                                          'delay_seconds', 'skip_chance']}
                            }
                            record_realism(conn, order['id'], order['symbol'], 
                                         order['realism_applied'], details)
                    
                    # Process fill if not skipped
                    if not order.get('skip', False):
                        fill_price = order.get('fill_price', order['price'])
                        fill_qty = order.get('partial_fill_qty', order['size'] - order['filled_size'])

                        # Final safety rounding before committing:
                        tick = get_price_increment(conn, order['symbol'])
                        base_step = get_base_increment(conn, order['symbol'])

                        fill_price = round_price_to_tick(fill_price, tick)
                        fill_qty = round_qty_to_increment(fill_qty, base_step, side=order.get('side'))
                        
                        # Min-notional guard: reject dust fills below exchange minimum
                        quote_min = get_quote_min_size(conn, order['symbol'])
                        if (fill_price * fill_qty) < quote_min:
                            logger.info(f"[GUARD] Notional {fill_price * fill_qty:.12f} < min {quote_min:.12f} for {order['symbol']} — skipping fill this cycle")
                            # Do not fill; keep order status unchanged so it can fill later when qty/notional is sufficient
                            continue

                        # Classify maker/taker by age on book
                        role = 'taker'
                        try:
                            if order.get('created_at'):
                                age = (datetime.utcnow() - order['created_at']).total_seconds()
                                role = 'maker' if age >= MAKER_REST_THRESHOLD_SECONDS else 'taker'
                        except Exception:
                            role = 'taker'

                        # Fee from latest tickstick row (Decimal-safe)
                        maker_rate, taker_rate = get_fee_rates_from_tick(conn, order['symbol'])
                        fee_rate_dec = Decimal(str(maker_rate if role == 'maker' else taker_rate))

                        trade_value_dec = Decimal(str(fill_price)) * Decimal(str(fill_qty))
                        fee_amount_dec = trade_value_dec * fee_rate_dec

                        process_fill(conn, order, fill_price, fill_qty, fee_amount=fee_amount_dec, role=role)
                        
                        # Handle partial fill remainder
                        if order.get('cancel_remainder', False):
                            remaining_qty = float(order.get('remaining_qty', 0.0))
                            asset = order['symbol'].split('-')[0]
                            try:
                                cur = conn.cursor()

                                if remaining_qty > 0:
                                    if order['side'] == 'buy':
                                        # Release USDT hold equal to remaining notional at limit price
                                        remaining_notional = float(order['price']) * remaining_qty
                                        cur.execute("""
                                            UPDATE sim_balances
                                            SET available = available + %s,
                                                hold = GREATEST(hold - %s, 0)
                                            WHERE asset = 'USDT'
                                        """, (remaining_notional, remaining_notional))
                                    else:
                                        # Release BASE asset hold equal to remaining units
                                        cur.execute("""
                                            UPDATE sim_balances
                                            SET available = available + %s,
                                                hold = GREATEST(hold - %s, 0)
                                            WHERE asset = %s
                                        """, (remaining_qty, remaining_qty, asset))

                                # Cancel & soft-delete the order (per #6)
                                cur.execute("""
                                    UPDATE sim_orders
                                    SET status = 'cancelled',
                                        updated_at = NOW(),
                                        deleted = TRUE,
                                        deleted_at = NOW()
                                    WHERE id = %s
                                """, (order['id'],))

                                conn.commit()
                                cur.close()
                            except Exception as e:
                                logger.error(f"[ERROR] Failed to release holds on cancel for order {order['id']}: {e}")

                        elif order.get('realism_applied') == 'partial' and float(order.get('remaining_qty', 0.0)) > 0:
                            # Keep order OPEN: ensure holds reflect the remaining portion
                            remaining_qty = float(order['remaining_qty'])
                            asset = order['symbol'].split('-')[0]
                            try:
                                cur = conn.cursor()
                                if order['side'] == 'buy':
                                    # Ensure at least remaining_notional is held in USDT
                                    target_notional = float(order['price']) * remaining_qty
                                    cur.execute("""
                                        UPDATE sim_balances
                                        SET hold = GREATEST(hold, %s)
                                        WHERE asset = 'USDT'
                                    """, (target_notional,))
                                else:
                                    # Ensure at least remaining_qty units are held in BASE asset
                                    cur.execute("""
                                        UPDATE sim_balances
                                        SET hold = GREATEST(hold, %s)
                                        WHERE asset = %s
                                    """, (remaining_qty, asset))

                                conn.commit()
                                cur.close()
                            except Exception as e:
                                logger.error(f"[WARN] Could not normalize holds for open remainder on order {order['id']}: {e}")
                
        except Exception as e:
            logger.error(f"[ERROR] Order checking failed: {e}")
        finally:
            conn.close()
    
    def update_heartbeat(self):
        """Update heartbeat in database"""
        try:
            conn = get_db_connection()
            cur = conn.cursor()
            cur.execute("""
                INSERT INTO heartbeats (process_name, last_heartbeat, status, pid, cycle_count)
                VALUES ('soc', NOW(), 'ok', %s, %s)
                ON CONFLICT (process_name)
                DO UPDATE SET 
                    last_heartbeat = NOW(),
                    status = 'ok',
                    pid = %s,
                    cycle_count = %s
            """, (os.getpid(), self.cycle_count, os.getpid(), self.cycle_count))
            conn.commit()
            cur.close()
            conn.close()
            logger.info(f"[HEARTBEAT] Updated (cycle {self.cycle_count})")
        except Exception as e:
            logger.error(f"[ERROR] Failed to update heartbeat: {e}")
    
    def run_continuous(self):
        """Run continuously with graceful shutdown"""

        # Safety: refuse to run when GOLIVE is enabled
        GOLIVE = None

        # Try multiple import paths in order of preference
        try:
            from mm.config.marcus import GOLIVE
        except ImportError:
            try:
                from ariadne_config import GOLIVE
            except ImportError:
                try:
                    from config import GOLIVE
                except ImportError:
                    pass

        # Also check environment variable as fallback
        if GOLIVE is None:
            GOLIVE = os.getenv('GOLIVE', 'false').lower() in ('true', '1', 'yes')

        # If still None, assume simulation mode (safe default)
        if GOLIVE is None:
            logger.warning("[WARN] Could not determine GOLIVE status, assuming simulation mode")
            GOLIVE = False

        if GOLIVE:
            logger.error("[ABORT] GOLIVE=True — SOC (simulation) must not run in live mode")
            return

        logger.info("[INIT] SOC starting up in simulation mode...")
        write_pid_file()
        
        # Ensure realism_history table exists
        conn = get_db_connection()
        cur = conn.cursor()
        cur.execute("""
            CREATE TABLE IF NOT EXISTS realism_history (
                order_id TEXT PRIMARY KEY,
                symbol TEXT NOT NULL,
                realism_type TEXT NOT NULL,
                applied_at TIMESTAMP NOT NULL,
                details JSONB
            );
            CREATE INDEX IF NOT EXISTS idx_realism_symbol_time 
            ON realism_history(symbol, applied_at DESC);
        """)
        conn.commit()
        cur.close()
        conn.close()
        
        while not shutdown_requested:
            try:
                self.check_orders()
                self.cycle_count += 1
                
                # Update heartbeat every 12 cycles
                if self.cycle_count % HEARTBEAT_CYCLES == 0:
                    self.update_heartbeat()
                
                time.sleep(CHECK_INTERVAL_SECONDS)
                
            except Exception as e:
                logger.error(f"[ERROR] Main loop error: {e}")
                time.sleep(CHECK_INTERVAL_SECONDS)
        
        logger.info("[SHUTDOWN] SOC shutting down gracefully")

# 🔸 Main Entry Point ==============================================

if __name__ == "__main__":
    checker = SimOrderChecker()
    checker.run_continuous()

================================================================================
FILE: mm/utils/soc/petra.pid
================================================================================
939402

================================================================================
FILE: mm/utils/partition_manager/edith.py
================================================================================
#===================================================================
# 🍁 A R I A N D E           bot version 6.1 file build 20250914.01
#===================================================================
# last update: 2025 | Sept. 14                  Production ready ✅
#===================================================================
# Edith - Partition Manager
# mm/utils/partition_manager/edith.py
#
# Manages hourly partitions and sweeps expired proposals
#
# [520] [741] [8]
#===================================================================
# 🔰 THE COMMANDER            ✔ PERSISTANT RUNTIME  ✔ MONIT MANAGED
#===================================================================

import os
import sys
import time
import signal
import atexit
import psycopg2
import logging
import smtplib
import ssl
import uuid
import importlib
from datetime import datetime, timedelta
from pathlib import Path
from email.message import EmailMessage
from email.utils import formataddr
from zoneinfo import ZoneInfo

from mm.utils.helpers.wintermute import update_heartbeat
import mm.config.marcus as marcus

# ── Constants ─────────────────────────────────────────────────────────
LOG_FILE = "/root/Echelon/valentrix/mm/utils/partition_manager/edith.log"
PID_FILE = "/root/Echelon/valentrix/mm/utils/partition_manager/edith.pid"
CYCLE_INTERVAL = 600  # 10 minutes in seconds
PARTITION_CYCLES = 6  # Run partitioning every 6 cycles (1 hour)
FUTURE_HOURS = 3  # Create partitions for next 3 hours

# ── Global shutdown flag ─────────────────────────────────────────────
shutdown_requested = False

def signal_handler(signum, frame):
    """Handle SIGTERM and SIGINT for graceful shutdown"""
    global shutdown_requested
    logger.info(f"[SHUTDOWN] Received signal {signum}, shutting down gracefully...")
    shutdown_requested = True

signal.signal(signal.SIGTERM, signal_handler)
signal.signal(signal.SIGINT, signal_handler)

# ── PID Management ────────────────────────────────────────────────────
def _cleanup_pidfile():
    try:
        if os.path.exists(PID_FILE):
            os.remove(PID_FILE)
    except Exception:
        pass

# Remove stale PID if dead
if os.path.exists(PID_FILE):
    try:
        with open(PID_FILE) as f:
            old = f.read().strip()
        if old.isdigit() and not os.path.exists(f"/proc/{old}"):
            os.remove(PID_FILE)
    except Exception:
        pass

# Write our PID
try:
    with open(PID_FILE, "w") as f:
        f.write(str(os.getpid()))
except Exception as e:
    print(f"[PID ERROR] {e}", file=sys.stderr)
    sys.exit(1)

atexit.register(_cleanup_pidfile)

# ── Logging Setup ─────────────────────────────────────────────────────
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] %(levelname)s: %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S',
    handlers=[
        logging.FileHandler(LOG_FILE),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# ── Email Function ────────────────────────────────────────────────────
def send_email(subject: str, status: str, title: str, message: str) -> str:
    importlib.reload(marcus)
    if not bool(getattr(marcus, "ALERT_EMAIL_ENABLED", False)):
        return "disabled"
    if str(getattr(marcus, "ALERT_EMAIL_ENCRYPT", "SSL")).upper() != "SSL":
        return "Simple Mail Transfer Protocol not established. No conn."

    host = getattr(marcus, "ALERT_EMAIL_SMTP_SERVER", None)
    port = getattr(marcus, "ALERT_EMAIL_SMTP_PORT", None)
    recipient = getattr(marcus, "ALERT_EMAIL_RECIPIENT", None)

    USERCODE = "EDI"  # hardcode per file

    # ---- Edit Sender Info (per file) ----
    user = os.getenv(f"{USERCODE}_USR")
    pwd = os.getenv(f"{USERCODE}_PWD")
    sender_email = user
    sender_name = os.getenv(f"{USERCODE}_NAME")
    # -------------------------------------

    # status color map
    STATUS_COLORS = {
        "STATCON3": "#F1C232",
        "STATCON2": "#E69138",
        "STATCON1": "#CC0000",
        "SIGCON1": "#FB6D8B",
        "OPSCON5": "#F5F5F5",
        "OPSCON1": "#990000",
    }
    status_text = str(status).upper()
    status_color = STATUS_COLORS.get(status_text, "#BE644C")

    msg = EmailMessage()
    domain = sender_email.split("@")[1] if "@" in sender_email else "hodlcorp.io"
    msg_id = f"<{uuid.uuid4()}@{domain}>"
    msg["Message-ID"] = msg_id
    msg["From"] = formataddr((sender_name, sender_email))
    msg["To"] = recipient
    msg["Subject"] = subject
    msg["X-Priority"] = "1"
    msg["X-MSMail-Priority"] = "High"
    msg["Importance"] = "High"

    # footer fields
    now_tz = datetime.now(ZoneInfo("America/Toronto"))
    sent_str = now_tz.strftime("%Y-%m-%d %H:%M:%S America/Toronto")
    epoch_ms = int(now_tz.timestamp() * 1000)
    mid_clean = msg_id.strip("<>").split("@", 1)[0]

    # full HTML body (single block)
    html_body = f"""
<div style="font-family: monospace;">
  <table role="presentation" width="100%" height="20px" cellpadding="8px" cellspacing="0" border="0">
    <!-- Top Banner -->
    <tr style="font-family: Georgia, 'Times New Roman', Times, serif;font-size:20px;font-weight:600;background-color:#333;">
      <td align="left" style="color:#EFEFEF;letter-spacing:12px;">INTCOMM</td>
      <td align="right" style="color:{status_color};letter-spacing:4px;">{status_text}</td>
    </tr>

    <!-- Message Title -->
    <tr width="100%" cellpadding="6px" style="font-family: Tahoma, Geneva, sans-serif;text-align:left;font-size:14px;font-weight:600;color:#333;">
      <td colspan="2">
        {title}
      </td>
    </tr>

    <!-- Message Content -->
    <tr width="100%" cellpadding="6px" style="font-family: Tahoma, Geneva, sans-serif;text-align:left;font-size:11px;font-weight:400;line-height:1.5;color:#333;">
      <td colspan="2">
        {message}
      </td>
    </tr>

    <!-- UNUSED SPACER ROW -->
    <tr width="100%" height="25px"><td colspan="2">&nbsp;</td></tr>
  </table>

  <!-- Footer -->
  <table role="presentation" width="400px" height="20px" cellpadding="4" cellspacing="0" border="0" style="font-family: Tahoma, Geneva, sans-serif;">
    <!-- DOCINT -->
    <tr style="background-color:#333;">
      <td colspan="2" style="color:#efefef;font-size:12px;font-weight:600;">DOCINT</td>
    </tr>

    <tr style="background-color:#E9E9E5;">
      <td width="30px" style="color:#333;font-size:10px;font-weight:600;">SENT</td>
      <td width="10px" style="color:#333;font-size:10px;font-weight:600;">&rarr;</td>
      <td style="color:#333;font-size:11px;font-weight:400;">{sent_str}</td>
    </tr>

    <tr style="background-color:#F2F2F0;">
      <td width="30px" style="color:#333;font-size:10px;font-weight:600;">EPOCH</td>
      <td width="10px" style="color:#333;font-size:10px;font-weight:600;">&rarr;</td>
      <td style="color:#333;font-size:11px;font-weight:400;">{epoch_ms} (ms since 1970/01/01 0:00 UTC)</td>
    </tr>

    <tr style="background-color:#E9E9E5;">
      <td width="30px" style="color:#333;font-size:10px;font-weight:600;">m.ID</td>
      <td width="10px" style="color:#333;font-size:10px;font-weight:600;">&rarr;</td>
      <td style="color:#333;font-size:11px;font-weight:400;">{mid_clean}</td>
    </tr>
  </table>
</div>
"""

    msg.add_alternative(html_body, subtype="html")

    ctx = ssl.create_default_context()
    with smtplib.SMTP_SSL(host, port, context=ctx, timeout=10) as s:
        if user and pwd:
            s.login(user, pwd)
        s.send_message(msg)

    return msg_id

# ── Database Connection ───────────────────────────────────────────────
def get_db_connection():
    """Create and return a PostgreSQL connection"""
    return psycopg2.connect(
        dbname="ariadne",
        user="postgres",
        host="localhost"
    )

# ── Partition Management Functions ────────────────────────────────────
def create_future_partitions(cur, hours_ahead: int = 3) -> int:
    """Create partitions for the next N hours"""
    created_count = 0
    now = datetime.utcnow()
    
    for h in range(1, hours_ahead + 1):
        target_hour = (now + timedelta(hours=h)).replace(minute=0, second=0, microsecond=0)
        hour_after = target_hour + timedelta(hours=1)
        
        start_ts = int(target_hour.timestamp())
        end_ts = int(hour_after.timestamp())
        
        partition_name = f"tickstick_{target_hour.strftime('%Y_%m_%d_%H')}"
        
        try:
            cur.execute(f"""
                CREATE TABLE IF NOT EXISTS {partition_name}
                PARTITION OF tickstick
                FOR VALUES FROM ({start_ts}) TO ({end_ts})
            """)
            created_count += 1
            logger.info(f"[CREATE] Created partition {partition_name}")
            
        except psycopg2.errors.DuplicateTable:
            logger.debug(f"[EXISTS] Partition {partition_name} already exists")
        except Exception as e:
            logger.error(f"[ERROR] Failed to create partition {partition_name}: {e}")
    
    return created_count

def drop_old_partitions(cur) -> int:
    """Drop partitions older than 1 hour"""
    dropped_count = 0
    cutoff_time = datetime.utcnow() - timedelta(hours=1)
    
    cur.execute("""
        SELECT 
            child.relname AS partition_name,
            pg_get_expr(child.relpartbound, child.oid) AS partition_range
        FROM pg_inherits
        JOIN pg_class parent ON pg_inherits.inhparent = parent.oid
        JOIN pg_class child ON pg_inherits.inhrelid = child.oid
        WHERE parent.relname = 'tickstick'
        ORDER BY child.relname
    """)
    
    partitions = cur.fetchall()
    
    for partition_name, partition_range in partitions:
        try:
            to_str = partition_range.split('TO (')[1].split(')')[0].strip("'")
            to_ts = int(to_str)
            to_datetime = datetime.utcfromtimestamp(to_ts)
            
            if to_datetime < cutoff_time:
                cur.execute(f"DROP TABLE IF EXISTS {partition_name}")
                dropped_count += 1
                logger.info(f"[DROP] Dropped old partition {partition_name}")
                
        except Exception as e:
            logger.error(f"[ERROR] Failed to parse partition {partition_name}: {e}")
    
    return dropped_count

# ── Proposals Sweeper ─────────────────────────────────────────────────
def sweep_expired_proposals(cur, age_minutes: int = 10) -> int:
    """Mark proposals older than age_minutes and still pending as expired"""
    try:
        cur.execute(f"""
            UPDATE proposals
               SET status = 'expired',
                   decision_stamp = NOW(),
                   decision_notes = COALESCE(decision_notes, '') || 
                                    CASE WHEN decision_notes IS NULL OR decision_notes = '' 
                                         THEN 'auto-expired by housekeeping' 
                                         ELSE ' | auto-expired by housekeeping' 
                                    END
             WHERE status = 'pending'
               AND created_at < NOW() - INTERVAL '{age_minutes} minutes'
        """)
        updated = cur.rowcount or 0
        if updated > 0:
            logger.info(f"[SWEEP] Proposals expired (>{age_minutes}m): {updated}")
        return updated
    except Exception as e:
        logger.error(f"[ERROR] Proposal sweep failed: {e}")
        return 0

# ── Table Bootstrap ───────────────────────────────────────────────────
def ensure_table_structure(conn) -> None:
    """Ensure the tickstick table exists with proper partitioning"""
    cur = conn.cursor()
    
    try:
        cur.execute("""
            SELECT EXISTS (
                SELECT FROM pg_tables 
                WHERE schemaname = 'public' 
                AND tablename = 'tickstick'
            )
        """)
        
        if not cur.fetchone()[0]:
            logger.info("[INIT] Creating partitioned tickstick table...")
            cur.execute("""
                CREATE TABLE tickstick (
                    id SERIAL,
                    timestamp BIGINT NOT NULL,
                    symbol TEXT NOT NULL,
                    symbol_name TEXT,
                    buy NUMERIC(20,8),
                    sell NUMERIC(20,8),
                    last NUMERIC(20,8),
                    best_bid_size NUMERIC(20,8),
                    best_ask_size NUMERIC(20,8),
                    change_rate NUMERIC(10,6),
                    change_price NUMERIC(20,8),
                    high NUMERIC(20,8),
                    low NUMERIC(20,8),
                    vol NUMERIC(20,8),
                    vol_value NUMERIC(20,8),
                    average_price NUMERIC(20,8),
                    taker_fee_rate NUMERIC(10,8),
                    maker_fee_rate NUMERIC(10,8),
                    taker_coefficient NUMERIC(10,8),
                    maker_coefficient NUMERIC(10,8)
                ) PARTITION BY RANGE (timestamp);
                
                CREATE INDEX idx_tickstick_latest ON tickstick(symbol, timestamp DESC);
            """)
            conn.commit()
            logger.info("[INIT] Created partitioned tickstick table")
            
    except Exception as e:
        logger.error(f"[ERROR] Failed to ensure table structure: {e}")
        conn.rollback()
    finally:
        cur.close()

# ── Main Loop ─────────────────────────────────────────────────────────
def main():
    """Main execution loop"""
    logger.info("="*70)
    logger.info("[INIT] EDITH starting as persistent background process")
    
    conn = None
    cycle_count = 0
    
    try:
        # Initial connection
        conn = get_db_connection()
        logger.info("[INIT] Database connection established")
        
        # Ensure table exists
        ensure_table_structure(conn)
        
        # Main loop
        while not shutdown_requested:
            start_time = time.time()
            cycle_count += 1
            
            try:
                cur = conn.cursor()
                
                # Sweep proposals every cycle (10 minutes)
                sweep_expired_proposals(cur, age_minutes=10)
                
                # Partition management every 6 cycles (1 hour)
                if cycle_count % PARTITION_CYCLES == 1 or cycle_count == 1:
                    logger.info(f"[PARTITION] Running partition management (cycle {cycle_count})")
                    
                    # Create future partitions
                    created = create_future_partitions(cur, hours_ahead=FUTURE_HOURS)
                    if created > 0:
                        logger.info(f"[PARTITION] Created {created} new partitions")
                    
                    # Drop old partitions
                    dropped = drop_old_partitions(cur)
                    if dropped > 0:
                        logger.info(f"[PARTITION] Dropped {dropped} old partitions")
                
                # Commit changes
                conn.commit()
                cur.close()
                
                # Update heartbeat every cycle
                update_heartbeat("edith", conn)
                
            except psycopg2.OperationalError as e:
                logger.error(f"[DB FATAL] Database connection lost: {e}")
                try:
                    send_email(
                        subject="[ STATCON1 ] Edith executed a corrective exit",
                        status="STATCON1",
                        title="Database Connection Error Triggering Exit",
                        message=f"<p><b>Edith lost database connection:</b><br><i>{e}</i></p><p>Monit should restart Edith.</p>"
                    )
                except:
                    pass
                sys.exit(1)
                
            except psycopg2.InterfaceError as e:
                logger.error(f"[DB FATAL] Database interface error: {e}")
                try:
                    send_email(
                        subject="[ STATCON1 ] Edith executed a corrective exit",
                        status="STATCON1",
                        title="Database Interface Error Triggering Exit",
                        message=f"<p><b>Database interface error:</b><br><i>{e}</i></p><p>Monit should restart Edith.</p>"
                    )
                except:
                    pass
                sys.exit(1)
                
            except Exception as e:
                error_str = str(e)
                if "connection already closed" in error_str:
                    logger.info("[DB RECONNECT] Connection closed, attempting reconnect")
                    try:
                        conn.close()
                    except:
                        pass
                    
                    try:
                        conn = get_db_connection()
                        logger.info("[DB RECONNECT] Successfully reconnected")
                    except Exception as reconnect_error:
                        logger.error(f"[DB FATAL] Reconnection failed: {reconnect_error}")
                        try:
                            send_email(
                                subject="[ STATCON1 ] Edith executed a corrective exit",
                                status="STATCON1",
                                title="Database Reconnection Failed",
                                message=f"<p><b>Failed to reconnect after connection loss:</b><br><i>{reconnect_error}</i></p>"
                            )
                        except:
                            pass
                        sys.exit(1)
                else:
                    logger.error(f"[ERROR] Cycle error: {e}")
            
            # Calculate sleep time for accurate 10-minute cycles
            elapsed = time.time() - start_time
            sleep_time = max(0, CYCLE_INTERVAL - elapsed)
            
            if sleep_time > 0 and not shutdown_requested:
                logger.debug(f"[CYCLE] Completed in {elapsed:.2f}s, sleeping {sleep_time:.2f}s")
                time.sleep(sleep_time)
        
        logger.info("[SHUTDOWN] Edith shutting down gracefully")
        
    except Exception as e:
        logger.error(f"[FATAL] {e}")
        sys.exit(1)
    finally:
        _cleanup_pidfile()
        if conn:
            try:
                conn.close()
            except:
                pass

if __name__ == "__main__":
    main()

================================================================================
FILE: mm/utils/partition_manager/edith.pid
================================================================================
3370985

================================================================================
FILE: mm/utils/seldon_engine/signals.py
================================================================================
#>> A R I A N D E v6
#>> last update: 2025 | Sept. 4
#>>
#>> Quorra Signal Functions
#>> mm/utils/seldon_engine/signals.py
#>>
#>> Aggregated methods for measuring specific market dynamics.
#>> Outputs normalized scores (0–1) with breakdown for detailed insight.
#>>
#>> [520] [741] [8]
#>>────────────────────────────────────────────────────────────────

# Build|20250904.01

from random import uniform

def trend_strength(symbol: str) -> dict:
    # Dummy example with submetrics
    sma_cross = uniform(0, 1)
    ema_slope = uniform(0, 1)
    final = round((sma_cross + ema_slope) / 2, 2)
    return {
        "final": final,
        "sma_cross": round(sma_cross, 2),
        "ema_slope": round(ema_slope, 2)
    }

def volatility_status(symbol: str) -> dict:
    # Dummy example
    boll_width = uniform(0, 1)
    atr = uniform(0, 1)
    final = round((boll_width + atr) / 2, 2)
    return {
        "final": final,
        "bollinger": round(boll_width, 2),
        "atr": round(atr, 2)
    }

def pattern_alignment(symbol: str) -> dict:
    # Dummy example
    hns = uniform(0, 1)
    wedges = uniform(0, 1)
    final = round((hns + wedges) / 2, 2)
    return {
        "final": final,
        "head_and_shoulders": round(hns, 2),
        "wedges": round(wedges, 2)
    }

def momentum_confidence(symbol: str) -> dict:
    # Dummy example
    macd = uniform(0, 1)
    rsi = uniform(0, 1)
    final = round((macd + rsi) / 2, 2)
    return {
        "final": final,
        "macd": round(macd, 2),
        "rsi": round(rsi, 2)
    }

def orderbook_balance(symbol: str) -> dict:
    # Dummy example
    bid_ask_ratio = uniform(0, 1)
    depth_imbalance = uniform(0, 1)
    final = round((bid_ask_ratio + depth_imbalance) / 2, 2)
    return {
        "final": final,
        "bid_ask": round(bid_ask_ratio, 2),
        "depth_imbalance": round(depth_imbalance, 2)
    }

def volume_surge(symbol: str) -> dict:
    # Dummy example
    recent_vs_avg = uniform(0, 1)
    sudden_spike = uniform(0, 1)
    final = round((recent_vs_avg + sudden_spike) / 2, 2)
    return {
        "final": final,
        "recent_avg": round(recent_vs_avg, 2),
        "spike": round(sudden_spike, 2)
    }

def candle_alerts(symbol: str) -> dict:
    # Dummy example
    doji = uniform(0, 1)
    engulfing = uniform(0, 1)
    final = round((doji + engulfing) / 2, 2)
    return {
        "final": final,
        "doji": round(doji, 2),
        "engulfing": round(engulfing, 2)
    }

def onchain_insight(symbol: str) -> dict:
    # Placeholder for now
    return {
        "final": 0.5,
        "whale_activity": 0.5,
        "netflow": 0.5,
        "dormant_wakeups": 0.5
    }

================================================================================
FILE: mm/utils/seldon_engine/quorra.py
================================================================================
#===================================================================
# 🍁 A R I A N D E           bot version 6.1 file build 20250904.01
#===================================================================
# last update: 2025 | Sept. 4                   Production ready ❌
#===================================================================
# Quorra (The Seldon Engine)
# mm/utils/seldon_engine/quorra.pyy
#
# This is an intuitive engine, not an oracle. It does not predict 
# price. It assists in gauging relative risk/reward across 
# multiple technical, market, and soon, on-chain dimensions. 
#
# [520] [741] [8]
#===================================================================
# 🔰 THE COMMANDER            ✖ PERSISTANT RUNTIME  ✖ MONIT MANAGED
#===================================================================

# 🔸 Standard Library Imports ======================================

import logging
import json
from mm.utils.seldon_engine import signals
from mm.utils.seldon_engine.onchain import get_onchain_score
from mm.utils.seldon_engine.seldon_config import LOGGING, SCORING_PATH, WEIGHTS
from mm.utils.helpers import inara

# 🔸 Application Imports ===========================================

logger = logging.getLogger("Quorra") 

class SigmaOps:
    def __init__(self):
        self.weights = WEIGHTS
        self.mode = inara.get_mode()
        self.client = inara.get_trading_client()

    def intuit(self, base: str, quote: str) -> float:
        symbol = f"{base.upper()}-{quote.upper()}"
        mode = get_mode()

        # 🔹 Get on-chain score and unpack breakdown ===============
        
        onchain_result = get_onchain_score(symbol)
        onchain_total = onchain_result.get("onchain_score", 0)

        # 🔹 Gather signal results (dicts with subscores + final) ==
        
        signals_data = {
            "trend": signals.trend_strength(symbol),
            "volatility": signals.volatility_status(symbol),
            "patterns": signals.pattern_alignment(symbol),
            "momentum": signals.momentum_confidence(symbol),
            "orderbook": signals.orderbook_balance(symbol),
            "volume_spike": signals.volume_surge(symbol),
            "candlestick": signals.candle_alerts(symbol),
        }

        # 🔹 Attach onchain as dict ================================
        signals_data["onchain"] = {
            **onchain_result,
            "final": onchain_total
        }

        # 🔹 Compute weighted score using .get("final") values =====
        weighted_score = 0
        for key, val in signals_data.items():
            score = val.get("final", 0)
            weighted_score += score * self.weights.get(key, 1)

        score = max(0, min(100, round(weighted_score)))

        if LOGGING:
            self._log_debug(symbol, signals_data, score)

        return score

    def _log_debug(self, symbol: str, breakdowns: dict, total_score: int):
        data = {
            "symbol": symbol,
            "mode": get_mode(),
            "score": total_score,
            "details": breakdowns
        }
        try:
            with open(SCORING_PATH, "a") as f:
                f.write(json.dumps(data) + "\n")
        except Exception as e:
            logger.warning(f"Quorra logging error: {e}")


================================================================================
FILE: mm/utils/seldon_engine/lamar.py
================================================================================
#===================================================================
# 🍁 A R I A N D E           bot version 6.1 file build 20250909.01
#===================================================================
# last update: 2025 | Sept. 9                   Production ready ❌
#===================================================================
# Lamar - Proposal Approvals Orchestrator
# mm/utils/seldon_engine/lamar.py
#
# Listens to Postgres and notifies managers.
#
# [520] [741] [8]
#===================================================================
# 🔰 THE COMMANDER            ✔ PERSISTANT RUNTIME  ✔ MONIT MANAGED
#===================================================================

import json
import select
import logging
from psycopg2.extras import DictCursor
from mm.utils.helpers.wintermute import get_db_connection
from mm.utils.helpers.inara import get_mode

logger = logging.getLogger(__name__)

# Incoming channels
CHAN_VET = "proposals_vet_changed"
CHAN_EXPIRED = "proposals_expired"
CHAN_HOLD_CREATED = "holds_created"  # Julius/Helen notify back when hold is ready

# Outgoing channels
CHAN_CREATE_HOLD_J = "holds_create_julius"
CHAN_CREATE_HOLD_H = "holds_create_helen"
CHAN_READY_M = "proposals_ready_malcolm"
CHAN_READY_P = "proposals_ready_petra"
CHAN_DENIED_M = "proposals_denied_malcolm"
CHAN_DENIED_P = "proposals_denied_petra"

class SigInt: 
    def __init__(self):
        self.listen_conn = None
        self.oper_conn = get_db_connection()
        self.pending_holds = {}  # prop_id -> creator mapping while waiting for hold

    def _open_listen(self):
        self.listen_conn = get_db_connection(autocommit=True)
        cur = self.listen_conn.cursor()
        cur.execute(f"LISTEN {CHAN_VET};")
        cur.execute(f"LISTEN {CHAN_EXPIRED};")
        cur.execute(f"LISTEN {CHAN_HOLD_CREATED};")
        cur.close()
        logger.info(f"[LISTEN] Channels: {CHAN_VET}, {CHAN_EXPIRED}, {CHAN_HOLD_CREATED}")

    def _notify(self, channel, payload):
        cur = self.oper_conn.cursor()
        cur.execute(f"NOTIFY {channel}, %s;", (json.dumps(payload),))
        self.oper_conn.commit()
        cur.close()

    def _log_routing(self, proposal_id, routed_to, notes=None):
        cur = self.oper_conn.cursor()
        cur.execute(
            """
            INSERT INTO proposal_router_log (proposal_id, routed_to, notes)
            VALUES (%s, %s, %s)
            ON CONFLICT (proposal_id, routed_to) DO NOTHING;
            """,
            (proposal_id, routed_to, notes),
        )
        self.oper_conn.commit()
        cur.close()

    def _fetch_proposal(self, prop_id):
        cur = self.oper_conn.cursor(cursor_factory=DictCursor)
        cur.execute("SELECT * FROM proposals WHERE prop_id = %s;", (prop_id,))
        row = cur.fetchone()
        cur.close()
        return dict(row) if row else None

    def _handle_vet_update(self, prop_id):
        """Handle when any vet field changes"""
        proposal = self._fetch_proposal(prop_id)
        if not proposal:
            logger.warning(f"Proposal {prop_id} not found.")
            return

        creator = proposal["creator"]
        
        # Check for denials
        for vet_type, manager in [("risk_vet", "grayson"), ("invt_vet", "helen"), ("bank_vet", "julius")]:
            if proposal.get(vet_type) == "denied":
                channel = CHAN_DENIED_M if creator == "Malcolm" else CHAN_DENIED_P
                self._notify(channel, {"proposal_id": prop_id, "denied_by": manager})
                self._log_routing(prop_id, "originator", notes=f"denied by {manager}")
                return

        # Check if all approved
        if all(proposal.get(field) == "approved" for field in ["risk_vet", "invt_vet", "bank_vet"]):
            mode = get_mode()
            
            if mode == "simulation":
                # In simulation, request hold creation from appropriate manager
                if creator == "Malcolm":
                    self._notify(CHAN_CREATE_HOLD_J, {"proposal_id": prop_id})
                    self.pending_holds[prop_id] = creator
                    self._log_routing(prop_id, "julius", notes="requesting hold creation")
                elif creator == "Petra":
                    self._notify(CHAN_CREATE_HOLD_H, {"proposal_id": prop_id})
                    self.pending_holds[prop_id] = creator
                    self._log_routing(prop_id, "helen", notes="requesting hold creation")
            else:
                # In live mode, holds are created differently (handled by managers directly)
                # Just mark as ready immediately
                self._mark_ready(prop_id, creator)

    def _handle_hold_created(self, prop_id):
        """Handle when Julius/Helen confirm hold is created"""
        creator = self.pending_holds.pop(prop_id, None)
        if not creator:
            logger.warning(f"Unexpected hold created for {prop_id}")
            return
        
        # Now that hold is created, mark as ready
        self._mark_ready(prop_id, creator)

    def _mark_ready(self, prop_id, creator):
        """Send ready signal to originator"""
        # Update proposal status
        cur = self.oper_conn.cursor()
        cur.execute(
            "UPDATE proposals SET status = 'approved' WHERE prop_id = %s;",
            (prop_id,)
        )
        self.oper_conn.commit()
        cur.close()
        
        # Notify originator
        channel = CHAN_READY_M if creator == "Malcolm" else CHAN_READY_P
        self._notify(channel, {"proposal_id": prop_id})
        self._log_routing(prop_id, "originator", notes="fully approved and ready")

    def _handle_expiry(self, prop_id):
        """Handle expired proposals"""
        proposal = self._fetch_proposal(prop_id)
        if not proposal:
            return

        creator = proposal["creator"]
        channel = CHAN_DENIED_M if creator == "Malcolm" else CHAN_DENIED_P
        self._notify(channel, {"proposal_id": prop_id, "expired": True})
        self._log_routing(prop_id, "originator", notes="expired")
        
        # Clean up any pending hold request
        self.pending_holds.pop(prop_id, None)

    def run(self):
        self._open_listen()
        logger.info("[LAMAR] Routing loop started.")

        while True:
            if select.select([self.listen_conn], [], [], 5) == ([], [], []):
                continue

            self.listen_conn.poll()
            while self.listen_conn.notifies:
                notify = self.listen_conn.notifies.pop(0)
                try:
                    payload = json.loads(notify.payload)
                    prop_id = payload.get("proposal_id")
                    
                    if not prop_id:
                        continue

                    if notify.channel == CHAN_VET:
                        self._handle_vet_update(prop_id)
                    elif notify.channel == CHAN_EXPIRED:
                        self._handle_expiry(prop_id)
                    elif notify.channel == CHAN_HOLD_CREATED:
                        self._handle_hold_created(prop_id)
                        
                except Exception as e:
                    logger.error(f"Error processing notification: {e}")

if __name__ == "__main__":
    lamar = Lamar()
    lamar.run()

================================================================================
FILE: mm/utils/seldon_engine/seldon_config.py
================================================================================
#>> A R I A N D E v6
#>> last update: 2025 | Sept. 4
#>>
#>> Congig (The Seldon Engine)
#>> mm/utils/seldon_engine/seldon_config.py
#>>
#>> Centralized weights and logging toggle
#>>
#>> Auth'd -> Commander
#>>
#>> [520] [741] [8]
#>>────────────────────────────────────────────────────────────────

# Build|20250904.01

# ── LOGGING ──────────────────────────────────────────────────────
LOGGING: bool = True
SCORING_PATH: str = "mm/utils/seldon_engine/scoring.json"

# ── SCORE WEIGHTING ──────────────────────────────────────────────
WEIGHTS = {
"trend": 1.0,
"volatility": 1.0,
"patterns": 1.2,
"momentum": 1.1,
"orderbook": 0.9,
"volume_spike": 1.0,
"candlestick": 1.1,
"onchain": 1.0,
}

================================================================================
FILE: mm/utils/seldon_engine/onchain.py
================================================================================
#>> A R I A N D E v6
#>> last update: 2025 | Sept. 4
#>>
#>> On-Chain Signals (The Seldon Engine)
#>> mm/utils/seldon_engine/onchain.py
#>>
#>> On-chain signals
#>> Returns a dict with subscores and final onchain_score
#>>
#>> Auth'd -> Commander
#>>
#>> [520] [741] [8]
#>>────────────────────────────────────────────────────────────────

# Build|20250904.01

import requests
import time
import logging

logger = logging.getLogger("Quorra")

# ── Configurable weights ─────────────────────────────
ONCHAIN_ENDPOINTS = {
    "exchange_flows": "https://api.coinglass.com/api/pro/v1/futures/open_interest_chart?symbol={symbol}",
    "whale_alerts": "https://api.whale-alert.io/v1/transactions?api_key=YOUR_KEY&min_value=1000000&currency={symbol}",
    "gas_tracker": "https://api.etherscan.io/api?module=gastracker&action=gasoracle&apikey=YOUR_KEY",
    "dex_liquidity": "https://api.geckoterminal.com/api/v2/networks/eth/pools/{pool_id}",
    "dormant_wallets": "https://api.glassnode.com/v1/metrics/addresses/supply_last_active_more_1y_percent?api_key=YOUR_KEY&asset={symbol}"
}


# ── Core Scoring Functions ──────────────────────────

def score_exchange_flows(symbol: str) -> int:
    try:
        url = ONCHAIN_ENDPOINTS["exchange_flows"].format(symbol=symbol.upper())
        r = requests.get(url)
        data = r.json()
        # Simulate a score: large inflows are bad (0–40), large outflows good (60–100)
        flow_delta = data.get("data", {}).get("netInflow", 0)
        if flow_delta > 0:
            return max(0, 40 - int(flow_delta / 10))
        else:
            return min(100, 60 + abs(int(flow_delta / 10)))
    except Exception as e:
        logger.warning(f"exchange_flows error: {e}")
        return 50

def score_whale_activity(symbol: str) -> int:
    try:
        url = ONCHAIN_ENDPOINTS["whale_alerts"].format(symbol=symbol.upper())
        r = requests.get(url)
        data = r.json()
        count = len(data.get("transactions", []))
        if count > 10:
            return 20  # lots of whale action
        elif count > 5:
            return 40
        else:
            return 70
    except Exception as e:
        logger.warning(f"whale_activity error: {e}")
        return 50

def score_gas_mempool() -> int:
    try:
        r = requests.get(ONCHAIN_ENDPOINTS["gas_tracker"])
        data = r.json().get("result", {})
        fast = int(data.get("FastGasPrice", 50))
        # Lower gas = more stable, higher = congestion = bearish
        if fast > 80:
            return 30
        elif fast > 60:
            return 50
        else:
            return 75
    except Exception as e:
        logger.warning(f"gas_tracker error: {e}")
        return 50

def score_dormant_wallets(symbol: str) -> int:
    try:
        url = ONCHAIN_ENDPOINTS["dormant_wallets"].format(symbol=symbol.upper())
        r = requests.get(url)
        pct = float(r.json()["data"][-1][1])
        # If too many old wallets are waking up — bearish
        if pct < 30:
            return 75
        elif pct < 40:
            return 50
        else:
            return 25
    except Exception as e:
        logger.warning(f"dormant_wallets error: {e}")
        return 50


# Placeholder for DEX liquidity — depends on platform integration

def score_dex_liquidity(pool_id: str = "") -> int:
    try:
        if not pool_id:
            return 50  # neutral if no context
        url = ONCHAIN_ENDPOINTS["dex_liquidity"].format(pool_id=pool_id)
        r = requests.get(url)
        tvl = float(r.json()["data"]["attributes"]["reserve_in_usd"])
        if tvl > 10_000_000:
            return 80
        elif tvl > 5_000_000:
            return 60
        else:
            return 40
    except Exception as e:
        logger.warning(f"dex_liquidity error: {e}")
        return 50


def get_onchain_score(symbol: str, pool_id: str = "") -> int:
    scores = {
        "exchange_flows": score_exchange_flows(symbol),
        "whale_activity": score_whale_activity(symbol),
        "gas_mempool": score_gas_mempool(),
        "dormant_wallets": score_dormant_wallets(symbol),
        "dex_liquidity": score_dex_liquidity(pool_id),
    }
    total = round(sum(scores.values()) / len(scores))
    scores["onchain_score"] = total
    return scores

================================================================================
FILE: mm/utils/tools/dashboard.py
================================================================================
#>> A R I A N D E v6
#>> last update: 2025 | Sept. 5
#>>
#>> SIMDASH - CLI Interface
#>> mm/utils/tools/dashboard.py
#>>
#>> Ariadne Dashboard - Terminal UI for Market Maker Bot
#>> Simulation mode monitoring 
#>> Press [ X ] to exit the dashboard.
#>>
#>> Auth'd -> Commander
#>>
#>> [520] [741] [8]      
#>>────────────────────────────────────────────────────────────────

# Build|20250905.02

import os
import sys
import sqlite3
import json
import time
from datetime import datetime, timedelta
from tabulate import tabulate
import pytz

# Database paths
SIM_DB_PATH = "/root/Echelon/valentrix/mm/data/sims/ariadne_sim.db"
TICK_DB_PATH = "/root/Echelon/valentrix/mm/data/live/tick_snapshots.db"
STATE_FILE_PATH = "/root/Echelon/valentrix/mm/data/state/sim_state.json"
LOG_FILE_PATH = "/root/Echelon/valentrix/mm/logs/ariadne.log"

# Color codes for terminal
RESET = "\033[0m"
RED = "\033[91m"
GREEN = "\033[92m"
YELLOW = "\033[93m"
BLUE = "\033[94m"
CYAN = "\033[96m"
WHITE = "\033[97m"
BOLD = "\033[1m"
DIM = "\033[2m"

TZ_TORONTO = pytz.timezone("America/Toronto")

class AriadneDashboard:
   def __init__(self):
       self.sim_db = SIM_DB_PATH
       self.tick_db = TICK_DB_PATH
       self.state_file = STATE_FILE_PATH
       self.log_file = LOG_FILE_PATH
       self.refresh_interval = 5
       self.countdown = self.refresh_interval
       
       # Load start time once and keep it
       state = self._load_state()
       self.start_time = state.get('start_time', time.time())
       
   def clear_screen(self):
       os.system('cls' if os.name == 'nt' else 'clear')
   
   def run(self):
       """Main dashboard loop"""
       while True:
           try:
               self.countdown = self.refresh_interval
               self.display()
               
               for i in range(self.refresh_interval):
                   time.sleep(1)
                   self.countdown -= 1
                   
           except KeyboardInterrupt:
               break
           except Exception as e:
               print(f"Dashboard error: {e}")
               time.sleep(5)
   
   def display(self):
       """Render the complete dashboard"""
       self.clear_screen()
       
       # Get all data
       state = self._load_state()
       balances = self._get_balances()
       orders = self._get_orders()
       fills = self._get_fills()
       metrics = self._calculate_metrics(state, fills)
       
       # Header
       print(f"\n{'='*120}")
       print(f"{'ARIADNE v4.4 - Market Maker Dashboard':^120}")
       print(f"{'='*120}\n")
       
       # Top row with proper runtime
       runtime = self._format_runtime()
       total_exposure = balances.get('hold', 0.0)
       
       print(f"RUNTIME: {runtime:<20}    Next refresh in {self.countdown}s    TOTAL EXPOSURE: ${total_exposure:,.2f} USDT")
       print(f"{'-'*120}\n")
       
       # Main sections with fixed column widths
       print(f"{'ACCOUNT SUMMARY':<24}{'ORDER SUMMARY':<24}{'P&L SUMMARY':<24}{'METRICS':<24}{'CURRENT PARAMS':<24}")
       print(f"{'-'*23} {'-'*23} {'-'*23} {'-'*23} {'-'*23}")
       
       # Row 1
       print(f"{'Available USDT':<15}${balances.get('available', 0):>7.2f} ", end="")
       print(f"{'Unfilled Orders':<15}{len([o for o in orders if not self._is_filled(o[0], fills)]):>8} ", end="")
       print(f"{'Gross Profit':<15}${metrics['gross_profit']:>7.2f} ", end="")
       print(f"{'Avg Order Time':<15}{metrics['avg_order_time']:>8} ", end="")
       print(f"{'Target Spread':<15}{'1.0%':>8}")
       
       # Row 2
       print(f"{'Held (unfilled)':<15}${balances.get('hold', 0):>7.2f} ", end="")
       print(f"{'Filled Orders':<15}{len(fills):>8} ", end="")
       print(f"{'Fees':<15}${metrics['total_fees']:>7.2f} ", end="")
       print(f"{'Avg Spread Capture':<15}{metrics['avg_spread']:>7.2f}% ", end="")
       print(f"{'Min Volume':<15}{'$5M':>8}")
       
       # Row 3
       print(f"{'Committed (filled)':<15}${balances.get('committed', 0):>7.2f} ", end="")
       print(f"{'Total Current Orders':<15}{len(orders):>8} ", end="")
       print(f"{'Net Profit':<15}${metrics['net_profit']:>7.2f} ", end="")
       print(f"{'Fill Rate':<15}{metrics['fill_rate']:>7.2f}% ", end="")
       print(f"{'Max Volume':<15}{'$200M':>8}")
       
       # Row 4
       print(f"{'Total USDT':<15}${balances.get('total', 0):>7.2f} ", end="")
       print(f"{'':<23} ", end="")
       print(f"{'Current P&L %':<15}{metrics['pnl_pct']:>7.2f}% ", end="")
       print(f"{'':<23} ", end="")
       print(f"{'Daily Target':<15}{'4.45%':>8}")
       
       # Orders section
       print(f"\n{'ORDERS'}")
       print(f"{'-'*120}")
       self._display_orders(orders, fills)
       
       # Console section - fixed height
       print(f"\n{'CONSOLE'}")
       print(f"{'-'*120}")
       self._display_console()
   
   def _display_orders(self, orders, fills):
       """Display unfilled and filled orders tables"""
       
       # UNFILLED
       print(f"\n{'UNFILLED'}")
       unfilled_data = []
       
       for order in orders[:20]:
           order_id, ts, symbol, side, price, size, sim_order_id, _ = order
           
           if not self._is_filled(order_id, fills):
               order_time = datetime.fromtimestamp(ts, TZ_TORONTO).strftime('%H:%M:%S')
               bid, ask = self._get_market_prices(symbol)
               market_price = ask if side.lower() == 'buy' else bid
               
               if market_price and market_price > 0:
                   diff_pct = ((market_price - price) / price * 100)
                   diff_str = f"{diff_pct:+.1f}%"
                   market_str = f"${market_price:.2f}"
               else:
                   diff_str = ""
                   market_str = ""
               
               side_colored = f"{RED}BUY{RESET}" if side.lower() == 'buy' else f"{GREEN}SELL{RESET}"
               
               unfilled_data.append([
                   order_time,
                   symbol,
                   side_colored,
                   f"@ {price:.8f}",
                   f"{size:.6f}",
                   f"${price * size:.2f}",
                   market_str,
                   diff_str
               ])
       
       if unfilled_data:
           headers = ["TIME", "SYMBOL", "SIDE", "ORDER B/A", "AMOUNT", "TOTAL", "MARKET", "DIFF%"]
           print(tabulate(unfilled_data, headers=headers, tablefmt="plain",
                         colalign=("left", "left", "left", "right", "right", "right", "right", "right")))
       else:
           print("(no unfilled orders)")
       
       # FILLED
       print(f"\n{'FILLED'}")
       
       if not fills:
           print("(no filled orders yet)")
       else:
           filled_data = []
           for fill in fills[:10]:
               fill_id, order_id, fill_ts, fill_price, fill_size = fill
               
               order_info = self._get_order_info(order_id)
               if order_info:
                   _, order_ts, symbol, side, order_price, _, _, _ = order_info
                   
                   fill_time = datetime.fromtimestamp(fill_ts, TZ_TORONTO).strftime('%H:%M:%S')
                   side_colored = f"{RED}BUY{RESET}" if side.lower() == 'buy' else f"{GREEN}SELL{RESET}"
                   
                   spread = 0.0
                   if side.lower() == 'buy':
                       spread = ((order_price - fill_price) / order_price * 100)
                   else:
                       spread = ((fill_price - order_price) / order_price * 100)
                   
                   filled_data.append([
                       fill_time,
                       symbol,
                       side_colored,
                       f"@ {order_price:.8f}",
                       f"{fill_size:.6f}",
                       f"${order_price * fill_size:.2f}",
                       f"${fill_price:.2f}",
                       f"{spread:+.1f}%"
                   ])
           
           headers = ["TIME", "SYMBOL", "SIDE", "ORDER B/A", "AMOUNT", "TOTAL", "FILL PRICE", "SPREAD%"]
           print(tabulate(filled_data, headers=headers, tablefmt="plain",
                         colalign=("left", "left", "left", "right", "right", "right", "right", "right")))
   
   def _display_console(self):
       """Display last 10 log lines - fixed height"""
       try:
           with open(self.log_file, 'r') as f:
               lines = f.readlines()
               # Always show exactly 10 lines (pad if needed)
               last_lines = lines[-10:] if len(lines) >= 10 else lines
               
               for line in last_lines:
                   if ' - ' in line:
                       parts = line.split(' - ', 2)
                       if len(parts) >= 3:
                           # Keep original timestamp from log
                           print(f"{parts[0]} {parts[2].strip()[:100]}")  # Limit line length
                   else:
                       print(line.strip()[:120])
               
               # Pad to 10 lines if needed
               for _ in range(10 - len(last_lines)):
                   print("")
       except:
           print("(log file not available)")
           for _ in range(9):
               print("")
   
   def _format_runtime(self):
       """Calculate runtime from stored start time"""
       elapsed = time.time() - self.start_time
       days = int(elapsed // 86400)
       hours = int((elapsed % 86400) // 3600)
       minutes = int((elapsed % 3600) // 60)
       return f"{days}d {hours}h {minutes}m"
   
   def _get_balances(self):
       """Get current balances from database"""
       try:
           conn = sqlite3.connect(self.sim_db, timeout=1)
           c = conn.cursor()
           
           c.execute("SELECT available, hold FROM simulation_balances WHERE currency = 'USDT'")
           row = c.fetchone()
           available = float(row[0]) if row else 0.0
           hold = float(row[1]) if row else 0.0
           
           # Calculate committed from non-USDT assets
           c.execute("SELECT currency, available FROM simulation_balances WHERE currency != 'USDT' AND available > 0")
           committed = 0.0
           for currency, amount in c.fetchall():
               bid, _ = self._get_market_prices(f"{currency}-USDT")
               if bid:
                   committed += amount * bid
           
           conn.close()
           return {
               'available': available,
               'hold': hold,
               'committed': committed,
               'total': available + hold + committed
           }
       except Exception as e:
           return {'available': 0, 'hold': 0, 'committed': 0, 'total': 0}
   
   def _get_orders(self):
       """Get all orders from database"""
       try:
           conn = sqlite3.connect(self.sim_db, timeout=1)
           c = conn.cursor()
           c.execute("SELECT * FROM orders ORDER BY timestamp DESC")
           orders = c.fetchall()
           conn.close()
           return orders
       except:
           return []
   
   def _get_order_info(self, order_id):
       """Get specific order info"""
       try:
           conn = sqlite3.connect(self.sim_db, timeout=1)
           c = conn.cursor()
           c.execute("SELECT * FROM orders WHERE id = ?", (order_id,))
           order = c.fetchone()
           conn.close()
           return order
       except:
           return None
   
   def _get_fills(self):
       """Get all filled orders"""
       try:
           conn = sqlite3.connect(self.sim_db, timeout=1)
           c = conn.cursor()
           c.execute("SELECT * FROM simulated_trades ORDER BY fill_timestamp DESC")
           fills = c.fetchall()
           conn.close()
           return fills
       except:
           return []
   
   def _get_market_prices(self, symbol):
       """Get current bid/ask from tick database"""
       try:
           conn = sqlite3.connect(self.tick_db, timeout=1)
           c = conn.cursor()
           c.execute("SELECT buy, sell FROM tick_snapshots WHERE symbol = ? ORDER BY timestamp DESC LIMIT 1", (symbol,))
           row = c.fetchone()
           conn.close()
           if row:
               return float(row[0]), float(row[1])
           return None, None
       except:
           return None, None
   
   def _is_filled(self, order_id, fills):
       """Check if order has been filled"""
       return any(f[1] == order_id for f in fills)
   
   def _calculate_metrics(self, state, fills):
       """Calculate performance metrics"""
       total_fees = len(fills) * 0.001 * 100
       gross_profit = 0.0
       
       for fill in fills:
           _, order_id, _, fill_price, fill_size = fill
           order_info = self._get_order_info(order_id)
           if order_info:
               _, _, _, side, order_price, _, _, _ = order_info
               if side.lower() == 'sell':
                   gross_profit += (fill_price - order_price) * fill_size
       
       return {
           'gross_profit': gross_profit,
           'total_fees': total_fees,
           'net_profit': gross_profit - total_fees,
           'pnl_pct': ((gross_profit - total_fees) / 2500.0 * 100) if gross_profit > 0 else 0,
           'avg_order_time': "7m 48s",
           'avg_spread': 0.0,
           'fill_rate': (len(fills) / max(len(self._get_orders()), 1) * 100)
       }
   
   def _load_state(self):
       """Load bot state from file"""
       try:
           with open(self.state_file, 'r') as f:
               return json.load(f)
       except:
           return {}

if __name__ == "__main__":
   dashboard = AriadneDashboard()
   dashboard.run()

================================================================================
FILE: mm/utils/tools/clear_override.py
================================================================================
#>> A R I A N D E v6.1
#>> last update: 2025 | Sept. 15
#>>
#>> clear_override.py
#>> mm/utils/tools/clear_override.py
#>>
#>> PURPOSE:
#>>   This is a utility script to clear any runtime override that has been set in Inara.
#>>   When an override is active, Inara returns the overridden mode instead of reading
#>>   the configuration in mm/config/marcus.py.
#>>
#>>   Running this script will reset Inara’s internal mode back to "none".
#>>   On the next call to get_mode(), Inara will re-read marcus.MODE
#>>   and use that as the system’s operational mode.
#>>
#>> USAGE:
#>>   python3 mm/utils/tools/clear_override.py
#>>
#>> NOTES:
#>>   • This does not change marcus.MODE itself.
#>>   • This only clears the override so marcus is respected again.
#>>   • No alert is sent; this script is intended for direct operator use.
#>>
#>> Auth'd -> Commander
#>>
#>>───────────────────────────────────────────────────

# Build|20250915.02

import logging

logger = logging.getLogger("ariadne.clear_override")

def main():
    from mm.utils.helpers import inara

    # Clear LRU cache and reset the override
    inara.get_mode.cache_clear()
    inara._mode = ""
    logger.info("Inara override cleared. Next get_mode() will read marcus.MODE.")
    print("Override cleared. Mode cache reset. marcus.MODE now active.")

if __name__ == "__main__":
    main()

================================================================================
FILE: mm/utils/tools/scripts/colors.py
================================================================================
#!/usr/bin/env python3
"""
Display all ANSI color codes
"""

# Regular colors
colors = {
    'Black': '\033[30m',
    'Red': '\033[31m',
    'Green': '\033[32m',
    'Yellow': '\033[33m',
    'Blue': '\033[34m',
    'Magenta': '\033[35m',
    'Cyan': '\033[36m',
    'White': '\033[37m',
    'Default': '\033[39m',
    
    # Bright colors
    'Bright Black': '\033[90m',
    'Bright Red': '\033[91m',
    'Bright Green': '\033[92m',
    'Bright Yellow': '\033[93m',
    'Bright Blue': '\033[94m',
    'Bright Magenta': '\033[95m',
    'Bright Cyan': '\033[96m',
    'Bright White': '\033[97m',
}

# Background colors
bg_colors = {
    'BG Black': '\033[40m',
    'BG Red': '\033[41m',
    'BG Green': '\033[42m',
    'BG Yellow': '\033[43m',
    'BG Blue': '\033[44m',
    'BG Magenta': '\033[45m',
    'BG Cyan': '\033[46m',
    'BG White': '\033[47m',
    'BG Default': '\033[49m',
    
    # Bright backgrounds
    'BG Bright Black': '\033[100m',
    'BG Bright Red': '\033[101m',
    'BG Bright Green': '\033[102m',
    'BG Bright Yellow': '\033[103m',
    'BG Bright Blue': '\033[104m',
    'BG Bright Magenta': '\033[105m',
    'BG Bright Cyan': '\033[106m',
    'BG Bright White': '\033[107m',
}

# Styles
styles = {
    'Bold': '\033[1m',
    'Dim': '\033[2m',
    'Italic': '\033[3m',
    'Underline': '\033[4m',
    'Blink': '\033[5m',
    'Reverse': '\033[7m',
    'Hidden': '\033[8m',
    'Strikethrough': '\033[9m',
}

RESET = '\033[0m'

print("\nFOREGROUND COLORS")
print("="*50)
for name, code in colors.items():
    print(f"{code}{name:<20} {code} ■■■■■ Sample Text{RESET}")

print("\nBACKGROUND COLORS")
print("="*50)
for name, code in bg_colors.items():
    print(f"{code}{name:<20} {code} ■■■■■ Sample Text{RESET}")

print("\nSTYLES")
print("="*50)
for name, code in styles.items():
    print(f"{code}{name:<20} {code} Sample Text{RESET}")

print("\nCOMBINATIONS")
print("="*50)
print(f"\033[1;32mBold Green{RESET}")
print(f"\033[4;91mUnderlined Bright Red{RESET}")
print(f"\033[42;97mWhite on Green Background{RESET}")
print(f"\033[1;3;33mBold Italic Yellow{RESET}")
print(f"\033[5;35mBlinking Magenta{RESET}")

print("\nUSEFUL CODES")
print("="*50)
print(f"Reset: \\033[0m")
print(f"Your Green: \\033[92m (Bright Green)")
print(f"Your Dark Green: \\033[32m (Green)")
print(f"Your Light Gray: \\033[37m (White/Light Gray)")
print(f"Your Red: \\033[91m (Bright Red)")

================================================================================
FILE: mm/utils/tools/scripts/plaintext.py
================================================================================
#!/usr/bin/env python3
"""
Simple AI reference file generator
Outputs three files to mm/config/plaintext/ with current date
"""

import os
import psycopg2
import psycopg2.extras
from datetime import datetime

def get_current_date():
    return datetime.now().strftime('%Y%m%d')

def generate_file_structure():
    date_str = get_current_date()
    output_file = f"mm/config/plaintext/file_structure_plaintext_{date_str}.txt"
    
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("PROJECT FILE STRUCTURE\n")
        f.write("=" * 50 + "\n")
        f.write(f"Generated: {datetime.now().isoformat()}\n\n")
        
        for root, dirs, files in os.walk("mm"):
            # Remove __pycache__ and .git directories
            dirs[:] = [d for d in dirs if d not in ['__pycache__', '.git']]
            
            level = root.replace("mm", "").count(os.sep)
            
            if level == 0:
                f.write("mm/\n")
            else:
                dirname = os.path.basename(root)
                prefix = "│   " * (level - 1) + "├── "
                f.write(f"{prefix}{dirname}/\n")
            
            # Sort files
            files.sort()
            for i, file in enumerate(files):
                is_last_file = (i == len(files) - 1)
                if level == 0:
                    prefix = "└── " if is_last_file else "├── "
                else:
                    prefix = "│   " * level + ("└── " if is_last_file else "├── ")
                f.write(f"{prefix}{file}\n")

def generate_code_pages():
    date_str = get_current_date()
    output_file = f"mm/config/plaintext/code_pages_plaintext_{date_str}.txt"
    
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("ALL CODE IN PLAINTEXT\n")
        f.write("=" * 50 + "\n")
        f.write(f"Generated: {datetime.now().isoformat()}\n\n")
        
        for root, dirs, files in os.walk("mm"):
            # Skip .git, assets, and __pycache__ directories
            dirs[:] = [d for d in dirs if d not in ['.git', 'assets', '__pycache__']]
            
            for file in files:
                # Skip .log, .txt files and __init__.py files
                if file.endswith('.log') or file.endswith('.txt') or file == '__init__.py':
                    continue
                    
                file_path = os.path.join(root, file)
                
                f.write("\n" + "=" * 80 + "\n")
                f.write(f"FILE: {file_path}\n")
                f.write("=" * 80 + "\n")
                
                try:
                    with open(file_path, 'r') as code_file:
                        content = code_file.read()
                        f.write(content)
                        if not content.endswith('\n'):
                            f.write('\n')
                except:
                    f.write("[COULD NOT READ FILE]\n")

def generate_db_schema():
    date_str = get_current_date()
    output_file = f"mm/config/plaintext/db_schema_plaintext_{date_str}.txt"
    
    dsn = os.getenv("PG_DSN", "dbname=ariadne user=postgres host=localhost")
    conn = psycopg2.connect(dsn)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("DATABASE SCHEMA\n")
        f.write("=" * 50 + "\n")
        f.write(f"Generated: {datetime.now().isoformat()}\n\n")
        
        cur = conn.cursor(cursor_factory=psycopg2.extras.DictCursor)
        
        # Get all tables
        cur.execute("SELECT tablename FROM pg_tables WHERE schemaname = 'public' ORDER BY tablename")
        tables = [row[0] for row in cur.fetchall()]
        
        # Table listing at the beginning
        f.write("TABLE LIST\n")
        f.write("-" * 20 + "\n")
        for i, table_name in enumerate(tables, 1):
            f.write(f"{i:2d}. {table_name}\n")
        f.write(f"\nTotal: {len(tables)} tables\n\n")
        f.write("=" * 60 + "\n\n")
        
        # Individual table details
        for table_name in tables:
            f.write(f"TABLE: {table_name}\n")
            f.write("=" * (len(table_name) + 7) + "\n")
            
            # Columns
            cur.execute("""
                SELECT column_name, data_type, character_maximum_length, is_nullable, column_default
                FROM information_schema.columns
                WHERE table_schema = 'public' AND table_name = %s
                ORDER BY ordinal_position
            """, (table_name,))
            
            columns = cur.fetchall()
            f.write("COLUMNS:\n")
            for col in columns:
                f.write(f"  {col['column_name']:<25} {col['data_type']}")
                if col['character_maximum_length']:
                    f.write(f"({col['character_maximum_length']})")
                if col['is_nullable'] == 'NO':
                    f.write(" NOT NULL")
                if col['column_default']:
                    f.write(f" DEFAULT {col['column_default']}")
                f.write("\n")
            
            # Constraints
            cur.execute("""
                SELECT constraint_name, constraint_type
                FROM information_schema.table_constraints
                WHERE table_schema = 'public' AND table_name = %s
            """, (table_name,))
            
            constraints = cur.fetchall()
            if constraints:
                f.write("\nCONSTRAINTS:\n")
                for const in constraints:
                    f.write(f"  {const['constraint_name']} ({const['constraint_type']})\n")
            
            # Indexes
            cur.execute("""
                SELECT indexname, indexdef
                FROM pg_indexes
                WHERE schemaname = 'public' AND tablename = %s
            """, (table_name,))
            
            indexes = cur.fetchall()
            if indexes:
                f.write("\nINDEXES:\n")
                for idx in indexes:
                    f.write(f"  {idx['indexname']}\n")
                    f.write(f"    {idx['indexdef']}\n")
            
            # Triggers
            cur.execute("""
                SELECT trigger_name, event_manipulation, action_timing, action_statement
                FROM information_schema.triggers
                WHERE event_object_schema = 'public' AND event_object_table = %s
            """, (table_name,))
            
            triggers = cur.fetchall()
            if triggers:
                f.write("\nTRIGGERS:\n")
                for trigger in triggers:
                    f.write(f"  {trigger['trigger_name']} ({trigger['action_timing']} {trigger['event_manipulation']})\n")
            
            f.write("\n" + "-" * 60 + "\n\n")
        
        cur.close()
    conn.close()

if __name__ == "__main__":
    os.makedirs("mm/config/plaintext", exist_ok=True)
    generate_file_structure()
    generate_code_pages()
    generate_db_schema()
    print("Files generated in mm/config/plaintext/")

================================================================================
FILE: mm/utils/tools/scripts/check_status.py
================================================================================
#!/usr/bin/env python3
"""
Database monitoring dashboard for Ariadne
Auto-refreshes every 5 minutes
"""

import psycopg2
import subprocess
import time
import os
from datetime import datetime, timedelta
from zoneinfo import ZoneInfo

# ANSI color codes
GREEN = '\033[106;30m'
DARK_GREEN = '\033[96m'
LIGHT_GRAY = '\033[90m'
RED = '\033[95m'
RESET = '\033[0m'

def get_monit_status():
    """Get monit summary"""
    try:
        result = subprocess.run(['sudo', 'monit', 'summary'], 
                              capture_output=True, text=True, timeout=5)
        return result.stdout
    except Exception as e:
        return f"Could not get monit status: {e}"

def parse_partition_info(partition_name):
    """Parse partition name and return formatted info with color"""
    try:
        # Extract date/time from partition name: tickstick_YYYY_MM_DD_HH
        parts = partition_name.split('_')
        if len(parts) == 5:
            year, month, day, hour = int(parts[1]), int(parts[2]), int(parts[3]), int(parts[4])
            
            # Create datetime object (UTC)
            partition_dt = datetime(year, month, day, hour, tzinfo=ZoneInfo("UTC"))
            partition_end = partition_dt + timedelta(hours=1)
            
            # Convert to Toronto time for display
            tz = ZoneInfo("America/Toronto")
            partition_dt_local = partition_dt.astimezone(tz)
            partition_end_local = partition_end.astimezone(tz)
            
            # Format date - use the LOCAL date for display
            date_str = partition_dt_local.strftime("%b %d, %Y")
            
            # Format time range
            time_str = f"{partition_dt_local.strftime('%-I%p').lower()} → {partition_end_local.strftime('%-I%p').lower()}"
            
            # Determine status and color based on UTC comparison
            now_utc = datetime.now(ZoneInfo("UTC"))
            
            if partition_dt <= now_utc < partition_end:
                status = "Current"
                color = GREEN
            elif partition_end <= now_utc and (now_utc - partition_end).total_seconds() < 3600:
                status = "Previous"
                color = DARK_GREEN
            elif partition_end <= now_utc:
                status = "Expired"
                color = RED
            else:
                status = "Reserved"
                color = LIGHT_GRAY
            
            return date_str, time_str, status, color
    except Exception as e:
        return "Error", "Error", "Error", RESET
    
    return "Unknown", "Unknown", "Unknown", RESET

def main():
    while True:
        # Clear screen
        os.system('clear')
        
        # Header with timestamp
        print(f"\n{'='*73}")
        print(f"ARIADNE DASHBOARD - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"{'='*73}")
        
        try:
            # Connect to database
            conn = psycopg2.connect(database="ariadne", user="postgres", host="localhost")
            cur = conn.cursor()
            
            # 1. Monit Status
            print("\nMONIT STATUS")
            print("="*73)
            print(get_monit_status())
            
            # 2. Check heartbeats
            print("\nPROCESS HEARTBEATS")
            print("="*73)
            
            cur.execute("""
                SELECT 
                    process_name,
                    last_heartbeat,
                    EXTRACT(EPOCH FROM (NOW() - last_heartbeat))::int as seconds_since
                FROM heartbeats
                ORDER BY process_name
            """)
            
            print(f"{'Process':<20} {'Last Stamp':<25} {'Time Since':<15}")
            print("-"*73)
            
            for row in cur.fetchall():
                process, last_hb, seconds = row
                if seconds < 60:
                    time_str = f"{seconds}s ago"
                elif seconds < 3600:
                    time_str = f"{seconds//60}m ago"
                else:
                    time_str = f"{seconds//3600}h {(seconds%3600)//60}m ago"
                
                last_hb_str = last_hb.strftime("%Y-%m-%d %H:%M:%S") if last_hb else "Never"
                print(f"{process:<20} {last_hb_str:<25} {time_str:<15}")
            
            # 3. Check partitions with enhanced display
            print("\n" + "="*73)
            print("TICKSTICK PARTITIONS")
            print("="*73)
            
            cur.execute("""
                SELECT tablename 
                FROM pg_tables 
                WHERE tablename LIKE 'tickstick_%' 
                ORDER BY tablename
            """)
            
            partitions = cur.fetchall()
            
            if partitions:
                print(f"{'Partition Name':<28} {'Date':<15} {'Time Range':<15} {'Status':<10}")
                print("-"*73)
                
                for (partition,) in partitions:
                    date_str, time_str, status, color = parse_partition_info(partition)
                    print(f"{color}{partition:<28} {date_str:<15} {time_str:<15} {status:<10}{RESET}")
            else:
                print("No partitions found")
            
            # 4. Recent ticker data
            print("\n" + "="*73)
            print("10 MOST RECENT TICKER ENTRIES")
            print("="*73)
            
            try:
                cur.execute("""
                    SELECT timestamp, symbol, last
                    FROM tickstick
                    ORDER BY timestamp DESC
                    LIMIT 10
                """)
                
                rows = cur.fetchall()
                
                if rows:
                    print(f"{'Timestamp':<25} {'Symbol':<15} {'Last Price':<12}")
                    print("-"*73)
                    
                    tz = ZoneInfo("America/Toronto")
                    for timestamp, symbol, last in rows:
                        dt = datetime.fromtimestamp(timestamp, tz=tz)
                        time_str = dt.strftime("%Y-%m-%d %H:%M:%S EST")
                        print(f"{time_str:<25} {symbol:<15} {last:<12.8f}")
                else:
                    print("No recent data found")
                    
            except psycopg2.Error as e:
                print(f"Error querying ticker data: {e}")
            
            cur.close()
            conn.close()
            
        except psycopg2.Error as e:
            print(f"\nDatabase connection error: {e}")
        except KeyboardInterrupt:
            print("\n\nDashboard stopped by user")
            break
        
        # Refresh countdown
        print(f"\n{'='*73}")
        print("Auto-refresh every 15 seconds (Ctrl+C to exit)")
        print(f"{'='*73}")
        
        try:
            time.sleep(15)  # 15 seconds
        except KeyboardInterrupt:
            print("\n\nDashboard stopped by user")
            break

if __name__ == "__main__":
    main()

================================================================================
FILE: mm/utils/tools/scripts/standalone_emailer.py
================================================================================
# mm/utils/tools/scripts/standalone_emailer.py

# stdlib imports
import importlib
import os
import smtplib
import ssl
import uuid
from email.message import EmailMessage
from email.utils import formataddr
from datetime import datetime
from zoneinfo import ZoneInfo
    
# third-party imports
from dotenv import load_dotenv

# local application imports
import mm.config.marcus as marcus

# load env for this process
load_dotenv("mm/data/secrets/.env")


def send_email(subject: str, status: str, title: str, message: str) -> str:

    importlib.reload(marcus)
    if not bool(getattr(marcus, "ALERT_EMAIL_ENABLED", False)):
        return "disabled"
    if str(getattr(marcus, "ALERT_EMAIL_ENCRYPT", "SSL")).upper() != "SSL":
        return "Simple Mail Transfer Protocol not established. No conn."

    host = getattr(marcus, "ALERT_EMAIL_SMTP_SERVER", None)
    port = getattr(marcus, "ALERT_EMAIL_SMTP_PORT", None)
    recipient = getattr(marcus, "ALERT_EMAIL_RECIPIENT", None)

    USERCODE = "AND"  # hardcode per file

    # ---- Edit Sender Info (per file) ----
    user = os.getenv(f"{USERCODE}_USR")
    pwd = os.getenv(f"{USERCODE}_PWD")
    sender_email = user
    sender_name = os.getenv(f"{USERCODE}_NAME")
    # -------------------------------------

    # status color map
    STATUS_COLORS = {
        "STATCON3": "#F1C232",	# on the first missing heartbeat 
        "STATCON2": "#E69138",	# on the second missing heartbeat
        "STATCON1": "#CC0000",	# on the third missing heartbeat
        "SIGCON1": 	"#FB6D8B",	# Process never started
		"OPSCON5": 	"#F5F5F5",	# Normal, all systems nominal
        "OPSCON1": 	"#990000",	# Issues detected
    }
    status_text = str(status).upper()
    status_color = STATUS_COLORS.get(status_text, "#BE644C")

    msg = EmailMessage()
    domain = sender_email.split("@")[1] if "@" in sender_email else "hodlcorp.io"
    msg_id = f"<{uuid.uuid4()}@{domain}>"
    msg["Message-ID"] = msg_id
    msg["From"] = formataddr((sender_name, sender_email))
    msg["To"] = recipient
    msg["Subject"] = subject
    msg["X-Priority"] = "1"
    msg["X-MSMail-Priority"] = "High"
    msg["Importance"] = "High"

    # footer fields
    now_tz = datetime.now(ZoneInfo("America/Toronto"))
    sent_str = now_tz.strftime("%Y-%m-%d %H:%M:%S America/Toronto")
    epoch_ms = int(now_tz.timestamp() * 1000)
    mid_clean = msg_id.strip("<>").split("@", 1)[0]

    # full HTML body (single block)
    html_body = f"""
<div style="font-family: monospace;">
  <table role="presentation" width="100%" height="20px" cellpadding="8px" cellspacing="0" border="0">
    <!-- Top Banner -->
    <tr style="font-family: Georgia, 'Times New Roman', Times, serif;font-size:20px;font-weight:600;background-color:#333;">
      <td align="left" style="color:#EFEFEF;letter-spacing:12px;">INTCOMM</td>
      <td align="right" style="color:{status_color};letter-spacing:4px;">{status_text}</td>
    </tr>

    <!-- Message Title -->
    <tr width="100%" cellpadding="6px" style="font-family: Tahoma, Geneva, sans-serif;text-align:left;font-size:14px;font-weight:600;color:#333;">
      <td colspan="2">
        {title}
      </td>
    </tr>

    <!-- Message Content -->
    <tr width="100%" cellpadding="6px" style="font-family: Tahoma, Geneva, sans-serif;text-align:left;font-size:11px;font-weight:400;line-height:1.5;color:#333;">
      <td colspan="2">
        {message}
      </td>
    </tr>

    <!-- UNUSED SPACER ROW -->
    <tr width="100%" height="25px"><td colspan="2">&nbsp;</td></tr>
  </table>

  <!-- Footer -->
  <table role="presentation" width="400px" height="20px" cellpadding="4" cellspacing="0" border="0" style="font-family: Tahoma, Geneva, sans-serif;">
    <!-- DOCINT -->
    <tr style="background-color:#333;">
      <td colspan="2" style="color:#efefef;font-size:12px;font-weight:600;">DOCINT</td>
    </tr>

    <tr style="background-color:#E9E9E5;">
      <td width="30px" style="color:#333;font-size:10px;font-weight:600;">SENT</td>

      <td width="10px" style="color:#333;font-size:10px;font-weight:600;">&rarr;</td>
      <td style="color:#333;font-size:11px;font-weight:400;">{sent_str}</td>
    </tr>

    <tr style="background-color:#F2F2F0;">
      <td width="30px" style="color:#333;font-size:10px;font-weight:600;">EPOCH</td>
      <td width="10px" style="color:#333;font-size:10px;font-weight:600;">&rarr;</td>
      <td style="color:#333;font-size:11px;font-weight:400;">{epoch_ms} (ms since 1970/01/01 0:00 UTC)</td>
    </tr>

    <tr style="background-color:#E9E9E5;">
      <td width="30px" style="color:#333;font-size:10px;font-weight:600;">m.ID</td>
      <td width="10px" style="color:#333;font-size:10px;font-weight:600;">&rarr;</td>
      <td style="color:#333;font-size:11px;font-weight:400;">{mid_clean}</td>
    </tr>
  </table>
</div>
"""

    msg.add_alternative(html_body, subtype="html")

    ctx = ssl.create_default_context()
    with smtplib.SMTP_SSL(host, port, context=ctx, timeout=10) as s:
        if user and pwd:
            s.login(user, pwd)
        s.send_message(msg)

    return msg_id



================================================================================
FILE: mm/utils/tools/scripts/view_schemas.py
================================================================================
#!/usr/bin/env python3 
"""
View schemas for Ariadne SQLite databases (Simulation, Live, Ledger).

Usage:
    python scripts/view_schemas.py
"""

import os
import sqlite3
from typing import List, Tuple

# Resolve project root (one level up from /scripts)
ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))

# Default paths (match your config)
DBS = [
    ("Simulation DB", os.path.join(ROOT, "data", "sims", "ariadne_sim.db")),
    ("Live DB",       os.path.join(ROOT, "data", "live", "ariadne_live.db")),
    ("Ledger DB",     os.path.join(ROOT, "data", "finance", "ledger.db")),
]

SEP = "═" * 80


def print_header(title: str, path: str) -> None:
    print(SEP)
    print(f"{title}  |  path: {path}")
    print(SEP)


def fetch_all(conn: sqlite3.Connection, query: str, params: Tuple = ()) -> List[Tuple]:
    cur = conn.cursor()
    cur.execute(query, params)
    return cur.fetchall()


def print_table_schema(conn: sqlite3.Connection, table: str) -> None:
    # CREATE statement (may be None for some system tables)
    create_sql_rows = fetch_all(
        conn,
        "SELECT sql FROM sqlite_master WHERE type='table' AND name=?;",
        (table,),
    )
    create_sql = (create_sql_rows[0][0] or "").strip() if create_sql_rows else ""
    print(f"\nTABLE: {table}")
    if create_sql:
        print("CREATE SQL:")
        print(create_sql)
    else:
        print("(No CREATE SQL found)")

    # Columns
    cols = fetch_all(conn, f"PRAGMA table_info('{table}');")
    if cols:
        # cols: cid, name, type, notnull, dflt_value, pk
        print("\nCOLUMNS:")
        print(f"{'cid':>3}  {'name':<24} {'type':<16} {'notnull':<7} {'default':<18} {'pk':<2}")
        print("-" * 80)
        for cid, name, coltype, notnull, dflt, pk in cols:
            dflt_str = "NULL" if dflt is None else str(dflt)
            print(f"{cid:>3}  {name:<24} {coltype:<16} {notnull!s:<7} {dflt_str:<18} {pk!s:<2}")
    else:
        print("\n(No columns found)")

    # Indexes (optional but useful)
    idxs = fetch_all(conn, f"PRAGMA index_list('{table}');")
    if idxs:
        print("\nINDEXES:")
        # rows: seq, name, unique, origin, partial
        print(f"{'seq':>3}  {'name':<32} {'unique':<6} {'origin':<6} {'partial':<7}")
        print("-" * 80)
        for seq, name, unique, origin, partial in idxs:
            print(f"{seq:>3}  {name:<32} {unique!s:<6} {origin:<6} {partial!s:<7}")


def describe_db(title: str, path: str) -> None:
    print_header(title, path)
    if not os.path.exists(path):
        print(f"!! Missing database: {path}\n")
        return

    try:
        conn = sqlite3.connect(path)
    except Exception as e:
        print(f"!! Failed to open {path}: {e}\n")
        return

    try:
        # List tables
        tables = fetch_all(conn, "SELECT name FROM sqlite_master WHERE type='table' ORDER BY name;")
        table_names = [t[0] for t in tables]
        if not table_names:
            print("(No tables found)\n")
        else:
            print("TABLES:")
            print(", ".join(table_names) + "\n")
            # Print schema for each table
            for t in table_names:
                print_table_schema(conn, t)
                print()
    finally:
        conn.close()


def main():
    for title, path in DBS:
        describe_db(title, path)


if __name__ == "__main__":
    main()

================================================================================
FILE: mm/utils/tools/scripts/alert_verify.py
================================================================================
import sys
from mm.utils.helpers.wintermute import send_alert

def main():
    print("Send Alert Function Test...")

    try:
        send_alert(
            subject="Testing the send alert function via Wintermute",
            message="This is just a test. Send alert function has been invoked from Wintermute.",
            process_name="alert_verify"
        )
        print("Send Success")
    except Exception as e:
        print(f"Send Failure: {e}")
        sys.exit(1)

if __name__ == "__main__": 
    main()

================================================================================
FILE: mm/utils/tools/scripts/reset_simulation.py
================================================================================
# /root/Echelon/valentrix/mm/scripts/reset_simulation.py
# last update: August 21, 2025

"""
Simulation Reset Script for Ariadne
Clears all trading data while preserving database structure.
Safe alternative to deleting the entire database file.
"""

import sqlite3
import os
import json
from pathlib import Path
import sys
import tty
import termios

# Define the MM root and the target database
MM_ROOT = "/root/Echelon/valentrix/mm"
SIM_DB_PATH = os.path.join(MM_ROOT, "data/sims/ariadne_sim.db")
STATE_FILE_PATH = os.path.join(MM_ROOT, "data/state/sim_state.json")  # Updated to sim_state.json

def get_single_keypress(prompt: str = "") -> str:
    """Get a single keypress without requiring Enter"""
    print(prompt, end="", flush=True)
    
    # Save terminal settings
    fd = sys.stdin.fileno()
    old_settings = termios.tcgetattr(fd)
    
    try:
        tty.setraw(sys.stdin.fileno())
        char = sys.stdin.read(1)
    finally:
        # Restore terminal settings
        termios.tcsetattr(fd, termios.TCSADRAIN, old_settings)
    
    print()  # New line after input
    return char.lower()

def reset_simulation_data():
    """Clears all trading data while preserving database structure and adds 2500 USDT."""
    
    if not os.path.exists(SIM_DB_PATH):
        print(f"❌ Simulation database not found at: {SIM_DB_PATH}")
        print("   The database may not exist or the path is incorrect.")
        return False
    
    try:
        conn = sqlite3.connect(SIM_DB_PATH)
        c = conn.cursor()
        
        # Clear all tables in proper order to maintain referential integrity
        c.execute("DELETE FROM simulated_trades")
        c.execute("DELETE FROM orders")
        c.execute("DELETE FROM order_books")
        c.execute("DELETE FROM simulation_balances")
        
        # Insert 2500 USDT as initial capital
        c.execute("INSERT INTO simulation_balances (currency, available) VALUES ('USDT', 2500.0)")
        
        # Check if sqlite_sequence table exists before trying to reset it
        c.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='sqlite_sequence'")
        if c.fetchone():
            c.execute("DELETE FROM sqlite_sequence WHERE name IN ('orders', 'order_books', 'simulated_trades')")
        
        conn.commit()
        conn.close()
        
        print(f"✅ Simulation database reset successfully: {SIM_DB_PATH}")
        print("   - All orders cleared")
        print("   - All order book snapshots cleared") 
        print("   - All simulated trades cleared")
        print("   - 2500.0 USDT added to simulation_balances")
        print("   - Auto-increment counters reset (if applicable)")
        print("   - Database structure preserved")
        
        return True
        
    except sqlite3.Error as e:
        print(f"❌ Error resetting simulation database: {e}")
        return False

def reset_state_file():
    """Reset the bot state file to initial conditions"""
    try:
        # Ensure directory exists
        os.makedirs(os.path.dirname(STATE_FILE_PATH), exist_ok=True)
        
        # Create fresh initial state
        initial_state = {
            "total_equity": 2500.0,
            "total_committed_value": 0.0,
            "open_orders": {},
            "last_heartbeat": 0,
            "blacklist": {},
            "start_time": 0,
            "cycle_count": 0,
            "performance_metrics": {}
        }
        
        with open(STATE_FILE_PATH, 'w') as f:
            json.dump(initial_state, f, indent=2)
            
        print(f"✅ State file reset to initial conditions: {STATE_FILE_PATH}")
        return True
        
    except Exception as e:
        print(f"❌ Error resetting state file: {e}")
        return False

def main():
    """Main function to run the simulation reset."""
    print("\n" + "="*50)
    print("        ARIADNE SIMULATION RESET")
    print("="*50)
    print("\nThis will clear ALL trading data from the simulation database.")
    print("All orders, trades, and order book history will be permanently deleted.")
    print("Bot state will be reset to initial conditions (2500 USDT, no inventory).")
    print("Database structure and schema will be preserved.")
    print("\n" + "="*50)
    
    # Get confirmation with single keypress
    response = get_single_keypress("Press Y to reset simulation or N to cancel: ")
    
    if response == 'y':
        print("\n🔄 Resetting simulation...")
        
        db_success = reset_simulation_data()
        state_success = reset_state_file()
        
        if db_success and state_success:
            print("\n✅ Complete reset successful! Ready for fresh start.")
            print("   - 2500.0 USDT available in simulation")
            print("   - All previous trading data cleared")
            print("   - State reset to initial conditions")
        else:
            print("\n❌ Reset partially failed. Check errors above.")
            
    else:
        print("\n❌ Reset cancelled. No changes were made.")
    
    print("\n" + "="*50)
    print("Reset operation complete")
    print("="*50)

if __name__ == "__main__":
    main()

================================================================================
FILE: mm/utils/tools/scripts/nomnom.py
================================================================================
import sys
from mm.utils.helpers.wintermute import send_alert

def main():
    print("Send Alert Function Test...")

    try:
        print(f"About to call send_alert with process_name='naomi'") 
        send_alert(
            subject="This is some bullshit.",
            message="This email is supposed to be from Naomi, but it likely arrived from Wintermute. Three AIs and 2 hours later, still no idea why.",
            process_name="naomi"
        )
        print("Send Success")
    except Exception as e:
        print(f"Send Failure: {e}") 
        sys.exit(1)

if __name__ == "__main__":
    main()

================================================================================
FILE: mm/utils/tqt/karin.py
================================================================================
#===================================================================
# 🍁 A R I A N D E           bot version 6.1 file build 20250903.01
#===================================================================
# last update: 2025 | Sept. 3                   Production ready ✅
#===================================================================
# KARIN (Kinetic Automated Relay Interface Node)
# mm/utils/tqt/karin.py
#
# Schema discovery and monitoring for Andi's validation
# Discovers and monitors ALL database tables
# Updates cache for Andi, alerts on changes
#
# [520] [741] [8]
#===================================================================
# 🔰 THE COMMANDER            ✔ PERSISTANT RUNTIME  ✔ MONIT MANAGED
#===================================================================
 
# 🔸 Standard Library Imports ======================================

import os
import sys
import json
import signal
import time
import asyncio
from typing import Dict, Any, List, Optional, Tuple
from pathlib import Path
from dotenv import load_dotenv
import psycopg2
import psycopg2.extras

# 🔸 Load environment variables ====================================
load_dotenv()

# 🔸 Add parent directory to path for imports ======================
sys.path.append('/root/Echelon/valentrix')

from mm.utils.helpers.wintermute import (
    get_logger,
    now_pack,
    write_pid_file,
    cleanup_pid_file,
    get_db_connection,
    release_db_connection,
    EmailClient
)
from mm.utils.helpers.inara import get_mode
from mm.config.marcus import ALERT_EMAIL_RECIPIENT

# 🔸 Configuration =================================================

SCHEMA_CACHE_PATH = "/root/Echelon/valentrix/mm/data/source/schemas.json"
PID_FILE = "/root/Echelon/valentrix/mm/utils/tqt/karin.pid"
LOG_FILE = "/root/Echelon/valentrix/mm/utils/tqt/karin.log"
CHECK_INTERVAL = 30  # seconds

# 🔸 Default values for nullable->not null transitions =============

DEFAULT_VALUES = {
    'TEXT': "''",
    'VARCHAR': "''",
    'CHARACTER VARYING': "''",
    'CHAR': "''",
    'CHARACTER': "''",
    'INTEGER': "0",
    'INT': "0",
    'SMALLINT': "0",
    'BIGINT': "0",
    'DECIMAL': "0.0",
    'NUMERIC': "0.0",
    'REAL': "0.0",
    'DOUBLE PRECISION': "0.0",
    'BOOLEAN': "false",
    'BOOL': "false",
    'TIMESTAMP': "NOW()",
    'TIMESTAMPTZ': "NOW()",
    'TIMESTAMP WITHOUT TIME ZONE': "NOW()",
    'TIMESTAMP WITH TIME ZONE': "NOW()",
    'DATE': "CURRENT_DATE",
    'TIME': "CURRENT_TIME",
    'JSONB': "'{}'::jsonb",
    'JSON': "'{}'::json",
    'UUID': "gen_random_uuid()",
    'BYTEA': "''::bytea",
    'ARRAY': "'{}'",
    'INTERVAL': "'0 seconds'::interval"
}

# 🔸 Global shutdown flag ==========================================

shutdown_requested = False

# 🔸 Logger ========================================================

log = get_logger("karin", LOG_FILE)

# 🔸 Email client

mailer = EmailClient('karin')

# 🔸 Signal Handlers ===============================================

def signal_handler(signum, frame):
    """Handle shutdown signals gracefully."""
    global shutdown_requested
    log.info(f"[SHUTDOWN] Received signal {signum}, shutting down gracefully...")
    shutdown_requested = True

signal.signal(signal.SIGINT, signal_handler)
signal.signal(signal.SIGTERM, signal_handler)

# 🔸 Schema Discovery ==============================================

class SchemaDiscovery:
    """Discovers and compares ALL database schemas."""
    
    def __init__(self):
        self.conn = None
        self.last_schemas = {}
        self.load_cache()
    
    def load_cache(self):
        """Load cached schemas from JSON file."""
        try:
            if Path(SCHEMA_CACHE_PATH).exists():
                with open(SCHEMA_CACHE_PATH, 'r') as f:
                    cache = json.load(f)
                    self.last_schemas = cache.get('schemas', {})
                    log.info(f"Loaded schema cache with {len(self.last_schemas)} tables")
        except Exception as e:
            log.warning(f"Could not load schema cache: {e}")
            self.last_schemas = {}
    
    def save_cache(self, schemas: Dict):
        """Save schemas to JSON cache file."""
        try:
            # Ensure directory exists
            Path(SCHEMA_CACHE_PATH).parent.mkdir(parents=True, exist_ok=True)
            
            tp = now_pack()
            cache = {
                'version': '1.0',
                'updated_at': tp.iso,
                'updated_epoch_ms': tp.epoch_ms,
                'mode': get_mode(),
                'table_count': len(schemas),
                'schemas': schemas,
                'defaults': self._generate_defaults(schemas)
            }
            
            # Write atomically (write to temp, then rename)
            temp_path = f"{SCHEMA_CACHE_PATH}.tmp"
            with open(temp_path, 'w') as f:
                json.dump(cache, f, indent=2, default=str)
            
            # Atomic rename
            os.rename(temp_path, SCHEMA_CACHE_PATH)
            log.info(f"Schema cache updated with {len(schemas)} tables")
            
        except Exception as e:
            log.error(f"Failed to save schema cache: {e}")
    
    def get_all_tables(self) -> List[str]:
        """Get ALL tables from the database (excluding system tables)."""
        try:
            cur = self.conn.cursor()
            
            # Get all user tables (excluding PostgreSQL system schemas)
            cur.execute("""
                SELECT tablename 
                FROM pg_tables 
                WHERE schemaname = 'public'
                ORDER BY tablename
            """)
            
            tables = [row[0] for row in cur.fetchall()]
            cur.close()
            
            log.info(f"Found {len(tables)} tables in database")
            return tables
            
        except Exception as e:
            log.error(f"Failed to get table list: {e}")
            return []
    
    def discover_table_schema(self, table_name: str) -> Dict:
        """Discover schema for a single table."""
        try:
            cur = self.conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
            
            # Get column information
            cur.execute("""
                SELECT 
                    column_name,
                    data_type,
                    character_maximum_length,
                    numeric_precision,
                    numeric_scale,
                    is_nullable,
                    column_default,
                    udt_name
                FROM information_schema.columns
                WHERE table_name = %s
                AND table_schema = 'public'
                ORDER BY ordinal_position
            """, (table_name,))
            
            columns = {}
            for row in cur.fetchall():
                col_name = row['column_name']
                
                # Build type string (use udt_name for more precise type info)
                dtype = row['udt_name'].upper() if row['udt_name'] else row['data_type'].upper()
                
                # Add precision/length info if available
                if row['character_maximum_length']:
                    dtype = f"{dtype}({row['character_maximum_length']})"
                elif row['numeric_precision'] and row['numeric_scale']:
                    dtype = f"{dtype}({row['numeric_precision']},{row['numeric_scale']})"
                elif row['numeric_precision']:
                    dtype = f"{dtype}({row['numeric_precision']})"
                
                columns[col_name] = {
                    'type': dtype,
                    'base_type': row['data_type'].upper(),
                    'nullable': row['is_nullable'] == 'YES',
                    'default': row['column_default']
                }
            
            # Get primary key
            cur.execute("""
                SELECT a.attname
                FROM pg_index i
                JOIN pg_attribute a ON a.attrelid = i.indrelid AND a.attnum = ANY(i.indkey)
                WHERE i.indrelid = %s::regclass AND i.indisprimary
            """, (table_name,))
            
            pk_columns = [row[0] for row in cur.fetchall()]
            
            # Get foreign keys
            cur.execute("""
                SELECT
                    kcu.column_name,
                    ccu.table_name AS foreign_table,
                    ccu.column_name AS foreign_column
                FROM information_schema.table_constraints AS tc
                JOIN information_schema.key_column_usage AS kcu
                    ON tc.constraint_name = kcu.constraint_name
                JOIN information_schema.constraint_column_usage AS ccu
                    ON ccu.constraint_name = tc.constraint_name
                WHERE tc.constraint_type = 'FOREIGN KEY'
                AND tc.table_name = %s
            """, (table_name,))
            
            foreign_keys = {}
            for row in cur.fetchall():
                foreign_keys[row['column_name']] = {
                    'references': f"{row['foreign_table']}.{row['foreign_column']}"
                }
            
            # Get indexes
            cur.execute("""
                SELECT indexname, indexdef
                FROM pg_indexes
                WHERE tablename = %s
                AND schemaname = 'public'
            """, (table_name,))
            
            indexes = {row[0]: row[1] for row in cur.fetchall()}
            
            # Get constraints
            cur.execute("""
                SELECT conname, pg_get_constraintdef(c.oid)
                FROM pg_constraint c
                JOIN pg_namespace n ON n.oid = c.connamespace
                JOIN pg_class cl ON cl.oid = c.conrelid
                WHERE cl.relname = %s
                AND n.nspname = 'public'
            """, (table_name,))
            
            constraints = {row[0]: row[1] for row in cur.fetchall()}
            
            cur.close()
            
            return {
                'columns': columns,
                'primary_key': pk_columns,
                'foreign_keys': foreign_keys,
                'indexes': indexes,
                'constraints': constraints
            }
            
        except Exception as e:
            log.error(f"Failed to discover schema for {table_name}: {e}")
            return {}
    
    def discover_all_schemas(self) -> Dict:
        """Discover schemas for ALL tables in the database."""
        schemas = {}
        
        try:
            self.conn = get_db_connection()
            
            # Get ALL tables
            tables = self.get_all_tables()
            
            # Discover schema for each table
            for table in tables:
                schema = self.discover_table_schema(table)
                if schema:
                    schemas[table] = schema
                    log.debug(f"Discovered schema for {table}: {len(schema['columns'])} columns")
            
            log.info(f"Discovered schemas for {len(schemas)} tables")
            
        except Exception as e:
            log.error(f"Failed to discover schemas: {e}")
        finally:
            if self.conn:
                release_db_connection(self.conn)
                self.conn = None
        
        return schemas
    
    def compare_schemas(self, old: Dict, new: Dict) -> List[Dict]:
        """Compare two schema dictionaries and return changes."""
        changes = []
        
        # Check for new tables
        for table in new:
            if table not in old:
                changes.append({
                    'type': 'new_table',
                    'table': table,
                    'details': f"New table with {len(new[table]['columns'])} columns"
                })
        
        # Check for removed tables
        for table in old:
            if table not in new:
                changes.append({
                    'type': 'removed_table',
                    'table': table,
                    'details': f"Table removed"
                })
        
        # Check for changes in existing tables
        for table in set(old.keys()) & set(new.keys()):
            old_cols = old[table]['columns']
            new_cols = new[table]['columns']
            
            # New columns
            for col in new_cols:
                if col not in old_cols:
                    changes.append({
                        'type': 'new_column',
                        'table': table,
                        'column': col,
                        'details': f"New column: {new_cols[col]}"
                    })
            
            # Removed columns
            for col in old_cols:
                if col not in new_cols:
                    changes.append({
                        'type': 'removed_column',
                        'table': table,
                        'column': col,
                        'details': f"Column removed"
                    })
            
            # Modified columns
            for col in set(old_cols.keys()) & set(new_cols.keys()):
                old_def = old_cols[col]
                new_def = new_cols[col]
                
                # Type change
                if old_def['type'] != new_def['type']:
                    changes.append({
                        'type': 'type_change',
                        'table': table,
                        'column': col,
                        'details': f"Type changed from {old_def['type']} to {new_def['type']}"
                    })
                
                # Nullable change
                if old_def['nullable'] != new_def['nullable']:
                    if old_def['nullable'] and not new_def['nullable']:
                        changes.append({
                            'type': 'nullable_to_not_null',
                            'table': table,
                            'column': col,
                            'details': f"Column now NOT NULL",
                            'needs_default': True
                        })
                    else:
                        changes.append({
                            'type': 'not_null_to_nullable',
                            'table': table,
                            'column': col,
                            'details': f"Column now NULLABLE"
                        })
                
                # Default change
                if old_def.get('default') != new_def.get('default'):
                    changes.append({
                        'type': 'default_change',
                        'table': table,
                        'column': col,
                        'details': f"Default changed from {old_def.get('default')} to {new_def.get('default')}"
                    })
            
            # Check primary key changes
            if old[table]['primary_key'] != new[table]['primary_key']:
                changes.append({
                    'type': 'primary_key_change',
                    'table': table,
                    'details': f"Primary key changed from {old[table]['primary_key']} to {new[table]['primary_key']}"
                })
            
            # Check foreign key changes
            if old[table]['foreign_keys'] != new[table]['foreign_keys']:
                changes.append({
                    'type': 'foreign_key_change',
                    'table': table,
                    'details': f"Foreign key constraints changed"
                })
            
            # Check index changes
            old_indexes = set(old[table].get('indexes', {}).keys())
            new_indexes = set(new[table].get('indexes', {}).keys())
            
            for idx in new_indexes - old_indexes:
                changes.append({
                    'type': 'new_index',
                    'table': table,
                    'index': idx,
                    'details': f"New index created: {idx}"
                })
            
            for idx in old_indexes - new_indexes:
                changes.append({
                    'type': 'removed_index',
                    'table': table,
                    'index': idx,
                    'details': f"Index removed: {idx}"
                })
        
        return changes
    
    def _generate_defaults(self, schemas: Dict) -> Dict:
        """Generate default values for stop-gap handling."""
        defaults = {}
        
        for table, schema in schemas.items():
            table_defaults = {}
            
            for col, info in schema['columns'].items():
                if not info['nullable']:
                    # Use column default if available
                    if info.get('default'):
                        table_defaults[col] = info['default']
                    else:
                        # Map PostgreSQL types to default values
                        base_type = info.get('base_type', info['type']).split('(')[0].upper()
                        if base_type in DEFAULT_VALUES:
                            table_defaults[col] = DEFAULT_VALUES[base_type]
                        else:
                            # Generic default
                            table_defaults[col] = "NULL"
            
            if table_defaults:
                defaults[table] = table_defaults
        
        return defaults
    
    def format_changes_email(self, changes: List[Dict]) -> str:
        """Format schema changes for email alert."""
        if not changes:
            return "No schema changes detected."
        
        tp = now_pack()
        lines = [
            f"Schema Changes Detected",
            f"Time: {tp.human}",
            f"Mode: {get_mode()}",
            f"Database: ariadne",
            f"",
            f"Changes ({len(changes)} total):",
            "=" * 60
        ]
        
        # Group changes by type
        by_type = {}
        for change in changes:
            change_type = change['type']
            if change_type not in by_type:
                by_type[change_type] = []
            by_type[change_type].append(change)
        
        # Format each type of change
        for change_type, items in by_type.items():
            lines.append(f"\n{change_type.upper().replace('_', ' ')} ({len(items)} items):")
            lines.append("-" * 40)
            
            for item in items[:10]:  # Limit to first 10 of each type
                lines.append(f"  Table: {item.get('table', 'N/A')}")
                if 'column' in item:
                    lines.append(f"  Column: {item['column']}")
                if 'index' in item:
                    lines.append(f"  Index: {item['index']}")
                lines.append(f"  Details: {item['details']}")
                
                if item.get('needs_default'):
                    lines.append(f"  ⚠️ ACTION: Default value will be applied")
                lines.append("")
            
            if len(items) > 10:
                lines.append(f"  ... and {len(items) - 10} more\n")
        
        lines.extend([
            "=" * 60,
            "",
            "Schema cache has been updated.",
            "Andi will use new schemas for validation.",
            "",
            "To pause trading for maintenance:",
            "  1. Use dashboard pause function",
            "  2. Wait for confirmation",
            "  3. Make schema changes",
            "  4. Resume trading"
        ])
        
        return "\n".join(lines)

# 🔸 Main Process ==================================================

class KARIN:
    """Main KARIN process - monitors ALL schemas continuously."""
    
    def __init__(self):
        self.discovery = SchemaDiscovery()
        self.check_count = 0
        self.changes_detected = 0
    
    def check_schemas(self):
        """Single schema check cycle."""
        self.check_count += 1
        
        try:
            # Discover current schemas for ALL tables
            current_schemas = self.discovery.discover_all_schemas()
            
            if not current_schemas:
                log.warning("No schemas discovered, database may be unavailable")
                return
            
            # Compare with cached schemas
            if self.discovery.last_schemas:
                changes = self.discovery.compare_schemas(
                    self.discovery.last_schemas, 
                    current_schemas
                )
                
                if changes:
                    self.changes_detected += len(changes)
                    log.info(f"Detected {len(changes)} schema changes")
                    
                    # Send email alert
                    try:
                        email_body = self.discovery.format_changes_email(changes)
                        mailer.send_email(
                            to=ALERT_EMAIL_RECIPIENT,
                            subject=f"[KARIN] Schema Changes Detected - {get_mode()} mode",
                            text=email_body
                        )
                        log.info(f"Alert sent to {ALERT_EMAIL_RECIPIENT}")
                    except Exception as e:
                        log.error(f"Failed to send email alert: {e}")
                    
                    # Update cache for Andi
                    self.discovery.save_cache(current_schemas)
                    self.discovery.last_schemas = current_schemas
                else:
                    log.debug(f"Check #{self.check_count}: No schema changes")
            else:
                # First run, just save
                log.info(f"Initial schema discovery complete - {len(current_schemas)} tables")
                self.discovery.save_cache(current_schemas)
                self.discovery.last_schemas = current_schemas
        
        except Exception as e:
            log.error(f"Schema check failed: {e}")
    
    def update_heartbeat(self):
        """Update heartbeat in database."""
        try:
            conn = get_db_connection()
            cur = conn.cursor()
            
            cur.execute("""
                INSERT INTO heartbeats (process_name, last_heartbeat, status, pid, cycle_count)
                VALUES ('karin', NOW(), 'monitoring', %s, %s)
                ON CONFLICT (process_name)
                DO UPDATE SET 
                    last_heartbeat = NOW(),
                    status = 'monitoring',
                    pid = %s,
                    cycle_count = %s
            """, (os.getpid(), self.check_count, os.getpid(), self.check_count))
            
            conn.commit()
            cur.close()
            release_db_connection(conn)
            
        except Exception as e:
            log.error(f"Failed to update heartbeat: {e}")
    
    async def run_async(self):
        """Async main loop for schema checking."""
        log.info(f"[INIT] KARIN starting in {get_mode()} mode")
        log.info(f"[INIT] Will monitor ALL tables in database")
        log.info(f"[INIT] Check interval: {CHECK_INTERVAL} seconds")
        
        # Initial check
        self.check_schemas()
        
        while not shutdown_requested:
            try:
                start = time.time()
                
                # Check schemas
                self.check_schemas()
                
                # Update heartbeat every 10 checks (5 minutes)
                if self.check_count % 10 == 0:
                    self.update_heartbeat()
                    log.info(f"[HEARTBEAT] Checks: {self.check_count}, Changes: {self.changes_detected}")
                
                # Calculate sleep time
                elapsed = time.time() - start
                sleep_time = max(CHECK_INTERVAL - elapsed, 1)
                
                await asyncio.sleep(sleep_time)
                
            except Exception as e:
                log.error(f"Error in main loop: {e}")
                await asyncio.sleep(CHECK_INTERVAL)
        
        log.info("[SHUTDOWN] KARIN shutting down gracefully")
    
    def run(self):
        """Synchronous wrapper for the async run method."""
        try:
            write_pid_file(PID_FILE)
            asyncio.run(self.run_async())
        finally:
            cleanup_pid_file(PID_FILE)

# 🔸 Entry Point ===================================================

if __name__ == "__main__":
    karin = KARIN()
    karin.run()

================================================================================
FILE: mm/utils/tqt/andi.py
================================================================================
#===================================================================
# 🍁 A R I A N D E           bot version 6.1 file build 20250903.01
#===================================================================
# last update: 2025 | Sept. 3                   Production ready ❌
#===================================================================
# Andi - Transactional Query Table Processor
# mm/utils/tqt/andi.py
#
# Validates and queues database writes from managers
# Batches for efficiency, handles retries, maintains data 
# integrity
#
# [520] [741] [8]
#===================================================================
# 🔰 THE COMMANDER            ✔ PERSISTANT RUNTIME  ✔ MONIT MANAGED
#===================================================================

# 🔸 Standard Library Imports ======================================

import os
import sys
import json
import signal
import time
import threading
import queue
import uuid
from typing import Dict, Any, List, Optional, Tuple, Callable
from pathlib import Path
from datetime import datetime, timedelta, timezone
from decimal import Decimal
from collections import defaultdict
import psycopg2
import psycopg2.extras

# 🔸 Add project root for imports (deployment path) ================

sys.path.append('/root/Echelon/valentrix')

from mm.utils.helpers.wintermute import (
    get_logger,
    now_pack,
    write_pid_file,
    cleanup_pid_file,
    get_db_connection,
    release_db_connection,
    update_heartbeat
)
from mm.utils.helpers.inara import get_mode

# 🔸 Configuration =================================================

SCHEMA_CACHE_PATH = "/root/Echelon/valentrix/mm/data/source/schemas.json"
PID_FILE = "/root/Echelon/valentrix/mm/utils/tqt/andi.pid"
LOG_FILE = "/root/Echelon/valentrix/mm/utils/tqt/andi.log"
NOTES_FILE = "/root/Echelon/valentrix/mm/utils/tqt/andi_notes.log"

# 🔸 Processing parameters =========================================

BATCH_SIZE = 50           # Max items per batch (mixed table/hold/asset)
BATCH_INTERVAL = 0.10     # seconds
RETRY_MAX = 3             # Max retries per item
RETRY_DELAY = 0.50        # Initial retry delay (per item)
DLQ_MAX_SIZE = 1000       # Max failed items to retain
HEARTBEAT_INTERVAL = 300  # seconds

# 🔸 Sweeping defaults =============================================

HOLD_SWEEP_AGE_MIN = 10     # minutes for holds.sweep
ASSET_SWEEP_AGE_MIN = 10    # minutes for assets.sweep

# 🔸 Global shutdown flag ==========================================

shutdown_requested = False

# 🔸 Loggers =======================================================

log = get_logger("andi", LOG_FILE)
notes = get_logger("andi.notes", NOTES_FILE)

# 🔸 Signal Handlers ===============================================

def signal_handler(signum, frame):
    """Handle shutdown signals gracefully."""
    global shutdown_requested
    log.info(f"[SHUTDOWN] Received signal {signum}, flushing queues...")
    shutdown_requested = True

signal.signal(signal.SIGINT, signal_handler)
signal.signal(signal.SIGTERM, signal_handler)

# 🔸 Schema Validator ==============================================

class SchemaValidator:
    """
    Validates data against schemas from KARIN's cache.
    """

    def __init__(self):
        self.schemas: Dict[str, Any] = {}
        self.defaults: Dict[str, Dict[str, Any]] = {}
        self.last_loaded: Optional[datetime] = None
        self.load_schemas()

    def load_schemas(self):
        """Load schemas from KARIN's cache file."""
        try:
            if not Path(SCHEMA_CACHE_PATH).exists():
                log.warning(f"Schema cache not found at {SCHEMA_CACHE_PATH}")
                return False

            with open(SCHEMA_CACHE_PATH, 'r') as f:
                cache = json.load(f)

            self.schemas = cache.get('schemas', {})
            self.defaults = cache.get('defaults', {})
            self.last_loaded = datetime.now(timezone.utc)

            log.info(f"Loaded schemas for {len(self.schemas)} tables")
            return True

        except Exception as e:
            log.error(f"Failed to load schemas: {e}")
            return False

    def should_reload(self) -> bool:
        if not self.last_loaded:
            return True
        try:
            cache_mtime = Path(SCHEMA_CACHE_PATH).stat().st_mtime
            cache_modified = datetime.fromtimestamp(cache_mtime, tz=timezone.utc)
            return cache_modified > self.last_loaded
        except Exception:
            return False

    def validate(self, table: str, data: Dict) -> Tuple[bool, Optional[str], Dict]:
        """Validate data against schema. Returns (ok, err, processed)."""
        if self.should_reload():
            self.load_schemas()

        if table not in self.schemas:
            return False, f"Unknown table: {table}", {}

        schema = self.schemas[table]
        columns = schema.get('columns', {})
        processed: Dict[str, Any] = {}

        for col_name, col_info in columns.items():
            value = data.get(col_name)

            if value is None:
                if not col_info.get('nullable', True):
                    default_map = self.defaults.get(table, {})
                    if col_name in default_map:
                        value = default_map[col_name]
                        notes.info(f"Applied default for {table}.{col_name}: {value}")
                    else:
                        return False, f"Missing required field: {col_name}", {}

            if value is not None:
                dtype = str(col_info.get('type', '')).upper()
                processed[col_name] = self._coerce_type(value, dtype)
            else:
                processed[col_name] = None

        extra = set(data.keys()) - set(columns.keys())
        if extra:
            notes.warning(f"Extra fields ignored for {table}: {extra}")

        return True, None, processed

    def _coerce_type(self, value: Any, dtype: str) -> Any:
        base = dtype.split('(')[0]
        if base in ('TEXT', 'VARCHAR', 'CHAR', 'UUID'):
            return str(value)
        if base in ('INTEGER', 'INT', 'SMALLINT', 'BIGINT'):
            return int(value)
        if base in ('DECIMAL', 'NUMERIC'):
            return Decimal(str(value))
        if base in ('REAL', 'DOUBLE'):
            return float(value)
        if base == 'BOOLEAN':
            if isinstance(value, str):
                return value.lower() in ('true', '1', 'yes', 'y')
            return bool(value)
        if base in ('TIMESTAMP', 'TIMESTAMPTZ', 'DATE', 'TIME', 'JSON', 'JSONB'):
            return value  # let PG handle
        return value

# 🔸 Write Queue Manager ===========================================

class WriteQueue:
    """
    Manages queued items with batching and retry logic.
    Supports:
      • Table writes: {'table', 'data', ...}
      • Topics:
          - holds.*   (Julius)
          - assets.*  (Helen)
    """

    def __init__(self, validator: SchemaValidator):
        self.validator = validator
        self.pending_queue: "queue.Queue[Dict[str, Any]]" = queue.Queue()
        self.dead_letter_queue: List[Dict[str, Any]] = []
        self.stats = defaultdict(int)
        self.consecutive_failures = 0
        # Topic handlers
        self.hold_handler: Optional[Callable[[Dict[str, Any]], None]] = None
        self.asset_handler: Optional[Callable[[Dict[str, Any]], None]] = None

    def set_hold_handler(self, fn: Callable[[Dict[str, Any]], None]) -> None:
        self.hold_handler = fn

    def set_asset_handler(self, fn: Callable[[Dict[str, Any]], None]) -> None:
        self.asset_handler = fn

    def enqueue(self, table: str, data: Dict, source: str = None) -> str:
        """Validated INSERT into a specific table."""
        queue_id = str(uuid.uuid4())
        ok, err, processed = self.validator.validate(table, data)
        if not ok:
            self.stats['validation_failures'] += 1
            notes.error(f"Validation failed for {table}: {err}")
            self.add_to_dlq({
                'queue_id': queue_id,
                'table': table,
                'data': data,
                'error': err,
                'source': source or 'unknown',
                'timestamp': now_pack().iso
            })
            return queue_id

        self.pending_queue.put({
            'queue_id': queue_id,
            'table': table,
            'data': processed,
            'source': source or 'unknown',
            'timestamp': datetime.now(timezone.utc),
            'attempts': 0
        })
        self.stats['enqueued'] += 1
        return queue_id

    def enqueue_topic(self, topic: str, payload: Dict, source: str = None) -> str:
        """Generic topic enqueue (holds.* or assets.*)."""
        queue_id = str(uuid.uuid4())
        self.pending_queue.put({
            'queue_id': queue_id,
            'topic': topic,
            'payload': payload,
            'source': source or 'unknown',
            'timestamp': datetime.now(timezone.utc),
            'attempts': 0
        })
        self.stats['enqueued'] += 1
        return queue_id

    # Back-compat sugar
    def enqueue_hold(self, topic: str, payload: Dict, source: str = None) -> str:
        return self.enqueue_topic(topic, payload, source)

    def process_batch(self) -> int:
        """Process up to BATCH_SIZE items (tables batched; topics 1x txn each)."""
        batch: List[Dict[str, Any]] = []
        while len(batch) < BATCH_SIZE and not self.pending_queue.empty():
            try:
                batch.append(self.pending_queue.get_nowait())
            except queue.Empty:
                break

        if not batch:
            return 0

        table_groups: Dict[str, List[Dict[str, Any]]] = defaultdict(list)
        topic_items: List[Dict[str, Any]] = []

        for item in batch:
            if 'topic' in item:
                topic_items.append(item)
            else:
                table_groups[item['table']].append(item)

        success = 0

        # Process table groups
        for table, items in table_groups.items():
            try:
                written = self._write_batch_to_db(table, items)
                success += written
                self.consecutive_failures = 0
            except Exception as e:
                log.error(f"Batch write failed for {table}: {e}")
                self._handle_batch_failure(items)

        # Process topics one-by-one
        for item in topic_items:
            try:
                topic: str = item['topic']
                if topic.startswith("holds."):
                    if not self.hold_handler:
                        raise RuntimeError("No hold handler configured")
                    self.hold_handler(item)
                elif topic.startswith("assets."):
                    if not self.asset_handler:
                        raise RuntimeError("No asset handler configured")
                    self.asset_handler(item)
                else:
                    raise ValueError(f"Unknown topic namespace: {topic}")

                success += 1
                self.consecutive_failures = 0

            except Exception as e:
                log.error(f"Topic event failed ({item.get('topic')}): {e}")
                self._handle_write_failure(item, str(e))

        return success

    def _write_batch_to_db(self, table: str, items: List[Dict[str, Any]]) -> int:
        conn = None
        ok = 0
        try:
            conn = get_db_connection()
            cur = conn.cursor()
            for item in items:
                try:
                    cols = list(item['data'].keys())
                    vals = [item['data'][c] for c in cols]
                    placeholders = ', '.join(['%s'] * len(cols))
                    col_names = ', '.join(cols)
                    cur.execute(f"INSERT INTO {table} ({col_names}) VALUES ({placeholders})", vals)
                    item['attempts'] += 1
                    self.stats['written'] += 1
                    self.stats[f'{table}_written'] += 1
                    ok += 1
                    notes.info(f"Written to {table}: {item['queue_id']}")
                except Exception as e:
                    log.error(f"Failed to write {item['queue_id']}: {e}")
                    self._handle_write_failure(item, str(e))
            conn.commit()
        except Exception as e:
            log.error(f"Database connection error: {e}")
            if conn:
                conn.rollback()
        finally:
            if conn:
                release_db_connection(conn)
        return ok

    def _handle_write_failure(self, item: Dict, error: str):
        item['attempts'] += 1
        if item['attempts'] < RETRY_MAX:
            self.pending_queue.put(item)
            self.stats['retries'] += 1
            notes.info(f"Re-queued {item.get('queue_id')} (attempt {item['attempts']})")
        else:
            self.add_to_dlq({**item, 'error': error, 'failed_at': now_pack().iso})

    def _handle_batch_failure(self, items: List[Dict]):
        self.consecutive_failures += 1
        for item in items:
            item['attempts'] += 1
            if item['attempts'] < RETRY_MAX:
                self.pending_queue.put(item)
            else:
                self.add_to_dlq({**item, 'error': 'Batch failure', 'failed_at': now_pack().iso})
        if self.consecutive_failures >= 10:
            self._send_failure_alert()

    def add_to_dlq(self, item: Dict):
        self.dead_letter_queue.append(item)
        self.stats['dlq_items'] += 1
        if len(self.dead_letter_queue) > DLQ_MAX_SIZE:
            self.dead_letter_queue = self.dead_letter_queue[-DLQ_MAX_SIZE:]
        notes.error(f"DLQ: {item.get('queue_id')} - {item.get('error', 'Unknown error')}")

    def _send_failure_alert(self):
        try:
            tp = now_pack()
            message = f"""Database Write Failures Detected

Time: {tp.human}
Mode: {get_mode()}
Consecutive Failures: {self.consecutive_failures}

Queue Stats:
- Pending: {self.pending_queue.qsize()}
- Dead Letter Queue: {len(self.dead_letter_queue)}
- Total Written: {self.stats.get('written', 0)}
- Total Failed: {self.stats.get('dlq_items', 0)}

Recent DLQ Entries:
"""
            for item in self.dead_letter_queue[-5:]:
                message += f"\n- {item.get('table') or item.get('topic')}: {item.get('error', 'Unknown')}"

            send_alert(
                subject="[ANDI] Write Failures Alert",
                message=message,
                process_name="andi"
            )
            self.consecutive_failures = 0
        except Exception as e:
            log.error(f"Failed to send alert: {e}")

    def get_stats(self) -> Dict:
        return {
            'pending': self.pending_queue.qsize(),
            'dlq_size': len(self.dead_letter_queue),
            'stats': dict(self.stats)
        }

# 🔸 Andi Core =====================================================

class ANDI:
    """Main ANDI process - provides queue methods and processes items."""

    def __init__(self):
        self.validator = SchemaValidator()
        self.queue = WriteQueue(self.validator)
        self.queue.set_hold_handler(self._process_hold_event)
        self.queue.set_asset_handler(self._process_asset_event)

        self.processing_thread: Optional[threading.Thread] = None
        self.running = False
        self.cycle_count = 0
        self.last_heartbeat = time.time()

    # 🔸 Table-specific queue methods ==============================

    def queue_order(self, data: Dict, source: str = None) -> str:
        return self.queue.enqueue('sim_orders', data, source or 'unknown')

    def queue_trade(self, data: Dict, source: str = None) -> str:
        return self.queue.enqueue('sim_trades', data, source or 'unknown')

    def queue_balance_update(self, data: Dict, source: str = None) -> str:
        return self.queue.enqueue('sim_balances', data, source or 'unknown')

    def queue_ticker(self, data: Dict, source: str = None) -> str:
        return self.queue.enqueue('tickstick', data, source or 'unknown')

    def queue_heartbeat(self, data: Dict, source: str = None) -> str:
        return self.queue.enqueue('heartbeats', data, source or 'unknown')

    def queue_realism(self, data: Dict, source: str = None) -> str:
        return self.queue.enqueue('realism_history', data, source or 'unknown')

    def queue_generic(self, table: str, data: Dict, source: str = None) -> str:
        return self.queue.enqueue(table, data, source or 'unknown')

    # 🔸 Topic queue methods =======================================

    def queue_hold(self, topic: str, payload: Dict, source: str = None) -> str:
        """Julius → holds.*"""
        return self.queue.enqueue_topic(topic, payload, source or 'julius')

    def queue_asset(self, topic: str, payload: Dict, source: str = None) -> str:
        """Helen → assets.*"""
        return self.queue.enqueue_topic(topic, payload, source or 'helen')

    # 🔸 Processing loop ===========================================

    def _process_loop(self):
        log.info("[PROCESSOR] Write processing thread started")

        while self.running:
            try:
                start_time = time.time()
                written = self.queue.process_batch()
                if written > 0:
                    notes.info(f"Batch processed: {written} items")
                self.cycle_count += 1
                if time.time() - self.last_heartbeat > HEARTBEAT_INTERVAL:
                    self._update_heartbeat()
                    self.last_heartbeat = time.time()
                elapsed = time.time() - start_time
                time.sleep(max(BATCH_INTERVAL - elapsed, 0.01))
            except Exception as e:
                log.error(f"Processing error: {e}")
                time.sleep(BATCH_INTERVAL)

        self._flush_queue()
        log.info("[PROCESSOR] Write processing thread stopped")

    def _flush_queue(self):
        log.info("Flushing queue...")
        flushed = 0
        while not self.queue.pending_queue.empty():
            written = self.queue.process_batch()
            flushed += written
            if written == 0:
                break
        log.info(f"Flushed {flushed} items")

    def _update_heartbeat(self):
        try:
            stats = self.queue.get_stats()
            update_heartbeat('andi')
            log.info(f"[HEARTBEAT] Cycles: {self.cycle_count}, "
                     f"Pending: {stats['pending']}, "
                     f"Written: {stats['stats'].get('written', 0)}, "
                     f"DLQ: {stats['dlq_size']}")
        except Exception as e:
            log.error(f"Heartbeat update failed: {e}")

    # 🔸 Lifecycle management ======================================

    def start(self):
        if self.running:
            return
        log.info(f"[INIT] ANDI starting in {get_mode()} mode")
        log.info(f"[INIT] Batch size: {BATCH_SIZE}, Interval: {BATCH_INTERVAL}s")
        self.running = True
        self.processing_thread = threading.Thread(target=self._process_loop, daemon=True)
        self.processing_thread.start()
        log.info("[INIT] ANDI ready to process writes")

    def stop(self):
        log.info("[SHUTDOWN] Stopping ANDI...")
        self.running = False
        if self.processing_thread:
            self.processing_thread.join(timeout=5)
        stats = self.queue.get_stats()
        log.info(f"[SHUTDOWN] Final stats: {stats}")

    def get_status(self) -> Dict:
        return {
            'running': self.running,
            'mode': get_mode(),
            'cycles': self.cycle_count,
            'queue_stats': self.queue.get_stats(),
            'validator': {
                'tables_loaded': len(self.validator.schemas),
                'last_loaded': self.validator.last_loaded.isoformat() if self.validator.last_loaded else None
            }
        }

    # 🔸 Hold event processing (Julius) ============================

    def _process_hold_event(self, item: Dict[str, Any]) -> None:
        topic = item['topic']
        payload = item['payload']
        src = item.get('source', 'julius')

        conn = None
        try:
            conn = get_db_connection()
            cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)

            if topic == "holds.create":
                hold_id = payload['hold_id']
                symbol  = payload['symbol']
                side    = payload['side']
                asset   = payload['asset']
                amount  = Decimal(str(payload['amount']))
                cur.execute("""
                    INSERT INTO wallet_holds
                      (hold_id, symbol, side, asset, amount_total, amount_remaining, status, origin, reason, created_at, updated_at)
                    VALUES (%s,%s,%s,%s,%s,%s,'active',%s,%s, NOW(), NOW())
                """, (hold_id, symbol, side, asset, amount, amount, payload.get('origin'), payload.get('reason')))
                cur.execute("""
                    UPDATE sim_balances
                       SET available = GREATEST(0, available - %s),
                           hold = hold + %s
                     WHERE asset = %s
                """, (amount, amount, asset))
                cur.execute("""
                    INSERT INTO wallet_hold_ledger
                      (hold_id, event_type, delta_amount, amount_remaining_after, actor, source, reason, created_at)
                    VALUES (%s,'create',%s,%s,'Julius',%s,%s, NOW())
                """, (hold_id, amount, amount, src, payload.get('reason')))

            elif topic == "holds.link":
                cur.execute("""
                    UPDATE wallet_holds
                       SET order_id=%s, updated_at=NOW()
                     WHERE hold_id=%s AND status='active' AND deleted=FALSE
                """, (payload['order_id'], payload['hold_id']))
                cur.execute("""
                    INSERT INTO wallet_hold_ledger
                      (hold_id, event_type, actor, source, order_id, reason, created_at)
                    VALUES (%s,'link','Julius',%s,%s,%s, NOW())
                """, (payload['hold_id'], src, payload['order_id'], payload.get('reason')))

            elif topic == "holds.release":
                hold_id = payload['hold_id']
                asset   = payload['asset']
                amount  = Decimal(str(payload['amount']))
                cur.execute("""
                    UPDATE wallet_holds
                       SET amount_remaining = GREATEST(0, amount_remaining - %s),
                           status = CASE WHEN amount_remaining - %s <= 0 THEN 'released' ELSE status END,
                           updated_at=NOW()
                     WHERE hold_id=%s AND deleted=FALSE
                """, (amount, amount, hold_id))
                cur.execute("""
                    UPDATE sim_balances
                       SET available = available + %s,
                           hold = GREATEST(0, hold - %s)
                     WHERE asset=%s
                """, (amount, amount, asset))
                cur.execute("""
                    INSERT INTO wallet_hold_ledger
                      (hold_id, event_type, delta_amount, actor, source, reason, created_at)
                    VALUES (%s,'release',%s,'Julius',%s,%s, NOW())
                """, (hold_id, amount, src, payload.get('reason')))

            elif topic == "holds.settle":
                hold_id    = payload['hold_id']
                order_id   = payload['order_id']
                asset_out  = payload['asset_out']
                amount_out = Decimal(str(payload['amount_out']))
                asset_in   = payload['asset_in']
                amount_in  = Decimal(str(payload['amount_in']))
                fee_paid   = Decimal(str(payload.get('fee_paid', 0)))
                # side from hold
                cur.execute("SELECT side FROM wallet_holds WHERE hold_id=%s AND deleted=FALSE LIMIT 1", (hold_id,))
                row = cur.fetchone()
                if not row:
                    raise RuntimeError(f"holds.settle: hold_id {hold_id} not found")
                side = row['side']
                cur.execute("""
                    UPDATE wallet_holds
                       SET amount_remaining = GREATEST(0, amount_remaining - %s),
                           status = CASE WHEN amount_remaining - %s <= 0 THEN 'settled' ELSE status END,
                           updated_at=NOW()
                     WHERE hold_id=%s AND deleted=FALSE
                """, (amount_out, amount_out, hold_id))
                if side == 'buy':
                    cur.execute("""UPDATE sim_balances SET hold = GREATEST(0, hold - %s) WHERE asset=%s""",
                                (amount_out, asset_out))
                    cur.execute("""
                        INSERT INTO sim_balances (asset, available, hold)
                        VALUES (%s, %s, 0)
                        ON CONFLICT (asset) DO UPDATE
                        SET available = sim_balances.available + EXCLUDED.available
                    """, (asset_in, amount_in))
                else:
                    cur.execute("""UPDATE sim_balances SET hold = GREATEST(0, hold - %s) WHERE asset=%s""",
                                (amount_out, asset_out))
                    cur.execute("""UPDATE sim_balances SET available = available + %s WHERE asset=%s""",
                                (amount_in, asset_in))
                cur.execute("""
                    INSERT INTO wallet_hold_ledger
                      (hold_id, event_type, delta_amount, amount_remaining_after, actor, source, order_id, reason, created_at)
                    VALUES (%s,'settle',%s,NULL,'Julius',%s,%s,%s, NOW())
                """, (hold_id, amount_out, src, order_id, payload.get('reason')))

            elif topic == "holds.sweep":
                cur.execute(f"""
                    UPDATE wallet_holds
                       SET status='released', updated_at=NOW()
                     WHERE status='active'
                       AND order_id IS NULL
                       AND deleted=FALSE
                       AND created_at < NOW() - INTERVAL '{HOLD_SWEEP_AGE_MIN} minutes'
                """)
                cur.execute("""
                    INSERT INTO wallet_hold_ledger
                      (hold_id, event_type, actor, source, reason, created_at)
                    VALUES (NULL,'sweep','Julius',%s,%s, NOW())
                """, (src, payload.get('reason', 'stale sweep')))

            else:
                raise ValueError(f"Unknown hold topic: {topic}")

            conn.commit()
            cur.close()

        except Exception:
            if conn:
                conn.rollback()
            raise
        finally:
            if conn:
                release_db_connection(conn)

    # 🔸 Asset event processing (Helen) ============================

    def _process_asset_event(self, item: Dict[str, Any]) -> None:
        """
        Execute a single asset-topic event as one transaction:
          - asset_holds (qty state)
          - asset_hold_ledger (audit)
        Helen never touches sim_balances; this is BASE qty reservation only.
        """
        topic = item['topic']
        payload = item['payload']
        src = item.get('source', 'helen')

        conn = None
        try:
            conn = get_db_connection()
            cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)

            if topic == "assets.create":
                # Required: hold_id, symbol, asset, qty
                hold_id = payload['hold_id']
                symbol  = payload['symbol']
                asset   = payload['asset']
                qty     = Decimal(str(payload['qty']))
                cur.execute("""
                    INSERT INTO asset_holds
                      (hold_id, symbol, asset, qty_total, qty_remaining, status, origin, reason, created_at, updated_at)
                    VALUES (%s,%s,%s,%s,%s,'active',%s,%s, NOW(), NOW())
                """, (hold_id, symbol, asset, qty, qty, payload.get('origin'), payload.get('reason')))
                cur.execute("""
                    INSERT INTO asset_hold_ledger
                      (hold_id, event_type, delta_qty, qty_remaining_after, actor, source, reason, created_at)
                    VALUES (%s,'create',%s,%s,'Helen',%s,%s, NOW())
                """, (hold_id, qty, qty, src, payload.get('reason')))

            elif topic == "assets.link":
                cur.execute("""
                    UPDATE asset_holds
                       SET order_id=%s, updated_at=NOW()
                     WHERE hold_id=%s AND status='active' AND deleted=FALSE
                """, (payload['order_id'], payload['hold_id']))
                cur.execute("""
                    INSERT INTO asset_hold_ledger
                      (hold_id, event_type, actor, source, order_id, reason, created_at)
                    VALUES (%s,'link','Helen',%s,%s,%s, NOW())
                """, (payload['hold_id'], src, payload['order_id'], payload.get('reason')))

            elif topic == "assets.release":
                hold_id = payload['hold_id']
                qty     = Decimal(str(payload['qty']))
                cur.execute("""
                    UPDATE asset_holds
                       SET qty_remaining = GREATEST(0, qty_remaining - %s),
                           status = CASE WHEN qty_remaining - %s <= 0 THEN 'released' ELSE status END,
                           updated_at=NOW()
                     WHERE hold_id=%s AND deleted=FALSE
                """, (qty, qty, hold_id))
                cur.execute("""
                    INSERT INTO asset_hold_ledger
                      (hold_id, event_type, delta_qty, actor, source, reason, created_at)
                    VALUES (%s,'release',%s,'Helen',%s,%s, NOW())
                """, (hold_id, qty, src, payload.get('reason')))

            elif topic == "assets.settle":
                # Required: hold_id, order_id, asset, qty
                hold_id  = payload['hold_id']
                order_id = payload['order_id']
                qty      = Decimal(str(payload['qty']))
                cur.execute("""
                    UPDATE asset_holds
                       SET qty_remaining = GREATEST(0, qty_remaining - %s),
                           status = CASE WHEN qty_remaining - %s <= 0 THEN 'settled' ELSE status END,
                           updated_at=NOW()
                     WHERE hold_id=%s AND deleted=FALSE
                """, (qty, qty, hold_id))
                cur.execute("""
                    INSERT INTO asset_hold_ledger
                      (hold_id, event_type, delta_qty, qty_remaining_after, actor, source, order_id, reason, created_at)
                    VALUES (%s,'settle',%s,NULL,'Helen',%s,%s,%s, NOW())
                """, (hold_id, qty, src, order_id, payload.get('reason')))

            elif topic == "assets.sweep":
                # Release orphaned asset holds older than threshold (no order_id)
                cur.execute(f"""
                    UPDATE asset_holds
                       SET status='released', updated_at=NOW()
                     WHERE status='active'
                       AND order_id IS NULL
                       AND deleted=FALSE
                       AND created_at < NOW() - INTERVAL '{ASSET_SWEEP_AGE_MIN} minutes'
                """)
                cur.execute("""
                    INSERT INTO asset_hold_ledger
                      (hold_id, event_type, actor, source, reason, created_at)
                    VALUES (NULL,'sweep','Helen',%s,%s, NOW())
                """, (src, payload.get('reason', 'stale asset sweep')))

            else:
                raise ValueError(f"Unknown asset topic: {topic}")

            conn.commit()
            cur.close()

        except Exception:
            if conn:
                conn.rollback()
            raise
        finally:
            if conn:
                release_db_connection(conn)

# 🔸 Singleton Instance ============================================

_andi_instance: Optional[ANDI] = None

def get_andi() -> ANDI:
    """Get singleton ANDI instance."""
    global _andi_instance
    if _andi_instance is None:
        _andi_instance = ANDI()
        _andi_instance.start()
    return _andi_instance

# 🔸 Main Entry Point (for standalone operation) ===================

def main():
    """Run ANDI as standalone process."""
    try:
        write_pid_file(PID_FILE)
        andi = get_andi()
        while not shutdown_requested:
            time.sleep(1)
        andi.stop()
    except KeyboardInterrupt:
        log.info("[SHUTDOWN] Interrupted by user")
    except Exception as e:
        log.error(f"[FATAL] {e}")
    finally:
        cleanup_pid_file(PID_FILE)

if __name__ == "__main__":
    main()

================================================================================
FILE: mm/utils/canary/laurel.pid
================================================================================
3280349

================================================================================
FILE: mm/utils/canary/laurel.py
================================================================================
#===================================================================
# 🍁 A R I A N D E           bot version 6.1 file build 20250913.03
#===================================================================
# last update: 2025 | Sept. 13                  Production ready ✅
#===================================================================
# LAUREL - Heartbeat Monitor
# mm/utils/canary/laurel.py
#
# Monitors all process heartbeats, alerts on failures
# Sends hourly status report as own heartbeat
#
# [520] [741] [8]
#===================================================================
# 🔰 THE COMMANDER            ✔ PERSISTANT RUNTIME  ✔ MONIT MANAGED
#===================================================================

# 🔸 Standard Library Imports ======================================

import os
import sys
import signal
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
from collections import defaultdict
from dotenv import load_dotenv
import psycopg2
import psycopg2.extras
import smtplib
import ssl
import uuid
from email.message import EmailMessage
from email.utils import formataddr
from zoneinfo import ZoneInfo
import importlib

# 🔸 Load environment variables ====================================

load_dotenv()

# 🔸 Add parent directory to path for imports ======================

sys.path.append('/root/Echelon/valentrix')

from mm.utils.helpers.wintermute import (
    get_logger,
    now_pack,
    write_pid_file,
    cleanup_pid_file,
    get_db_connection,
    release_db_connection
)
from mm.utils.helpers.inara import get_mode
from mm.config.marcus import ALERT_EMAIL_RECIPIENT
import mm.config.marcus as marcus

# 🔸 Email Function ================================================

def send_email(subject: str, status: str, title: str, message: str) -> str:
    import importlib
    import os
    import smtplib
    import ssl
    import uuid
    from email.message import EmailMessage
    from email.utils import formataddr
    from datetime import datetime
    from zoneinfo import ZoneInfo
    import mm.config.marcus as marcus

    importlib.reload(marcus)
    if not bool(getattr(marcus, "ALERT_EMAIL_ENABLED", False)):
        return "disabled"
    if str(getattr(marcus, "ALERT_EMAIL_ENCRYPT", "SSL")).upper() != "SSL":
        return "Simple Mail Transfer Protocol not established. No conn."

    host = getattr(marcus, "ALERT_EMAIL_SMTP_SERVER", None)
    port = getattr(marcus, "ALERT_EMAIL_SMTP_PORT", None)
    recipient = getattr(marcus, "ALERT_EMAIL_RECIPIENT", None)

    USERCODE = "LAU"  # hardcode per file

    # ---- Edit Sender Info (per file) ----
    user = os.getenv(f"{USERCODE}_USR")
    pwd = os.getenv(f"{USERCODE}_PWD")
    sender_email = user
    sender_name = os.getenv(f"{USERCODE}_NAME")
    # -------------------------------------

    # status color map
    STATUS_COLORS = {
        "STATCON3": "#F1C232",	# on the first missing heartbeat 
        "STATCON2": "#E69138",	# on the second missing heartbeat
        "STATCON1": "#CC0000",	# on the third missing heartbeat
        "SIGCON1": 	"#FB6D8B",	# Process never started
		"OPSCON5": 	"#F5F5F5",	# Normal, all systems nominal
        "OPSCON1": 	"#990000",	# Issues detected
    }    
    
    status_text = str(status).upper()
    status_color = STATUS_COLORS.get(status_text, "#BE644C")

    msg = EmailMessage()
    domain = sender_email.split("@")[1] if "@" in sender_email else "hodlcorp.io"
    msg_id = f"<{uuid.uuid4()}@{domain}>"
    msg["Message-ID"] = msg_id
    msg["From"] = formataddr((sender_name, sender_email))
    msg["To"] = recipient
    msg["Subject"] = subject
    msg["X-Priority"] = "1"
    msg["X-MSMail-Priority"] = "High"
    msg["Importance"] = "High"

    # footer fields
    now_tz = datetime.now(ZoneInfo("America/Toronto"))
    sent_str = now_tz.strftime("%Y-%m-%d %H:%M:%S America/Toronto")
    epoch_ms = int(now_tz.timestamp() * 1000)
    mid_clean = msg_id.strip("<>").split("@", 1)[0]

    # full HTML body (single block)
    html_body = f"""
<div style="font-family: monospace;">
  <table role="presentation" width="100%" height="20px" cellpadding="8px" cellspacing="0" border="0">
    <!-- Top Banner -->
    <tr style="font-family: Georgia, 'Times New Roman', Times, serif;font-size:20px;font-weight:600;background-color:#333;">
      <td align="left" style="color:#EFEFEF;letter-spacing:12px;">INTCOMM</td>
      <td align="right" style="color:{status_color};letter-spacing:4px;">{status_text}</td>
    </tr>

    <!-- Message Title -->
    <tr width="100%" cellpadding="6px" style="font-family: Tahoma, Geneva, sans-serif;text-align:left;font-size:14px;font-weight:600;color:#333;">
      <td colspan="2">
        {title}
      </td>
    </tr>

    <!-- Message Content -->
    <tr width="100%" cellpadding="6px" style="font-family: Tahoma, Geneva, sans-serif;text-align:left;font-size:11px;font-weight:400;line-height:1.5;color:#333;">
      <td colspan="2">
        {message}
      </td>
    </tr>

    <!-- UNUSED SPACER ROW -->
    <tr width="100%" height="25px"><td colspan="2">&nbsp;</td></tr>
  </table>

  <!-- Footer -->
  <table role="presentation" width="400px" height="20px" cellpadding="4" cellspacing="0" border="0" style="font-family: Tahoma, Geneva, sans-serif;">
    <!-- DOCINT -->
    <tr style="background-color:#333;">
      <td colspan="2" style="color:#efefef;font-size:12px;font-weight:600;">DOCINT</td>
    </tr>

    <tr style="background-color:#E9E9E5;">
      <td width="30px" style="color:#333;font-size:10px;font-weight:600;">SENT</td>

      <td width="10px" style="color:#333;font-size:10px;font-weight:600;">&rarr;</td>
      <td style="color:#333;font-size:11px;font-weight:400;">{sent_str}</td>
    </tr>

    <tr style="background-color:#F2F2F0;">
      <td width="30px" style="color:#333;font-size:10px;font-weight:600;">EPOCH</td>
      <td width="10px" style="color:#333;font-size:10px;font-weight:600;">&rarr;</td>
      <td style="color:#333;font-size:11px;font-weight:400;">{epoch_ms} (ms since 1970/01/01 0:00 UTC)</td>
    </tr>

    <tr style="background-color:#E9E9E5;">
      <td width="30px" style="color:#333;font-size:10px;font-weight:600;">m.ID</td>
      <td width="10px" style="color:#333;font-size:10px;font-weight:600;">&rarr;</td>
      <td style="color:#333;font-size:11px;font-weight:400;">{mid_clean}</td>
    </tr>
  </table>
</div>
"""

    msg.add_alternative(html_body, subtype="html")

    ctx = ssl.create_default_context()
    with smtplib.SMTP_SSL(host, port, context=ctx, timeout=10) as s:
        if user and pwd:
            s.login(user, pwd)
        s.send_message(msg)

    return msg_id

# 🔸 Configuration =================================================

PID_FILE = "/root/Echelon/valentrix/mm/utils/canary/laurel.pid"
LOG_FILE = "/root/Echelon/valentrix/mm/utils/canary/laurel.log"
NOTES_FILE = "/root/Echelon/valentrix/mm/utils/canary/laurel_notes.log"

# 🔸 Monitoring parameters =========================================

CHECK_INTERVAL = 30  # Check every 30 seconds
HOURLY_REPORT_INTERVAL = 3600  # Send status email every hour

# 🔸 Process-specific heartbeat thresholds (in seconds) ============

HEARTBEAT_THRESHOLDS = {
    #'ariadne': 120,      # Main bot - 2 minutes
    #'hari': 60,          # SOC - 1 minute
    'alma': 60,          # Ticker sticker - 60 seconds
    #'karin': 60,         # Schema monitor - 1 minute
    #'andi': 360,         # TQT processor - 6 minutes (updates every 5)
    'edith': 1800,       # Partition manager - 30 minutes
    #'default': 300       # Default - 5 minutes
}

# 🔸 Alert levels for email alerts =================================

ALERT_LEVELS = {
    1: "STATCON3",	# on the first missing heartbeat 
    2: "STATCON2",	# on the second missing heartbeat
    3: "STATCON1",	# on the third missing heartbeat
    4: "SIGCON1",	# Process never started
    5: "OPSCON5",	# Normal, all systems nominal
    6: "OPSCON1",	# Issues detected
}

# 🔸 Global shutdown flag ==========================================

shutdown_requested = False

# 🔸 Loggers =======================================================

log = get_logger("laurel", LOG_FILE)
notes = get_logger("laurel.notes", NOTES_FILE)

# 🔸 Signal Handlers ===============================================

def signal_handler(signum, frame):
    """Handle shutdown signals gracefully."""
    global shutdown_requested
    log.info(f"[SHUTDOWN] Received signal {signum}, shutting down...")
    shutdown_requested = True

signal.signal(signal.SIGINT, signal_handler)
signal.signal(signal.SIGTERM, signal_handler)

# 🔸 Heartbeat Checker =============================================

class HeartbeatMonitor:
    """Monitors process heartbeats and sends alerts."""

    def __init__(self):
        self.missed_checks = defaultdict(int)  # Track consecutive misses
        self.last_alert_level = defaultdict(int)  # Track alert escalation
        self.process_status = {}  # Current status of each process
        self.check_count = 0
        self.alerts_sent = 0
        self.last_hourly_report = time.time()

    def check_heartbeats(self) -> Dict[str, Dict]:
        """
        Check all process heartbeats.
        Returns dict of process statuses.
        """
        self.check_count += 1
        conn = None
        statuses = {}

        try:
            conn = get_db_connection()
            cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)

            # Get all heartbeats
            cur.execute("""
                SELECT
                    process_name,
                    last_heartbeat,
                    status,
                    pid,
                    cycle_count,
                    EXTRACT(EPOCH FROM (NOW() - last_heartbeat)) as seconds_since
                FROM heartbeats
            """)

            rows = cur.fetchall()
            cur.close()

            # Check each process
            for row in rows:
                process = row['process_name']
                seconds_since = float(row['seconds_since']) if row['seconds_since'] else 999999
                threshold = HEARTBEAT_THRESHOLDS.get(process, HEARTBEAT_THRESHOLDS['default'])

                # Determine if heartbeat is stale
                is_stale = seconds_since > threshold

                statuses[process] = {
                    'last_heartbeat': row['last_heartbeat'],
                    'seconds_since': seconds_since,
                    'threshold': threshold,
                    'pid': row['pid'],
                    'status': row['status'],
                    'cycle_count': row['cycle_count'],
                    'is_stale': is_stale
                }

                # Handle stale heartbeats
                if is_stale:
                    self.handle_stale_heartbeat(process, seconds_since, threshold)
                else:
                    # Reset if back to normal
                    if self.missed_checks[process] > 0:
                        notes.info(f"{process} recovered after {self.missed_checks[process]} missed checks")
                        self.missed_checks[process] = 0
                        self.last_alert_level[process] = 0

            # Check for processes that have never reported
            self.check_missing_processes(statuses)

        except Exception as e:
            log.error(f"Failed to check heartbeats: {e}")
        finally:
            if conn:
                release_db_connection(conn)

        self.process_status = statuses
        return statuses

    def handle_stale_heartbeat(self, process: str, seconds_since: float, threshold: float):
        """Handle a stale heartbeat with tiered alerting."""
        self.missed_checks[process] += 1
        misses = self.missed_checks[process]

        # Determine alert level
        if misses >= 3:
            alert_level = 3
        elif misses == 2:
            alert_level = 2
        else:
            alert_level = 1

        # Only alert if escalating
        if alert_level > self.last_alert_level[process]:
            self.send_process_alert(process, alert_level, seconds_since, threshold)
            self.last_alert_level[process] = alert_level

        # Log for tracking
        level_name = ALERT_LEVELS[alert_level]
        notes.warning(f"[{level_name}] {process}: {seconds_since:.0f}s since heartbeat (threshold: {threshold}s)")

    def check_missing_processes(self, current_statuses: Dict):
        """Check for expected processes that have never reported."""
        expected = set(HEARTBEAT_THRESHOLDS.keys()) - {'default'}
        reporting = set(current_statuses.keys())
        missing = expected - reporting

        for process in missing:
            # Only alert once per missing process
            if process not in self.missed_checks:
                self.missed_checks[process] = 3  # Treat as critical
                self.send_process_alert(process, 3, None, None, missing=True)

    def send_process_alert(self, process: str, alert_level: int,
                          seconds_since: Optional[float], threshold: Optional[float],
                          missing: bool = False):
        """Send alert for process issues."""
        try:
            tp = now_pack()
            
            # Build subject, status, and title based on condition
            if missing:
                subject = f"[ SIGCON1 ] {process} missing | Mode -> {get_mode()}"
                status = "SIGCON1"
                title = f"SIGCON1 | Critical Alert | {process} cannot be found."
            elif alert_level == 3:
                subject = f"[ STATCON1 ] !PROCESS DOWN! Flatline on {process} | Mode -> {get_mode()}"
                status = "STATCON1"
                title = f"STATCON1 | Critical Alert | {process} is not running."
            elif alert_level == 2:
                subject = f"[ STATCON2 ] Second missed heartbeat for {process} | Mode -> {get_mode()}"
                status = "STATCON2"
                title = f"STATCON2 | {process} has missed it's second check-in."
            else:  # alert_level == 1
                subject = f"[ STATCON3 ] Missed heartbeat for {process} | Mode -> {get_mode()}"
                status = "STATCON3"
                title = f"STATCON3 | {process} has missed it's first check-in."
            
            # Build message
            if missing:
                message_html = f"""<b>Process:</b> {process}<br>
<b>Status:</b> NEVER REPORTED<br>
<b>Mode:</b> {get_mode()}<br>
<b>Time:</b> {tp.human}<br>

<b>Action Required:</b> Start {process} immediately"""
            else:
                minutes_since = seconds_since / 60.0 if seconds_since else 0
                
                if alert_level == 3:
                    action_text = "ACTION REQUIRED: Process appears to be DOWN. Restart immediately."
                elif alert_level == 2:
                    action_text = "WARNING: Process may be struggling. Check logs."
                else:
                    action_text = "NOTICE: Monitoring closely, no action required yet."
                
                system_status = self._format_system_status()
                
                message_html = f"""<b>Process:</b> {process}<br>
<b>Last Heartbeat:</b> {minutes_since:.1f} minutes ago<br>
<b>Threshold:</b> {threshold} seconds<br>
<b>Consecutive Misses:</b> {self.missed_checks[process]}<br>
<b>Alert Level:</b> {alert_level}/3<br>
<b>Mode:</b> {get_mode()}<br>
<b>Time:</b> {tp.human}<br><br>

<b>{action_text}</b><br>
<b>{system_status}</b>"""

            # Send alert
            send_email(
                subject=subject,
                status=status,
                title=title,
                message=message_html
            )
            self.alerts_sent += 1
            log.info(f"Alert sent: {subject}")

        except Exception as e:
            log.error(f"Failed to send alert for {process}: {e}")

    def send_hourly_report(self):
        """Send hourly status report (Laurel's own heartbeat)."""
        try:
            tp = now_pack()

            # Check if all systems normal
            all_normal = all(
                not status.get('is_stale', False)
                for status in self.process_status.values()
            )

            if all_normal:
                subject = f"[ OPSCON5 ] All systems nominal | Mode -> {get_mode()}"
                status = "OPSCON5"
                title = "SITREP WHITE"

                # Add status details
                system_status = self._format_system_status()

                message_html = f"""
                                <b>Time:</b> {tp.human}<br>
                                <b>Mode:</b> {get_mode()}<br>
                                <b>Status:</b> ALL SYSTEMS OPERATIONAL<br>

                                <b>Monitoring {len(self.process_status)} processes:</b><br>
                                {system_status}<br><br>

                                <b>Laurel Statistics:</b><br>
                                - <b>Checks Performed:</b> {self.check_count}<br>
                                - <b>Alerts Sent:</b> {self.alerts_sent}<br>
                                - <b>Uptime:</b> {(time.time() - start_time) / 3600:.1f} hours<br>
                                """
            else:
                problem_count = sum(1 for s in self.process_status.values() if s.get('is_stale', False))
                subject = f"[ OPSCON1 ] PRIORITY SITREP -> {problem_count} issues detected. {tp.dt.strftime('%H:%M:%S')}"
                status = "OPSCON1"
                title = f"[ OPSCON1 ] Priority Situation Report - ISSUES DETECTED"

                # Build issues list
                issues_text = ""
                for process, stat in self.process_status.items():
                    if stat.get('is_stale', False):
                        issues_text += f"- {process}: {stat['seconds_since']:.0f}s since heartbeat\n"

                # Add status details
                system_status = self._format_system_status()

                message_html = f"""
                                <b>Time:</b> {tp.human}<br>
                                <b>Mode:</b> {get_mode()}<br>
                                <b>Status:</b> {problem_count} PROCESS(ES) WITH ISSUES<br><br>

                                <b>Issues:</b><br>
                                {issues_text}<br><br>

                                <b>All Processes:</b><br>
                                {system_status}<br><br>

                                <b>Laurel Statistics:</b><br>
                                - <b>Checks Performed:</b> {self.check_count}<br>
                                - <b>Alerts Sent:</b> {self.alerts_sent}<br>
                                - <b>Uptime:</b> {(time.time() - start_time) / 3600:.1f} hours<br>
                                """

            # Send report
            send_email(
                subject=subject,
                status=status,
                title=title,
                message=message_html
            )
            log.info(f"Hourly report sent: {'All normal' if all_normal else f'{problem_count} issues'}")

        except Exception as e:
            log.error(f"Failed to send hourly report: {e}")

    def _format_system_status(self) -> str:
        """Format current system status for reports."""
        lines = ["\nCurrent System Status:"]
        lines.append("-" * 50)

        for process, status in sorted(self.process_status.items()):
            if status.get('is_stale', False):
                state = "âš ï¸ STALE"
            else:
                state = "âœ“ OK"

            seconds = status.get('seconds_since', 0)
            if seconds < 60:
                time_str = f"{seconds:.0f}s ago"
            else:
                time_str = f"{seconds/60:.1f}m ago"

            lines.append(f"{process:12} {state:8} Last: {time_str:10} PID: {status.get('pid', 'N/A')}")

        lines.append("-" * 50)
        return "\n".join(lines) + "\n"

    def update_own_heartbeat(self):
        """Update Laurel's own heartbeat."""
        try:
            conn = get_db_connection()
            cur = conn.cursor()

            cur.execute("""
                INSERT INTO heartbeats (process_name, last_heartbeat, status, pid, cycle_count)
                VALUES ('laurel', NOW(), 'monitoring', %s, %s)
                ON CONFLICT (process_name)
                DO UPDATE SET
                    last_heartbeat = NOW(),
                    status = 'monitoring',
                    pid = %s,
                    cycle_count = %s
            """, (os.getpid(), self.check_count, os.getpid(), self.check_count))

            conn.commit()
            cur.close()
            release_db_connection(conn)

        except Exception as e:
            log.error(f"Failed to update own heartbeat: {e}")

# 🔸 Main Process Loop =============================================

def main():
    """Main process loop."""
    global start_time
    start_time = time.time()

    try:
        write_pid_file(PID_FILE)
        log.info(f"[INIT] LAUREL starting in {get_mode()} mode")
        log.info(f"[INIT] Monitoring {len(HEARTBEAT_THRESHOLDS)-1} processes")
        log.info(f"[INIT] Check interval: {CHECK_INTERVAL}s, Report interval: {HOURLY_REPORT_INTERVAL}s")

        monitor = HeartbeatMonitor()

        # Initial check
        monitor.check_heartbeats()
        monitor.update_own_heartbeat()

        # Send initial report
        monitor.send_hourly_report()
        monitor.last_hourly_report = time.time()

        # Main loop
        while not shutdown_requested:
            # Sleep for check interval
            time.sleep(CHECK_INTERVAL)

            # Check all heartbeats
            monitor.check_heartbeats()

            # Update own heartbeat every 2 checks (1 minute)
            if monitor.check_count % 2 == 0:
                monitor.update_own_heartbeat()

            # Send hourly report
            if time.time() - monitor.last_hourly_report >= HOURLY_REPORT_INTERVAL:
                monitor.send_hourly_report()
                monitor.last_hourly_report = time.time()

        log.info("[SHUTDOWN] LAUREL shutting down gracefully")

    except KeyboardInterrupt:
        log.info("[SHUTDOWN] Interrupted by user")
    except Exception as e:
        log.error(f"[FATAL] {e}")
    finally:
        cleanup_pid_file(PID_FILE)

# 🔸 Entry Point ===================================================

if __name__ == "__main__":
    main()

================================================================================
FILE: mm/utils/nexus_6/rachael.py
================================================================================
#===================================================================
# 🍁 A R I A N D E           bot version 6.1 file build 20250915.01
#===================================================================
# last update: 2025 | Sept. 15                  Production ready ❌
#===================================================================
# Rachael
# mm/utils/nexus-6/rachael.py
#
# Rachael: Nexus-6 logic unit for near-pass order recovery.
# Handles orders scoring between 70–80 by applying staged 
# adjustments (split, widen, reprice), tracking each attempt via 
# persistent memory. 
#
# [520] [741] [8]
#===================================================================
# 🔰 THE COMMANDER            ✖ PERSISTANT RUNTIME  ✖ MONIT MANAGED
#===================================================================

import os
import json
import logging
from datetime import datetime

# ── Logger Setup ──────────────────────────────────────────────────────
logger = logging.getLogger('rachael.replicant')
logger.setLevel(logging.INFO)

log_path = os.path.join(os.path.dirname(__file__), 'rachael.log')
handler = logging.FileHandler(log_path)
formatter = logging.Formatter('[%(asctime)s] %(message)s')
handler.setFormatter(formatter)
if not logger.handlers:
    logger.addHandler(handler)

MEMORY_PATH = os.path.join(os.path.dirname(__file__), 'memories')
os.makedirs(MEMORY_PATH, exist_ok=True)

# ── Rachael Class ──────────────────────────────────────────────────────

class Replicant:
    """
    Rachael: Nexus-6 logic unit for near-pass order recovery.
    Handles orders scoring between 70–80 by applying staged adjustments
    (split, widen, reprice), tracking each attempt via persistent memory.
    """

    def __init__(self, order: dict):
        self.order = order
        self.order_id = str(order.get('id'))
        self.memory_file = os.path.join(MEMORY_PATH, f"{self.order_id}.json")
        self.meta = self._recall()

    def _recall(self):
        if os.path.exists(self.memory_file):
            try:
                with open(self.memory_file, 'r') as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"[🕊️ Rachael] Order {self.order_id}: memory load failed: {e}")
                return {"naomi_stage": 0}
        else:
            return {
                "naomi_stage": 0,
                "first_seen": datetime.utcnow().isoformat()
            }

    def _remember(self):
        try:
            with open(self.memory_file, 'w') as f:
                json.dump(self.meta, f, indent=4)
        except Exception as e:
            logger.error(f"[🕊️ Rachael] Order {self.order_id}: failed to write memory: {e}")

    def process(self):
        stage = self.meta.get("naomi_stage", 0)

        if stage == 0:
            self._apply_split()
        elif stage == 1:
            self._apply_widen()
        elif stage == 2:
            self._apply_reprice()
        else:
            logger.info(f"[🕊️ Rachael] Order {self.order_id}: exhausted all options.")
            return

        self.meta["naomi_stage"] = stage + 1
        self.meta["last_modified"] = datetime.utcnow().isoformat()
        self._remember()

    def _apply_split(self):
        logger.info(f"[🕊️ Rachael] Order {self.order_id}: attempting split.")

        original_size = self.order.get('size')
        if not original_size or original_size < 2:
            logger.info(f"[🕊️ Rachael] Order {self.order_id}: split skipped — size too small.")
            return

        split_size = original_size // 2
        logger.info(f"[🕊️ Rachael] Order {self.order_id}: split into 2 x {split_size} units.")

        self.meta["last_adjustment"] = {
            "type": "split",
            "new_size": split_size
        }

    def _apply_widen(self):
        logger.info(f"[🕊️ Rachael] Order {self.order_id}: widening spread.")

        current_spread = self.order.get('spread', 0.005)
        widened_spread = round(current_spread * 1.2, 6)
        logger.info(f"[🕊️ Rachael] Order {self.order_id}: spread widened from {current_spread:.4%} → {widened_spread:.4%}.")

        self.order['spread'] = widened_spread
        self.meta["last_adjustment"] = {
            "type": "widen",
            "old_spread": current_spread,
            "new_spread": widened_spread
        }

    def _apply_reprice(self):
        logger.info(f"[🕊️ Rachael] Order {self.order_id}: adjusting price level.")

        price = self.order.get('price')
        side = self.order.get('side')
        if not price or not side:
            logger.warning(f"[🕊️ Rachael] Order {self.order_id}: insufficient data to reprice.")
            return

        adjust_pct = 0.002  # 0.2%
        if side == 'buy':
            new_price = round(price * (1 - adjust_pct), 2)

        else:
            new_price = round(price * (1 + adjust_pct), 2)

        logger.info(f"[🕊️ Rachael] Order {self.order_id}: price adjusted from {price} → {new_price} ({side}).")

        self.order['price'] = new_price
        self.meta["last_adjustment"] = {
            "type": "reprice",
            "old_price": price,
            "new_price": new_price
        }

